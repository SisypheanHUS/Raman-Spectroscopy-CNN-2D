{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Re-run Model (best-model form experiment)",
   "id": "dc16b0f938b58234"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a fixed seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Define directories\n",
    "data_dir = 'data'\n",
    "synthetic_dir = os.path.join(data_dir, 'synthetic')\n",
    "maps_dir = os.path.join(data_dir, 'maps')\n",
    "labels_dir = os.path.join(data_dir, 'labels')\n",
    "visualizations_dir = os.path.join(data_dir, 'visualizations')\n",
    "experiment_dir = os.path.join('experiments', 'experiment_20250914_220130')  # Thay bằng timestamp của lần chạy trước\n",
    "model_dir = os.path.join(experiment_dir, 'models')\n",
    "results_dir = os.path.join(experiment_dir, 'results')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load saved data\n",
    "X_1d = np.load(os.path.join(synthetic_dir, \"synthetic_1d.npy\"))\n",
    "X_2d = np.load(os.path.join(maps_dir, \"spectral_maps_gadf.npy\"))\n",
    "labels_df = pd.read_csv(os.path.join(labels_dir, \"labels.csv\"))\n",
    "y = labels_df[\"label\"].values\n",
    "\n",
    "print(\"X_1d shape:\", X_1d.shape)\n",
    "print(\"X_2d shape:\", X_2d.shape)\n",
    "print(\"labels_df shape:\", labels_df.shape)\n",
    "print(\"Label distribution:\\n\", labels_df[\"label\"].value_counts())\n",
    "\n",
    "# Define baseline model (for completeness, in case needed)\n",
    "def create_baseline_model(input_shape=880):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Reshape((input_shape, 1)),\n",
    "        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),\n",
    "        layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_baseline_model(baseline_model, noise_data, epochs=10, batch_size=32):\n",
    "    try:\n",
    "        labels = np.load(os.path.join(data_dir, 'labels_noise_pure_182.npy'))\n",
    "        print(\"Đã tải nhãn từ labels_noise_pure_182.npy thành công!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải nhãn: {e}. Sử dụng nhãn ngẫu nhiên.\")\n",
    "        labels = np.random.randint(0, 2, size=noise_data.shape[0])\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(noise_data.shape[0]):\n",
    "        pure = noise_data[i, 0, 0, :, 0]\n",
    "        noisy = noise_data[i, 0, 1, :, 0]\n",
    "        X.append(noisy)\n",
    "        y.append(labels[i])\n",
    "    X = np.array(X)[:, :, np.newaxis]\n",
    "    y = np.array(y)\n",
    "    baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    baseline_model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    baseline_model.save_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    return baseline_model\n",
    "\n",
    "# Load noise data (if needed for baseline model training)\n",
    "def load_noise_data():\n",
    "    try:\n",
    "        noise_data = np.load(os.path.join(data_dir, 'dataset_noise_pure_182.npy'))\n",
    "        return noise_data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải dữ liệu nhiễu: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# Load or train baseline model\n",
    "baseline_model = create_baseline_model(input_shape=880)\n",
    "try:\n",
    "    baseline_model.load_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    print(\"Trọng số mô hình baseline đã được tải thành công!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tải trọng số: {e}. Training baseline model.\")\n",
    "    noise_data = load_noise_data()\n",
    "    if noise_data.size == 0:\n",
    "        raise FileNotFoundError(\"Không thể tải dữ liệu nhiễu.\")\n",
    "    baseline_model = train_baseline_model(baseline_model, noise_data)\n",
    "\n",
    "# Define DenseNet models\n",
    "def build_1d_densenet(input_shape=(880, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(48, 7, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv1D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv1D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_densenet(input_shape=(64, 64, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(48, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Define ResNet models\n",
    "def build_1d_resnet(input_shape=(880, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, 5, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_resnet(input_shape=(64, 64, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Plot confusion matrix (updated version)\n",
    "def plot_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='viridis', cbar=True,  # Changed to normalized values and viridis colormap\n",
    "                xticklabels=[f\"{i*10}% Ethanol\" for i in range(11)],\n",
    "                yticklabels=[f\"{i*10}% Ethanol\" for i in range(11)],\n",
    "                annot_kws={\"size\": 10}, norm=plt.Normalize(vmin=0, vmax=np.max(cm)))\n",
    "    plt.title(title, fontsize=14, pad=15)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation_1d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "data_augmentation_2d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_1d_train, X_1d_test, y_train, y_test = train_test_split(X_1d, y, test_size=0.2, random_state=42)\n",
    "X_2d_train, X_2d_test, y_train_2d, y_test_2d = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.arange(11), y=y)\n",
    "class_weight = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# Train models with fresh optimizers\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# DenseNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_densenet)\n",
    "densenet_1d = models.Sequential([data_augmentation_1d, build_1d_densenet()])\n",
    "densenet_1d.compile(optimizer=optimizer_1d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# DenseNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_densenet)\n",
    "densenet_2d = models.Sequential([data_augmentation_2d, build_2d_densenet()])\n",
    "densenet_2d.compile(optimizer=optimizer_2d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# ResNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_resnet)\n",
    "resnet_1d = models.Sequential([data_augmentation_1d, build_1d_resnet()])\n",
    "resnet_1d.compile(optimizer=optimizer_1d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# ResNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_resnet)\n",
    "resnet_2d = models.Sequential([data_augmentation_2d, build_2d_resnet()])\n",
    "resnet_2d.compile(optimizer=optimizer_2d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_1d_densenet = densenet_1d.predict(X_1d_test)\n",
    "y_pred_2d_densenet = densenet_2d.predict(X_2d_test)\n",
    "y_pred_1d_resnet = resnet_1d.predict(X_1d_test)\n",
    "y_pred_2d_resnet = resnet_2d.predict(X_2d_test)\n",
    "\n",
    "y_pred_1d_densenet_labels = np.argmax(y_pred_1d_densenet, axis=1)\n",
    "y_pred_2d_densenet_labels = np.argmax(y_pred_2d_densenet, axis=1)\n",
    "y_pred_1d_resnet_labels = np.argmax(y_pred_1d_resnet, axis=1)\n",
    "y_pred_2d_resnet_labels = np.argmax(y_pred_2d_resnet, axis=1)\n",
    "\n",
    "densenet_1d_acc = np.mean(y_pred_1d_densenet_labels == y_test)\n",
    "densenet_2d_acc = np.mean(y_pred_2d_densenet_labels == y_test_2d)\n",
    "resnet_1d_acc = np.mean(y_pred_1d_resnet_labels == y_test)\n",
    "resnet_2d_acc = np.mean(y_pred_2d_resnet_labels == y_test_2d)\n",
    "\n",
    "densenet_1d_precision = precision_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_precision = precision_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_precision = precision_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_precision = precision_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_recall = recall_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_recall = recall_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_recall = recall_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_recall = recall_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_f1 = f1_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_f1 = f1_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_f1 = f1_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_f1 = f1_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "print(\"\\nKết quả đánh giá:\")\n",
    "print(f\"DenseNet 1D - Accuracy: {densenet_1d_acc:.4f}, Precision: {densenet_1d_precision:.4f}, Recall: {densenet_1d_recall:.4f}, F1: {densenet_1d_f1:.4f}\")\n",
    "print(f\"DenseNet 2D (GADF) - Accuracy: {densenet_2d_acc:.4f}, Precision: {densenet_2d_precision:.4f}, Recall: {densenet_2d_recall:.4f}, F1: {densenet_2d_f1:.4f}\")\n",
    "print(f\"ResNet 1D - Accuracy: {resnet_1d_acc:.4f}, Precision: {resnet_1d_precision:.4f}, Recall: {resnet_1d_recall:.4f}, F1: {resnet_1d_f1:.4f}\")\n",
    "print(f\"ResNet 2D (GADF) - Accuracy: {resnet_2d_acc:.4f}, Precision: {resnet_2d_precision:.4f}, Recall: {resnet_2d_recall:.4f}, F1: {resnet_2d_f1:.4f}\")\n",
    "\n",
    "# Save results\n",
    "plot_confusion_matrix(y_test, y_pred_1d_densenet_labels, \"Confusion Matrix - DenseNet 1D\", \"cm_densenet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_densenet_labels, \"Confusion Matrix - DenseNet 2D (GADF)\", \"cm_densenet_2d_gadf.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_1d_resnet_labels, \"Confusion Matrix - ResNet 1D\", \"cm_resnet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_resnet_labels, \"Confusion Matrix - ResNet 2D (GADF)\", \"cm_resnet_2d_gadf.png\")\n",
    "\n",
    "densenet_1d.save(os.path.join(model_dir, 'densenet_1d_full.keras'))\n",
    "densenet_2d.save(os.path.join(model_dir, 'densenet_2d_full.keras'))\n",
    "resnet_1d.save(os.path.join(model_dir, 'resnet_1d_full.keras'))\n",
    "resnet_2d.save(os.path.join(model_dir, 'resnet_2d_full.keras'))\n",
    "\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_densenet.npy'), y_pred_1d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_densenet.npy'), y_pred_2d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_resnet.npy'), y_pred_1d_resnet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_resnet.npy'), y_pred_2d_resnet)\n",
    "\n",
    "metrics = {\n",
    "    'densenet_1d': {'accuracy': densenet_1d_acc, 'precision': densenet_1d_precision, 'recall': densenet_1d_recall, 'f1': densenet_1d_f1},\n",
    "    'densenet_2d': {'accuracy': densenet_2d_acc, 'precision': densenet_2d_precision, 'recall': densenet_2d_recall, 'f1': densenet_2d_f1},\n",
    "    'resnet_1d': {'accuracy': resnet_1d_acc, 'precision': resnet_1d_precision, 'recall': resnet_1d_recall, 'f1': resnet_1d_f1},\n",
    "    'resnet_2d': {'accuracy': resnet_2d_acc, 'precision': resnet_2d_precision, 'recall': resnet_2d_recall, 'f1': resnet_2d_f1}\n",
    "}\n",
    "with open(os.path.join(results_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
