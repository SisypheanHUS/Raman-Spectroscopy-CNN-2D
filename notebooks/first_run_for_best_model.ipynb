{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First Run For Best Model",
   "id": "2cbfd4f0ffd8669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.optimize import curve_fit\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, regularizers\n",
    "import pyts\n",
    "from pyts.transformation.transformation import GADF\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a fixed seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Create experiment directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join('experiments', f'experiment_{timestamp}')\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "model_dir = os.path.join(experiment_dir, 'models')\n",
    "results_dir = os.path.join(experiment_dir, 'results')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Define directories\n",
    "data_dir = 'data'\n",
    "synthetic_dir = os.path.join(data_dir, 'synthetic')\n",
    "maps_dir = os.path.join(data_dir, 'maps')\n",
    "labels_dir = os.path.join(data_dir, 'labels')\n",
    "visualizations_dir = os.path.join(data_dir, 'visualizations')\n",
    "problematic_spectra_dir = os.path.join(data_dir, 'problematic_spectra')\n",
    "os.makedirs(synthetic_dir, exist_ok=True)\n",
    "os.makedirs(maps_dir, exist_ok=True)\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "os.makedirs(visualizations_dir, exist_ok=True)\n",
    "os.makedirs(problematic_spectra_dir, exist_ok=True)\n",
    "\n",
    "# Save experiment configuration\n",
    "config = {\n",
    "    'seed': SEED,\n",
    "    'spectrum_length': 880,\n",
    "    'image_size': 64,\n",
    "    'num_synthetic_per_type': 999,\n",
    "    'num_types': 11,\n",
    "    'total_spectra': 11000,\n",
    "    'epochs': 10,\n",
    "    'batch_size_1d': 64,\n",
    "    'batch_size_2d': 32,\n",
    "    'validation_split': 0.1,\n",
    "    'test_size': 0.2,\n",
    "    'growth_rate': 12,\n",
    "    'num_classes': 11,\n",
    "    'experiment_timestamp': timestamp\n",
    "}\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "# Baseline functions\n",
    "def poly_baseline(x, p, intensity, b):\n",
    "    y = (x / len(x)) ** p + b\n",
    "    return y * intensity / max(y)\n",
    "\n",
    "def gaussian_baseline(x, mean, sd, intensity, b):\n",
    "    y = np.exp(-(x - mean) ** 2 / (2 * sd ** 2)) / (sd * np.sqrt(2 * np.pi)) + b\n",
    "    return y * intensity / max(y)\n",
    "\n",
    "def pg_baseline(x, p, in1, mean, sd, in2, b):\n",
    "    y1 = (x / len(x)) ** p + b\n",
    "    y2 = np.exp(-(x - mean) ** 2 / (2 * sd ** 2)) / (sd * np.sqrt(2 * np.pi)) + b\n",
    "    return y1 / max(y1) * in1 + y2 / max(y2) * in2\n",
    "\n",
    "def mix_min_no(sp, baseline):\n",
    "    return np.minimum(baseline, sp)\n",
    "\n",
    "def iterative_fitting_with_bounds_no(sp, model, ite=10):\n",
    "    fitted_baseline = np.zeros(sp.shape[0])\n",
    "    x = np.linspace(1, sp.shape[0], sp.shape[0])\n",
    "    tempb = sp\n",
    "    torch_tempb = tf.expand_dims(tempb, axis=0)\n",
    "    i = 0\n",
    "    while i < ite:\n",
    "        tadvice = model(torch_tempb)\n",
    "        if tadvice[0][0] >= 0.5 and tadvice[0][1] >= 0.5:\n",
    "            try:\n",
    "                p, c = curve_fit(pg_baseline, x, tempb,\n",
    "                                bounds=([1, 0.5, 0, 100, 0.5, -0.5], [3, 1, sp.shape[0], 600, 1, 0.5]),\n",
    "                                maxfev=10000)\n",
    "                fitted_baseline = pg_baseline(x, p[0], p[1], p[2], p[3], p[4], p[5])\n",
    "            except RuntimeError:\n",
    "                fitted_baseline = tempb\n",
    "        elif tadvice[0][0] >= 0.5:\n",
    "            try:\n",
    "                p, c = curve_fit(poly_baseline, x, tempb,\n",
    "                                bounds=([1, 0.5, -0.5], [3, 1, 0.5]),\n",
    "                                maxfev=10000)\n",
    "                fitted_baseline = poly_baseline(x, p[0], p[1], p[2])\n",
    "            except RuntimeError:\n",
    "                fitted_baseline = tempb\n",
    "        elif tadvice[0][1] >= 0.5:\n",
    "            try:\n",
    "                p, c = curve_fit(gaussian_baseline, x, tempb,\n",
    "                                bounds=([0, 100, 0.5, -0.5], [sp.shape[0], 600, 1, 0.5]),\n",
    "                                maxfev=10000)\n",
    "                fitted_baseline = gaussian_baseline(x, p[0], p[1], p[2], p[3])\n",
    "            except RuntimeError:\n",
    "                fitted_baseline = tempb\n",
    "        tempb = mix_min_no(tempb, fitted_baseline)\n",
    "        tempb_np = np.array(tempb)\n",
    "        torch_tempb = tempb_np.reshape(1, sp.shape[0])\n",
    "        i += 1\n",
    "    return tempb\n",
    "\n",
    "def create_baseline_model(input_shape=880):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Reshape((input_shape, 1)),\n",
    "        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),\n",
    "        layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_baseline_model(baseline_model, noise_data, epochs=10, batch_size=32):\n",
    "    # Load labels from labels_noise_pure_182.npy\n",
    "    try:\n",
    "        labels = np.load(os.path.join(data_dir, 'labels_noise_pure_182.npy'))\n",
    "        print(\"Đã tải nhãn từ labels_noise_pure_182.npy thành công!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải nhãn: {e}. Sử dụng nhãn ngẫu nhiên.\")\n",
    "        labels = np.random.randint(0, 2, size=noise_data.shape[0])\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(noise_data.shape[0]):\n",
    "        pure = noise_data[i, 0, 0, :, 0]\n",
    "        noisy = noise_data[i, 0, 1, :, 0]\n",
    "        X.append(noisy)\n",
    "        y.append(labels[i])  # Sử dụng nhãn thực\n",
    "    X = np.array(X)[:, :, np.newaxis]\n",
    "    y = np.array(y)\n",
    "    baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    baseline_model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    baseline_model.save_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    return baseline_model\n",
    "\n",
    "# Normalize spectrum\n",
    "def normalize_spectrum(spectrum):\n",
    "    spectrum = spectrum - np.min(spectrum)\n",
    "    if np.max(spectrum) > 0:\n",
    "        spectrum = spectrum / np.max(spectrum)\n",
    "    return spectrum\n",
    "\n",
    "# WhittakerSmooth and airPLS\n",
    "def WhittakerSmooth(x, w, lambda_=1, differences=1):\n",
    "    X = np.matrix(x)\n",
    "    m = X.size\n",
    "    E = eye(m, format='csc')\n",
    "    for i in range(differences):\n",
    "        E = E[1:] - E[:-1]\n",
    "    W = diags(w, 0, shape=(m, m))\n",
    "    A = csc_matrix(W + (lambda_ * E.T * E))\n",
    "    B = csc_matrix(W * X.T)\n",
    "    background = spsolve(A, B)\n",
    "    return np.array(background)\n",
    "\n",
    "def airPLS(x, lambda_=100, porder=1, itermax=15):\n",
    "    m = x.shape[0]\n",
    "    w = np.ones(m)\n",
    "    lambda_ = max(50, min(500, 50 * np.std(x) / (np.mean(np.abs(x)) + 1e-6)))\n",
    "    for i in range(1, itermax + 1):\n",
    "        z = WhittakerSmooth(x, w, lambda_, porder)\n",
    "        d = x - z\n",
    "        dssn = np.abs(d[d < 0].sum())\n",
    "        if dssn < 0.001 * np.abs(x).sum():\n",
    "            return z\n",
    "        if i == itermax:\n",
    "            print(f'WARNING: Max iteration reached! lambda_={lambda_:.2f}, dssn={dssn:.2e}')\n",
    "            np.save(os.path.join(problematic_spectra_dir, f'problematic_spectrum_{np.random.randint(1000000)}.npy'), x)\n",
    "            return WhittakerSmooth(x, np.ones(m), lambda_=50)\n",
    "        w[d >= 0] = 0\n",
    "        w[d < 0] = np.exp(i * np.abs(d[d < 0]) / dssn)\n",
    "        w[0] = np.exp(i * (d[d < 0]).max() / dssn)\n",
    "        w[-1] = w[0]\n",
    "    return z\n",
    "\n",
    "# Enhanced baseline correction\n",
    "def enhanced_baseline_correction(spectrum, baseline_model):\n",
    "    \"\"\"\n",
    "    Trừ nền: Dùng baseline_model để đoán và trừ nền sơ bộ (đa thức/Gaussian),\n",
    "    sau đó dùng airPLS để tinh chỉnh. Nếu lỗi, dùng airPLS trực tiếp.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        baseline = iterative_fitting_with_bounds_no(spectrum, baseline_model)\n",
    "        fine_corrected = airPLS(baseline, lambda_=100, itermax=15)\n",
    "        return np.clip(spectrum - fine_corrected, 0, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in baseline correction: {e}. Falling back to airPLS.\")\n",
    "        return np.clip(spectrum - airPLS(spectrum, lambda_=100, itermax=15), 0, None)\n",
    "\n",
    "# Interpolate spectrum\n",
    "def interpolate_spectrum(spectrum, original_length, target_length=880):\n",
    "    x_original = np.linspace(0, original_length - 1, original_length)\n",
    "    x_target = np.linspace(0, original_length - 1, target_length)\n",
    "    interpolator = interp1d(x_original, spectrum, kind='linear', fill_value=\"extrapolate\")\n",
    "    return interpolator(x_target)\n",
    "\n",
    "# Shift spectrum\n",
    "def shift_spectrum(spectrum, shift):\n",
    "    return np.roll(spectrum, shift)\n",
    "\n",
    "# Stretch spectrum\n",
    "def stretch_spectrum(spectrum, alpha):\n",
    "    original_len = len(spectrum)\n",
    "    new_len = int(original_len / alpha)\n",
    "    if new_len < 1:\n",
    "        new_len = 1\n",
    "    if new_len > original_len * 10:\n",
    "        new_len = original_len * 10\n",
    "    x_original = np.linspace(0, original_len - 1, original_len)\n",
    "    x_new = np.linspace(0, original_len - 1, new_len)\n",
    "    interpolator = interp1d(x_original, spectrum, kind='linear', fill_value=\"extrapolate\")\n",
    "    stretched = interpolator(x_new)\n",
    "    return interpolate_spectrum(stretched, new_len, original_len)\n",
    "\n",
    "# Generate synthetic spectrum\n",
    "def generate_synthetic_spectrum(input_spectrum, noise_data, spectrum_length=880):\n",
    "    \"\"\"\n",
    "    Tạo phổ tổng hợp từ một phổ gốc duy nhất:\n",
    "    - Thêm nhiễu từ noise_data và nhiễu Gaussian.\n",
    "    - Thêm đường nền (đa thức/Gaussian hoặc không).\n",
    "    - Dịch chuyển hoặc kéo dãn/nén.\n",
    "    - Không trộn với phổ khác.\n",
    "    \"\"\"\n",
    "    x_range = np.linspace(0, spectrum_length, spectrum_length)\n",
    "    # Add noise from dataset\n",
    "    noise_idx = np.random.randint(0, noise_data.shape[0])\n",
    "    pure = noise_data[noise_idx, 0, 0, :, 0]\n",
    "    noisy = noise_data[noise_idx, 0, 1, :, 0]\n",
    "    noise = noisy - pure  # Shape (880,)\n",
    "    scale = np.random.uniform(1.0, 2.0)\n",
    "    synthetic_spectrum = input_spectrum + noise * scale\n",
    "    # Add Gaussian noise\n",
    "    synthetic_spectrum += np.random.normal(0, 0.05 * np.std(input_spectrum), spectrum_length)\n",
    "    # Add synthetic baseline\n",
    "    baseline_type = np.random.choice(['poly', 'gaussian', 'none'], p=[0.3, 0.3, 0.4])\n",
    "    if baseline_type == 'poly':\n",
    "        baseline = poly_baseline(x_range, p=np.random.uniform(1.9, 2.1),\n",
    "                                intensity=np.random.uniform(0.75, 0.8),\n",
    "                                b=np.random.uniform(-0.1, 0.1))\n",
    "        synthetic_spectrum += baseline\n",
    "    elif baseline_type == 'gaussian':\n",
    "        baseline = gaussian_baseline(x_range, mean=np.random.uniform(0, spectrum_length),\n",
    "                                   sd=np.random.uniform(250, 300),\n",
    "                                   intensity=np.random.uniform(0.75, 0.8),\n",
    "                                   b=np.random.uniform(-0.1, 0.1))\n",
    "        synthetic_spectrum += baseline\n",
    "    # Probabilistic augmentation\n",
    "    aug_type = np.random.choice(['none', 'shift', 'stretch'], p=[0.5, 0.25, 0.25])\n",
    "    if aug_type == 'shift':\n",
    "        shift = np.random.randint(-10, 11)\n",
    "        synthetic_spectrum = shift_spectrum(synthetic_spectrum, shift)\n",
    "    elif aug_type == 'stretch':\n",
    "        alpha = np.random.uniform(0.5, 2.0)\n",
    "        synthetic_spectrum = stretch_spectrum(synthetic_spectrum, alpha)\n",
    "    return synthetic_spectrum\n",
    "\n",
    "# Create GADF map\n",
    "def create_gadf_map(spectrum, image_size=64):\n",
    "    spectrum = normalize_spectrum(spectrum)\n",
    "    spectrum = 2 * spectrum - 1\n",
    "    target_length = image_size * (spectrum.shape[0] // image_size)\n",
    "    if target_length != spectrum.shape[0]:\n",
    "        spectrum = interpolate_spectrum(spectrum, spectrum.shape[0], target_length)\n",
    "    gadf = GADF(image_size=image_size, overlapping=False, scale='-1')\n",
    "    return gadf.fit_transform(spectrum.reshape(1, -1))[0][:, :, np.newaxis]\n",
    "\n",
    "# Load Excel data\n",
    "def load_excel_data():\n",
    "    excel_path = os.path.join(data_dir, 'Ethanol_Methanol.xlsx')\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, usecols='A:L')\n",
    "        raman_shift = df['Raman Shift (cm-1)'].values\n",
    "        spectra_data = {\n",
    "            'ethanol': df['Ethanol'].values,\n",
    "            'methanol': df['Methanol'].values,\n",
    "            'EM1_a': df['EM1_a'].values,\n",
    "            'EM2_a': df['EM2_a'].values,\n",
    "            'EM3_a': df['EM3_a'].values,\n",
    "            'EM4_a': df['EM4_a'].values,\n",
    "            'EM5_a': df['EM5_a'].values,\n",
    "            'EM6_a': df['EM6_a'].values,\n",
    "            'EM7_a': df['EM7_a'].values,\n",
    "            'EM8_a': df['EM8_a'].values,\n",
    "            'EM9_a': df['EM9_a'].values\n",
    "        }\n",
    "        for key in spectra_data:\n",
    "            spectrum = spectra_data[key]\n",
    "            spectrum = spectrum[~np.isnan(spectrum)]\n",
    "            spectra_data[key] = interpolate_spectrum(spectrum, len(spectrum), 880)\n",
    "        return spectra_data, raman_shift\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải dữ liệu từ {excel_path}: {e}\")\n",
    "        return {}, np.array([])\n",
    "\n",
    "# Load noise data\n",
    "def load_noise_data():\n",
    "    try:\n",
    "        # Noise data shape: (15000, 1, 2, 880, 1) - 15000 samples, 2 spectra (pure, noisy), 880 points\n",
    "        noise_data = np.load(os.path.join(data_dir, 'dataset_noise_pure_182.npy'))\n",
    "        return noise_data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải dữ liệu nhiễu: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# Process data\n",
    "spectra_data, raman_shift = load_excel_data()\n",
    "noise_data = load_noise_data()\n",
    "if not spectra_data or noise_data.size == 0:\n",
    "    raise FileNotFoundError(\"Không thể tải dữ liệu từ file Excel hoặc file nhiễu.\")\n",
    "\n",
    "# Initialize and train baseline model\n",
    "baseline_model = create_baseline_model(input_shape=880)\n",
    "try:\n",
    "    baseline_model.load_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    print(\"Trọng số mô hình baseline đã được tải thành công!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tải trọng số: {e}. Training baseline model.\")\n",
    "    baseline_model = train_baseline_model(baseline_model, noise_data)\n",
    "\n",
    "# Generate synthetic data\n",
    "X_1d = []\n",
    "X_2d = []\n",
    "labels = []\n",
    "sample_ids = []\n",
    "example_spectra = {}\n",
    "total_spectra = 0\n",
    "spectrum_length = 880\n",
    "\n",
    "ratio_to_label = {0.0: 0, 0.1: 1, 0.2: 2, 0.3: 3, 0.4: 4, 0.5: 5, 0.6: 6, 0.7: 7, 0.8: 8, 0.9: 9, 1.0: 10}\n",
    "ratios = {\n",
    "    'ethanol': 1.0,\n",
    "    'methanol': 0.0,\n",
    "    'EM1_a': 0.9,\n",
    "    'EM2_a': 0.8,\n",
    "    'EM3_a': 0.7,\n",
    "    'EM4_a': 0.6,\n",
    "    'EM5_a': 0.5,\n",
    "    'EM6_a': 0.4,\n",
    "    'EM7_a': 0.3,\n",
    "    'EM8_a': 0.2,\n",
    "    'EM9_a': 0.1\n",
    "}\n",
    "\n",
    "print(\"Bắt đầu sinh dữ liệu tổng hợp...\")\n",
    "for spectrum_type, ethanol_ratio in ratios.items():\n",
    "    print(f\"Xử lý {spectrum_type} với ethanol ratio {ethanol_ratio}...\")\n",
    "    input_spectrum = spectra_data[spectrum_type]\n",
    "    normalized_raw = normalize_spectrum(input_spectrum)\n",
    "    corrected_spectrum = enhanced_baseline_correction(normalized_raw, baseline_model)\n",
    "    corrected_spectrum = normalize_spectrum(corrected_spectrum)\n",
    "    X_1d.append(corrected_spectrum)\n",
    "    X_2d.append(create_gadf_map(corrected_spectrum, image_size=64))\n",
    "    label_idx = ratio_to_label[ethanol_ratio]\n",
    "    labels.append(label_idx)\n",
    "    sample_ids.append(f\"{spectrum_type}_original\")\n",
    "    total_spectra += 1\n",
    "    if total_spectra == 1 or ethanol_ratio not in example_spectra:\n",
    "        example_spectra[ethanol_ratio] = {\n",
    "            'raw': normalized_raw,\n",
    "            'corrected': corrected_spectrum,\n",
    "            'ethanol': spectra_data['ethanol'] if ethanol_ratio > 0 else None,\n",
    "            'methanol': spectra_data['methanol'] if ethanol_ratio < 1 else None\n",
    "        }\n",
    "    for i in range(999):\n",
    "        synthetic_spectrum = generate_synthetic_spectrum(normalized_raw, noise_data, spectrum_length)\n",
    "        corrected_spectrum = enhanced_baseline_correction(synthetic_spectrum, baseline_model)\n",
    "        corrected_spectrum = normalize_spectrum(corrected_spectrum)\n",
    "        X_1d.append(corrected_spectrum)\n",
    "        X_2d.append(create_gadf_map(corrected_spectrum, image_size=64))\n",
    "        labels.append(label_idx)\n",
    "        sample_ids.append(f\"{spectrum_type}_synthetic_{i}\")\n",
    "        total_spectra += 1\n",
    "        if i == 0:\n",
    "            example_spectra[ethanol_ratio] = {\n",
    "                'raw': synthetic_spectrum,\n",
    "                'corrected': corrected_spectrum,\n",
    "                'ethanol': spectra_data['ethanol'] if ethanol_ratio > 0 else None,\n",
    "                'methanol': spectra_data['methanol'] if ethanol_ratio < 1 else None\n",
    "            }\n",
    "        if total_spectra % 1000 == 0:\n",
    "            print(f\"Đã xử lý {total_spectra} phổ.\")\n",
    "\n",
    "print(f\"Tổng số phổ: {total_spectra}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_1d = np.array(X_1d)[:, :, np.newaxis]\n",
    "X_2d = np.array(X_2d)\n",
    "labels_df = pd.DataFrame({'label': labels, 'sample_id': sample_ids})\n",
    "\n",
    "print(\"X_1d shape:\", X_1d.shape)\n",
    "print(\"X_2d shape:\", X_2d.shape)\n",
    "print(\"labels_df shape:\", labels_df.shape)\n",
    "print(\"Label distribution:\\n\", labels_df[\"label\"].value_counts())\n",
    "\n",
    "# Save data\n",
    "labels_df.to_csv(os.path.join(labels_dir, \"labels.csv\"), index=False)\n",
    "np.save(os.path.join(synthetic_dir, \"synthetic_1d.npy\"), X_1d)\n",
    "np.save(os.path.join(maps_dir, \"spectral_maps_gadf.npy\"), X_2d)\n",
    "\n",
    "# Plot spectra\n",
    "def plot_spectra_comparison(wavenumbers, raw_spectrum, corrected_spectrum, ethanol_spectrum, methanol_spectrum, title, filename):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(wavenumbers, raw_spectrum, label=\"Raw Spectrum\", color='blue')\n",
    "    plt.axvspan(1000, 1020, color='red', alpha=0.2, label=\"Vùng Methanol\")\n",
    "    plt.axvspan(870, 890, color='green', alpha=0.2, label=\"Vùng Ethanol\")\n",
    "    plt.xlabel(\"Bước sóng (cm⁻¹)\")\n",
    "    plt.ylabel(\"Cường độ chuẩn hóa\")\n",
    "    plt.title(f\"Phổ Raw ({title})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(wavenumbers, corrected_spectrum, label=\"Corrected Spectrum\", color='orange')\n",
    "    if ethanol_spectrum is not None:\n",
    "        plt.plot(wavenumbers, normalize_spectrum(ethanol_spectrum), label=\"Ethanol Component\", color='green', linestyle='--')\n",
    "    if methanol_spectrum is not None:\n",
    "        plt.plot(wavenumbers, normalize_spectrum(methanol_spectrum), label=\"Methanol Component\", color='red', linestyle='--')\n",
    "    plt.axvspan(1000, 1020, color='red', alpha=0.2)\n",
    "    plt.axvspan(870, 890, color='green', alpha=0.2)\n",
    "    plt.xlabel(\"Bước sóng (cm⁻¹)\")\n",
    "    plt.ylabel(\"Cường độ chuẩn hóa\")\n",
    "    plt.title(f\"Phổ Sau Trừ Nền và Thành Phần ({title})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Sử dụng phạm vi wavenumbers từ 500–3500 cm⁻¹\n",
    "wavenumbers = np.linspace(500, 3500, 880)\n",
    "for ratio in example_spectra:\n",
    "    title = f\"Ratio Ethanol/Methanol = {ratio:.2f}/{1-ratio:.2f}\" if ratio < 1.0 else \"Pure Ethanol\" if ratio == 1.0 else \"Pure Methanol\"\n",
    "    plot_spectra_comparison(\n",
    "        wavenumbers,\n",
    "        example_spectra[ratio]['raw'],\n",
    "        example_spectra[ratio]['corrected'],\n",
    "        example_spectra[ratio]['ethanol'],\n",
    "        example_spectra[ratio]['methanol'],\n",
    "        title,\n",
    "        f\"spectra_comparison_ratio_{ratio:.2f}.png\"\n",
    "    )\n",
    "\n",
    "# Define DenseNet models\n",
    "def build_1d_densenet(input_shape=(880, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(48, 7, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv1D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv1D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_densenet(input_shape=(64, 64, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(48, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Define ResNet models\n",
    "def build_1d_resnet(input_shape=(880, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, 5, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_resnet(input_shape=(64, 64, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=[f\"{i*10}% Ethanol\" for i in range(11)],\n",
    "                yticklabels=[f\"{i*10}% Ethanol\" for i in range(11)])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation_1d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "data_augmentation_2d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "y = labels_df[\"label\"].values\n",
    "X_1d_train, X_1d_test, y_train, y_test = train_test_split(X_1d, y, test_size=0.2, random_state=42)\n",
    "X_2d_train, X_2d_test, y_train_2d, y_test_2d = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.arange(11), y=y)\n",
    "class_weight = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# Train models with fresh optimizers\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# DenseNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_densenet)\n",
    "densenet_1d = models.Sequential([data_augmentation_1d, build_1d_densenet()])\n",
    "densenet_1d.compile(optimizer=optimizer_1d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# DenseNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_densenet)\n",
    "densenet_2d = models.Sequential([data_augmentation_2d, build_2d_densenet()])\n",
    "densenet_2d.compile(optimizer=optimizer_2d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# ResNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_resnet)\n",
    "resnet_1d = models.Sequential([data_augmentation_1d, build_1d_resnet()])\n",
    "resnet_1d.compile(optimizer=optimizer_1d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# ResNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_resnet)\n",
    "resnet_2d = models.Sequential([data_augmentation_2d, build_2d_resnet()])\n",
    "resnet_2d.compile(optimizer=optimizer_2d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_1d_densenet = densenet_1d.predict(X_1d_test)\n",
    "y_pred_2d_densenet = densenet_2d.predict(X_2d_test)\n",
    "y_pred_1d_resnet = resnet_1d.predict(X_1d_test)\n",
    "y_pred_2d_resnet = resnet_2d.predict(X_2d_test)\n",
    "\n",
    "y_pred_1d_densenet_labels = np.argmax(y_pred_1d_densenet, axis=1)\n",
    "y_pred_2d_densenet_labels = np.argmax(y_pred_2d_densenet, axis=1)\n",
    "y_pred_1d_resnet_labels = np.argmax(y_pred_1d_resnet, axis=1)\n",
    "y_pred_2d_resnet_labels = np.argmax(y_pred_2d_resnet, axis=1)\n",
    "\n",
    "densenet_1d_acc = np.mean(y_pred_1d_densenet_labels == y_test)\n",
    "densenet_2d_acc = np.mean(y_pred_2d_densenet_labels == y_test_2d)\n",
    "resnet_1d_acc = np.mean(y_pred_1d_resnet_labels == y_test)\n",
    "resnet_2d_acc = np.mean(y_pred_2d_resnet_labels == y_test_2d)\n",
    "\n",
    "densenet_1d_precision = precision_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_precision = precision_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_precision = precision_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_precision = precision_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_recall = recall_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_recall = recall_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_recall = recall_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_recall = recall_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_f1 = f1_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_f1 = f1_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_f1 = f1_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_f1 = f1_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "print(\"\\nKết quả đánh giá:\")\n",
    "print(f\"DenseNet 1D - Accuracy: {densenet_1d_acc:.4f}, Precision: {densenet_1d_precision:.4f}, Recall: {densenet_1d_recall:.4f}, F1: {densenet_1d_f1:.4f}\")\n",
    "print(f\"DenseNet 2D (GADF) - Accuracy: {densenet_2d_acc:.4f}, Precision: {densenet_2d_precision:.4f}, Recall: {densenet_2d_recall:.4f}, F1: {densenet_2d_f1:.4f}\")\n",
    "print(f\"ResNet 1D - Accuracy: {resnet_1d_acc:.4f}, Precision: {resnet_1d_precision:.4f}, Recall: {resnet_1d_recall:.4f}, F1: {resnet_1d_f1:.4f}\")\n",
    "print(f\"ResNet 2D (GADF) - Accuracy: {resnet_2d_acc:.4f}, Precision: {resnet_2d_precision:.4f}, Recall: {resnet_2d_recall:.4f}, F1: {resnet_2d_f1:.4f}\")\n",
    "\n",
    "# Save results\n",
    "plot_confusion_matrix(y_test, y_pred_1d_densenet_labels, \"Confusion Matrix - DenseNet 1D\", \"cm_densenet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_densenet_labels, \"Confusion Matrix - DenseNet 2D (GADF)\", \"cm_densenet_2d_gadf.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_1d_resnet_labels, \"Confusion Matrix - ResNet 1D\", \"cm_resnet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_resnet_labels, \"Confusion Matrix - ResNet 2D (GADF)\", \"cm_resnet_2d_gadf.png\")\n",
    "\n",
    "densenet_1d.save(os.path.join(model_dir, 'densenet_1d_full.keras'))\n",
    "densenet_2d.save(os.path.join(model_dir, 'densenet_2d_full.keras'))\n",
    "resnet_1d.save(os.path.join(model_dir, 'resnet_1d_full.keras'))\n",
    "resnet_2d.save(os.path.join(model_dir, 'resnet_2d_full.keras'))\n",
    "\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_densenet.npy'), y_pred_1d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_densenet.npy'), y_pred_2d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_resnet.npy'), y_pred_1d_resnet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_resnet.npy'), y_pred_2d_resnet)\n",
    "\n",
    "metrics = {\n",
    "    'densenet_1d': {'accuracy': densenet_1d_acc, 'precision': densenet_1d_precision, 'recall': densenet_1d_recall, 'f1': densenet_1d_f1},\n",
    "    'densenet_2d': {'accuracy': densenet_2d_acc, 'precision': densenet_2d_precision, 'recall': densenet_2d_recall, 'f1': densenet_2d_f1},\n",
    "    'resnet_1d': {'accuracy': resnet_1d_acc, 'precision': resnet_1d_precision, 'recall': resnet_1d_recall, 'f1': resnet_1d_f1},\n",
    "    'resnet_2d': {'accuracy': resnet_2d_acc, 'precision': resnet_2d_precision, 'recall': resnet_2d_recall, 'f1': resnet_2d_f1}\n",
    "}\n",
    "with open(os.path.join(results_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ],
   "id": "8a999f5cf4fdf111"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
