{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raman Spectroscopy Fake Alcohol Detection - Demo\n",
    "\n",
    "This notebook demonstrates the classification pipeline using pre-processed data.\n",
    "\n",
    "**For full data generation pipeline, see `main_pipeline.ipynb`**\n",
    "\n",
    "## Pipeline:\n",
    "1. Load pre-processed data (synthetic_1d.npy, spectral_maps_gadf.npy)\n",
    "2. Train DenseNet/ResNet models\n",
    "3. Evaluate with confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seeds\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "data_dir = '../data'\n",
    "synthetic_dir = os.path.join(data_dir, 'synthetic')\n",
    "maps_dir = os.path.join(data_dir, 'maps')\n",
    "labels_dir = os.path.join(data_dir, 'labels')\n",
    "visualizations_dir = os.path.join(data_dir, 'visualizations')\n",
    "\n",
    "# Load data\n",
    "X_1d = np.load(os.path.join(synthetic_dir, 'synthetic_1d.npy'))\n",
    "X_2d = np.load(os.path.join(maps_dir, 'spectral_maps_gadf.npy'))\n",
    "labels_df = pd.read_csv(os.path.join(labels_dir, 'labels.csv'))\n",
    "y = labels_df['label'].values\n",
    "\n",
    "print(f\"X_1d shape: {X_1d.shape}\")\n",
    "print(f\"X_2d shape: {X_2d.shape}\")\n",
    "print(f\"Labels: {len(y)} samples, {len(np.unique(y))} classes\")\n",
    "print(f\"\\nLabel distribution:\\n{labels_df['label'].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample spectra\n",
    "wavenumbers = np.linspace(500, 3500, 880)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(5):\n",
    "    idx = np.random.randint(0, X_1d.shape[0])\n",
    "    plt.plot(wavenumbers, X_1d[idx, :, 0], alpha=0.7, label=f'Label {y[idx]} ({y[idx]*10}% Ethanol)')\n",
    "\n",
    "plt.xlabel('Wavenumber (cm$^{-1}$)')\n",
    "plt.ylabel('Intensity (normalized)')\n",
    "plt.title('Sample Raman Spectra')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvspan(870, 890, color='green', alpha=0.2, label='Ethanol peak')\n",
    "plt.axvspan(1000, 1020, color='red', alpha=0.2, label='Methanol peak')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architectures\n",
    "\n",
    "Define DenseNet and ResNet models for both 1D (spectra) and 2D (GADF) inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_1d_densenet(input_shape=(880, 1), num_classes=11, growth_rate=12):\n",
    "    \"\"\"DenseNet for 1D spectral classification\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(48, 7, padding='same', activation='relu', \n",
    "                      kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv1D(filters, 3, padding='same', \n",
    "                              kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    \n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv1D(filters // 2, 1, padding='same', \n",
    "                          kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        return x\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def build_2d_densenet(input_shape=(64, 64, 1), num_classes=11, growth_rate=12):\n",
    "    \"\"\"DenseNet for 2D GADF image classification\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(48, 3, padding='same', activation='relu', \n",
    "                      kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv2D(filters, 3, padding='same', \n",
    "                              kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    \n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters // 2, 1, padding='same', \n",
    "                          kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "    \n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def build_1d_resnet(input_shape=(880, 1), num_classes=11):\n",
    "    \"\"\"ResNet for 1D spectral classification\"\"\"\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, 5, padding='same', activation='relu', \n",
    "                      kernel_regularizer=regularizers.l2(0.0001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', \n",
    "                          kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', \n",
    "                          kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "print(\"Model architectures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layers\n",
    "data_augmentation_1d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_1d_train, X_1d_test, y_train, y_test = train_test_split(\n",
    "    X_1d, y, test_size=0.2, random_state=42\n",
    ")\n",
    "X_2d_train, X_2d_test, y_train_2d, y_test_2d = train_test_split(\n",
    "    X_2d, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "class_weights = compute_class_weight('balanced', classes=np.arange(11), y=y)\n",
    "class_weight = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "print(f\"Train: {len(y_train)} samples\")\n",
    "print(f\"Test: {len(y_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Model (DenseNet 1D Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join('../experiments', f'experiment_{timestamp}')\n",
    "model_dir = os.path.join(experiment_dir, 'models')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Build model with augmentation\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=0.001, \n",
    "    decay_steps=10 * len(X_1d_train) // 64\n",
    ")\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "densenet_1d = models.Sequential([\n",
    "    data_augmentation_1d,\n",
    "    build_1d_densenet()\n",
    "])\n",
    "densenet_1d.compile(\n",
    "    optimizer=optimizer, \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    os.path.join(model_dir, 'best_densenet_1d.keras'), \n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "print(\"Model ready for training!\")\n",
    "print(f\"Experiment dir: {experiment_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (reduce epochs for demo - use 50-100 for production)\n",
    "history = densenet_1d.fit(\n",
    "    X_1d_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,  # Increase for better results\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    class_weight=class_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred = densenet_1d.predict(X_1d_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Metrics\n",
    "accuracy = np.mean(y_pred_labels == y_test)\n",
    "precision = precision_score(y_test, y_pred_labels, average='macro')\n",
    "recall = recall_score(y_test, y_pred_labels, average='macro')\n",
    "f1 = f1_score(y_test, y_pred_labels, average='macro')\n",
    "\n",
    "print(f\"\\nDenseNet 1D Results:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_labels)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
    "    xticklabels=[f\"{i*10}%\" for i in range(11)],\n",
    "    yticklabels=[f\"{i*10}%\" for i in range(11)],\n",
    "    cbar_kws={'label': '%'}\n",
    ")\n",
    "plt.xlabel('Predicted Ethanol %', fontsize=12)\n",
    "plt.ylabel('True Ethanol %', fontsize=12)\n",
    "plt.title('Normalized Confusion Matrix - DenseNet 1D', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "densenet_1d.save(os.path.join(model_dir, 'densenet_1d_full.keras'))\n",
    "print(f\"Model saved to {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This demo showed:\n",
    "1. Loading pre-processed data (1D spectra + 2D GADF maps)\n",
    "2. Building DenseNet/ResNet architectures\n",
    "3. Training with data augmentation and class weighting\n",
    "4. Evaluation with confusion matrices\n",
    "\n",
    "**For full pipeline including:**\n",
    "- Data loading from Excel\n",
    "- Baseline correction (airPLS)\n",
    "- Synthetic data generation\n",
    "- GADF transformation\n",
    "- Training all 4 models\n",
    "- Occlusion analysis\n",
    "\n",
    "**See `main_pipeline.ipynb`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
