{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First Run For Best Model",
   "id": "2cbfd4f0ffd8669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.optimize import curve_fit\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, regularizers\n",
    "import pyts\n",
    "from pyts.image import GramianAngularField as GADF\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a fixed seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Create experiment directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join('/kaggle/working/experiments', f'experiment_{timestamp}')\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "model_dir = os.path.join(experiment_dir, 'models')\n",
    "results_dir = os.path.join(experiment_dir, 'results')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# INPUT directory (read-only)\n",
    "data_dir = '/kaggle/input/ethanol-methanol/data'\n",
    "\n",
    "# OUTPUT directories (writable)\n",
    "output_dir = '/kaggle/working'\n",
    "synthetic_dir = os.path.join(output_dir, 'synthetic')\n",
    "maps_dir = os.path.join(output_dir, 'maps')\n",
    "labels_dir = os.path.join(output_dir, 'labels')\n",
    "visualizations_dir = os.path.join(output_dir, 'visualizations')\n",
    "problematic_spectra_dir = os.path.join(output_dir, 'problematic_spectra')\n",
    "os.makedirs(synthetic_dir, exist_ok=True)\n",
    "os.makedirs(maps_dir, exist_ok=True)\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "os.makedirs(visualizations_dir, exist_ok=True)\n",
    "os.makedirs(problematic_spectra_dir, exist_ok=True)\n",
    "\n",
    "# Save experiment configuration\n",
    "config = {\n",
    "    'seed': SEED,\n",
    "    'spectrum_length': 880,\n",
    "    'image_size': 64,\n",
    "    'num_synthetic_per_type': 999,\n",
    "    'num_types': 11,\n",
    "    'total_spectra': 11000,\n",
    "    'epochs': 10,\n",
    "    'batch_size_1d': 64,\n",
    "    'batch_size_2d': 32,\n",
    "    'validation_split': 0.1,\n",
    "    'test_size': 0.2,\n",
    "    'growth_rate': 12,\n",
    "    'num_classes': 11,\n",
    "    'experiment_timestamp': timestamp\n",
    "}\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n"
   ],
   "id": "8a999f5cf4fdf111"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ============================================================================\n# CONSTANTS: Chemically Meaningful Raman Bands\n# ============================================================================\nRAMAN_BANDS = {\n    'ethanol_cc': {'range': (870, 890), 'name': 'Ethanol C-C stretch'},\n    'methanol_co': {'range': (1020, 1055), 'name': 'Methanol C-O stretch'},\n    'ethanol_co': {'range': (1055, 1100), 'name': 'Ethanol C-O stretch'},\n    'ch_bend': {'range': (1450, 1480), 'name': 'C-H bending'},\n    'ch_stretch': {'range': (2800, 3000), 'name': 'C-H stretch'},\n    'oh_stretch': {'range': (3300, 3400), 'name': 'O-H stretch'},\n}\n\n# ============================================================================\n# BASELINE FUNCTIONS\n# ============================================================================\ndef poly_baseline(x, p, intensity, b):\n    y = (x / len(x)) ** p + b\n    return y * intensity / max(y)\n\ndef gaussian_baseline(x, mean, sd, intensity, b):\n    y = np.exp(-(x - mean) ** 2 / (2 * sd ** 2)) / (sd * np.sqrt(2 * np.pi)) + b\n    return y * intensity / max(y)\n\ndef pg_baseline(x, p, in1, mean, sd, in2, b):\n    y1 = (x / len(x)) ** p + b\n    y2 = np.exp(-(x - mean) ** 2 / (2 * sd ** 2)) / (sd * np.sqrt(2 * np.pi)) + b\n    return y1 / max(y1) * in1 + y2 / max(y2) * in2\n\ndef mix_min_no(sp, baseline):\n    return np.minimum(baseline, sp)\n\ndef iterative_fitting_with_bounds_no(sp, model, ite=10):\n    fitted_baseline = np.zeros(sp.shape[0])\n    x = np.linspace(1, sp.shape[0], sp.shape[0])\n    tempb = sp\n    torch_tempb = tf.expand_dims(tempb, axis=0)\n    i = 0\n    while i < ite:\n        tadvice = model(torch_tempb)\n        if tadvice[0][0] >= 0.5 and tadvice[0][1] >= 0.5:\n            try:\n                p, c = curve_fit(pg_baseline, x, tempb,\n                                bounds=([1, 0.5, 0, 100, 0.5, -0.5], [3, 1, sp.shape[0], 600, 1, 0.5]),\n                                maxfev=10000)\n                fitted_baseline = pg_baseline(x, p[0], p[1], p[2], p[3], p[4], p[5])\n            except RuntimeError:\n                fitted_baseline = tempb\n        elif tadvice[0][0] >= 0.5:\n            try:\n                p, c = curve_fit(poly_baseline, x, tempb,\n                                bounds=([1, 0.5, -0.5], [3, 1, 0.5]),\n                                maxfev=10000)\n                fitted_baseline = poly_baseline(x, p[0], p[1], p[2])\n            except RuntimeError:\n                fitted_baseline = tempb\n        elif tadvice[0][1] >= 0.5:\n            try:\n                p, c = curve_fit(gaussian_baseline, x, tempb,\n                                bounds=([0, 100, 0.5, -0.5], [sp.shape[0], 600, 1, 0.5]),\n                                maxfev=10000)\n                fitted_baseline = gaussian_baseline(x, p[0], p[1], p[2], p[3])\n            except RuntimeError:\n                fitted_baseline = tempb\n        tempb = mix_min_no(tempb, fitted_baseline)\n        tempb_np = np.array(tempb)\n        torch_tempb = tempb_np.reshape(1, sp.shape[0])\n        i += 1\n    return tempb\n\ndef create_baseline_model(input_shape=880):\n    model = models.Sequential([\n        layers.Input(shape=(input_shape,)),\n        layers.Reshape((input_shape, 1)),\n        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),\n        layers.AveragePooling1D(pool_size=2, strides=2),\n        layers.Flatten(),\n        layers.Dense(100, activation='relu'),\n        layers.Dense(2, activation='sigmoid')\n    ])\n    return model\n\ndef train_baseline_model(baseline_model, noise_data, epochs=10, batch_size=32):\n    try:\n        labels = np.load(os.path.join(data_dir, 'labels_noise_pure_182.npy'))\n        print(\"Labels loaded from labels_noise_pure_182.npy successfully!\")\n    except Exception as e:\n        print(f\"Error loading labels: {e}. Using random labels.\")\n        labels = np.random.randint(0, 2, size=noise_data.shape[0])\n\n    X = []\n    y = []\n    for i in range(noise_data.shape[0]):\n        pure = noise_data[i, 0, 0, :, 0]\n        noisy = noise_data[i, 0, 1, :, 0]\n        X.append(noisy)\n        y.append(labels[i])\n    X = np.array(X)[:, :, np.newaxis]\n    y = np.array(y)\n    baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    baseline_model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n    baseline_model.save_weights(os.path.join(data_dir, 'model.weights.h5'))\n    return baseline_model\n\n# ============================================================================\n# SPECTRUM PROCESSING FUNCTIONS\n# ============================================================================\ndef normalize_spectrum(spectrum):\n    spectrum = spectrum - np.min(spectrum)\n    if np.max(spectrum) > 0:\n        spectrum = spectrum / np.max(spectrum)\n    return spectrum\n\ndef WhittakerSmooth(x, w, lambda_=1, differences=1):\n    X = np.matrix(x)\n    m = X.size\n    E = eye(m, format='csc')\n    for i in range(differences):\n        E = E[1:] - E[:-1]\n    W = diags(w, 0, shape=(m, m))\n    A = csc_matrix(W + (lambda_ * E.T * E))\n    B = csc_matrix(W * X.T)\n    background = spsolve(A, B)\n    return np.array(background)\n\ndef airPLS(x, lambda_=100, porder=1, itermax=15):\n    \"\"\"\n    Adaptive iteratively reweighted Penalized Least Squares (airPLS) baseline correction.\n\n    Fixed to handle edge cases:\n    - Empty input spectra\n    - Spectra entirely above baseline (no negative residuals)\n    - Near-zero dssn values (division protection)\n    \"\"\"\n    m = x.shape[0]\n    if m == 0:\n        raise ValueError(\"Input spectrum is empty\")\n\n    w = np.ones(m)\n    lambda_ = max(50, min(500, 50 * np.std(x) / (np.mean(np.abs(x)) + 1e-6)))\n\n    for i in range(1, itermax + 1):\n        z = WhittakerSmooth(x, w, lambda_, porder)\n        d = x - z\n\n        neg_mask = d < 0\n        neg_count = np.sum(neg_mask)\n\n        # Handle case with no negative residuals (spectrum entirely above baseline)\n        if neg_count == 0:\n            return z\n\n        dssn = np.abs(d[neg_mask].sum())\n\n        # Avoid division by near-zero\n        if dssn < 1e-10:\n            return z\n\n        if dssn < 0.001 * np.abs(x).sum():\n            return z\n\n        if i == itermax:\n            print(f'WARNING: Max iteration reached! lambda_={lambda_:.2f}, dssn={dssn:.2e}')\n            np.save(os.path.join(problematic_spectra_dir, f'problematic_spectrum_{np.random.randint(1000000)}.npy'), x)\n            return WhittakerSmooth(x, np.ones(m), lambda_=50)\n\n        w[d >= 0] = 0\n        w[neg_mask] = np.exp(i * np.abs(d[neg_mask]) / dssn)\n        w[0] = np.exp(i * np.abs(d[neg_mask]).max() / dssn)\n        w[-1] = w[0]\n\n    return z\n\ndef enhanced_baseline_correction(spectrum, baseline_model):\n    \"\"\"\n    Baseline correction using hybrid DL model + airPLS refinement.\n    Falls back to airPLS only if the hybrid method fails.\n    \"\"\"\n    try:\n        baseline = iterative_fitting_with_bounds_no(spectrum, baseline_model)\n        fine_corrected = airPLS(baseline, lambda_=100, itermax=15)\n        return np.clip(spectrum - fine_corrected, 0, None)\n    except Exception as e:\n        print(f\"Error in baseline correction: {e}. Falling back to airPLS.\")\n        return np.clip(spectrum - airPLS(spectrum, lambda_=100, itermax=15), 0, None)\n\ndef interpolate_spectrum(spectrum, original_length, target_length=880):\n    x_original = np.linspace(0, original_length - 1, original_length)\n    x_target = np.linspace(0, original_length - 1, target_length)\n    interpolator = interp1d(x_original, spectrum, kind='linear', fill_value=\"extrapolate\")\n    return interpolator(x_target)\n\ndef shift_spectrum(spectrum, shift):\n    return np.roll(spectrum, shift)\n\ndef stretch_spectrum(spectrum, alpha):\n    original_len = len(spectrum)\n    new_len = int(original_len / alpha)\n    if new_len < 1:\n        new_len = 1\n    if new_len > original_len * 10:\n        new_len = original_len * 10\n    x_original = np.linspace(0, original_len - 1, original_len)\n    x_new = np.linspace(0, original_len - 1, new_len)\n    interpolator = interp1d(x_original, spectrum, kind='linear', fill_value=\"extrapolate\")\n    stretched = interpolator(x_new)\n    return interpolate_spectrum(stretched, new_len, original_len)\n\ndef generate_synthetic_spectrum(input_spectrum, noise_data, spectrum_length=880):\n    \"\"\"Generate synthetic spectrum with noise, baseline, and augmentation.\"\"\"\n    x_range = np.linspace(0, spectrum_length, spectrum_length)\n    # Add noise from dataset\n    noise_idx = np.random.randint(0, noise_data.shape[0])\n    pure = noise_data[noise_idx, 0, 0, :, 0]\n    noisy = noise_data[noise_idx, 0, 1, :, 0]\n    noise = noisy - pure\n    scale = np.random.uniform(1.0, 2.0)\n    synthetic_spectrum = input_spectrum + noise * scale\n    # Add Gaussian noise\n    synthetic_spectrum += np.random.normal(0, 0.05 * np.std(input_spectrum), spectrum_length)\n    # Add synthetic baseline\n    baseline_type = np.random.choice(['poly', 'gaussian', 'none'], p=[0.3, 0.3, 0.4])\n    if baseline_type == 'poly':\n        baseline = poly_baseline(x_range, p=np.random.uniform(1.9, 2.1),\n                                intensity=np.random.uniform(0.75, 0.8),\n                                b=np.random.uniform(-0.1, 0.1))\n        synthetic_spectrum += baseline\n    elif baseline_type == 'gaussian':\n        baseline = gaussian_baseline(x_range, mean=np.random.uniform(0, spectrum_length),\n                                   sd=np.random.uniform(250, 300),\n                                   intensity=np.random.uniform(0.75, 0.8),\n                                   b=np.random.uniform(-0.1, 0.1))\n        synthetic_spectrum += baseline\n    # Probabilistic augmentation\n    aug_type = np.random.choice(['none', 'shift', 'stretch'], p=[0.5, 0.25, 0.25])\n    if aug_type == 'shift':\n        shift = np.random.randint(-10, 11)\n        synthetic_spectrum = shift_spectrum(synthetic_spectrum, shift)\n    elif aug_type == 'stretch':\n        alpha = np.random.uniform(0.5, 2.0)\n        synthetic_spectrum = stretch_spectrum(synthetic_spectrum, alpha)\n    return synthetic_spectrum\n\ndef create_gadf_map(spectrum, image_size=64):\n    spectrum = normalize_spectrum(spectrum)\n    spectrum = 2 * spectrum - 1\n    target_length = image_size * (spectrum.shape[0] // image_size)\n    if target_length != spectrum.shape[0]:\n        spectrum = interpolate_spectrum(spectrum, spectrum.shape[0], target_length)\n    gadf = GADF(image_size=image_size, method='difference')\n    return gadf.fit_transform(spectrum.reshape(1, -1))[0][:, :, np.newaxis]\n\n# ============================================================================\n# PHYSICALLY MEANINGFUL METRICS (Task 3)\n# ============================================================================\ndef wavenumber_to_index(wavenumber, wavenumbers):\n    \"\"\"Convert wavenumber to array index.\"\"\"\n    return np.argmin(np.abs(wavenumbers - wavenumber))\n\ndef calculate_snr(spectrum, signal_region, noise_region):\n    \"\"\"\n    Calculate Signal-to-Noise Ratio.\n\n    Args:\n        spectrum: Input spectrum\n        signal_region: Tuple (start_idx, end_idx) for signal peak\n        noise_region: Tuple (start_idx, end_idx) for baseline/noise\n\n    Returns:\n        SNR in dB\n    \"\"\"\n    signal = spectrum[signal_region[0]:signal_region[1]]\n    noise = spectrum[noise_region[0]:noise_region[1]]\n\n    signal_power = np.mean(signal ** 2)\n    noise_power = np.var(noise)\n\n    if noise_power < 1e-10:\n        return np.inf\n\n    snr_db = 10 * np.log10(signal_power / noise_power)\n    return snr_db\n\ndef calculate_peak_ratios(spectrum, wavenumbers):\n    \"\"\"\n    Calculate intensity ratios between characteristic peaks.\n\n    Returns:\n        Dictionary of peak ratios\n    \"\"\"\n    ethanol_co_idx = wavenumber_to_index(1050, wavenumbers)\n    methanol_co_idx = wavenumber_to_index(1030, wavenumbers)\n    ch_stretch_idx = wavenumber_to_index(2900, wavenumbers)\n\n    # Get local maxima around expected positions (+/-5 points)\n    ethanol_co_peak = np.max(spectrum[max(0, ethanol_co_idx-5):ethanol_co_idx+5])\n    methanol_co_peak = np.max(spectrum[max(0, methanol_co_idx-5):methanol_co_idx+5])\n    ch_stretch_peak = np.max(spectrum[max(0, ch_stretch_idx-10):ch_stretch_idx+10])\n\n    return {\n        'ethanol_co_to_ch': ethanol_co_peak / (ch_stretch_peak + 1e-10),\n        'methanol_co_to_ch': methanol_co_peak / (ch_stretch_peak + 1e-10),\n        'ethanol_to_methanol_co': ethanol_co_peak / (methanol_co_peak + 1e-10)\n    }\n\ndef evaluate_preprocessing_quality(raw_spectrum, processed_spectrum, wavenumbers):\n    \"\"\"Evaluate preprocessing quality with physically meaningful metrics.\"\"\"\n    # SNR calculation (signal: C-H stretch region, noise: flat region)\n    signal_region = (673, 720)  # C-H stretch\n    noise_region = (50, 100)    # Flat baseline region\n\n    snr_raw = calculate_snr(raw_spectrum, signal_region, noise_region)\n    snr_processed = calculate_snr(processed_spectrum, signal_region, noise_region)\n\n    # Peak ratio preservation\n    ratios_raw = calculate_peak_ratios(raw_spectrum, wavenumbers)\n    ratios_processed = calculate_peak_ratios(processed_spectrum, wavenumbers)\n\n    return {\n        'snr_raw': snr_raw,\n        'snr_processed': snr_processed,\n        'snr_improvement': snr_processed - snr_raw,\n        'peak_ratios_raw': ratios_raw,\n        'peak_ratios_processed': ratios_processed\n    }\n\n# ============================================================================\n# CHEMICALLY MEANINGFUL OCCLUSION ANALYSIS (Task 2)\n# ============================================================================\nfrom sklearn.metrics import accuracy_score\n\ndef chemically_meaningful_occlusion(model, X, y, wavenumbers, bands=None):\n    \"\"\"\n    Occlusion analysis with non-overlapping chemically meaningful windows.\n\n    Args:\n        model: Trained classifier\n        X: Input spectra (N, 880, 1)\n        y: True labels\n        wavenumbers: Wavenumber array\n        bands: Dictionary of Raman bands\n\n    Returns:\n        Dictionary with occlusion results for each band\n    \"\"\"\n    if bands is None:\n        bands = RAMAN_BANDS\n\n    results = {}\n    y_pred_baseline = model.predict(X, verbose=0).argmax(axis=1)\n    baseline_acc = accuracy_score(y, y_pred_baseline)\n\n    for band_key, band_info in bands.items():\n        start_wn, end_wn = band_info['range']\n        start_idx = wavenumber_to_index(start_wn, wavenumbers)\n        end_idx = wavenumber_to_index(end_wn, wavenumbers)\n\n        X_occluded = X.copy()\n        X_occluded[:, start_idx:end_idx, :] = 0\n\n        y_pred = model.predict(X_occluded, verbose=0).argmax(axis=1)\n        acc = accuracy_score(y, y_pred)\n\n        results[band_key] = {\n            'name': band_info['name'],\n            'wavenumber_range': band_info['range'],\n            'index_range': (start_idx, end_idx),\n            'accuracy_drop': (baseline_acc - acc) * 100,\n            'occluded_accuracy': acc * 100\n        }\n\n    return {'bands': results, 'baseline_accuracy': baseline_acc * 100}\n\ndef plot_occlusion_analysis(occlusion_results, output_path):\n    \"\"\"Visualize occlusion analysis results.\"\"\"\n    bands = occlusion_results['bands']\n    baseline = occlusion_results['baseline_accuracy']\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n\n    band_names = [bands[k]['name'] for k in bands]\n    acc_drops = [bands[k]['accuracy_drop'] for k in bands]\n    colors = ['red' if d > 5 else 'orange' if d > 2 else 'green' for d in acc_drops]\n\n    bars = ax.bar(band_names, acc_drops, color=colors, edgecolor='black')\n\n    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n    ax.set_xlabel('Raman Band', fontsize=12)\n    ax.set_ylabel('Accuracy Drop (%)', fontsize=12)\n    ax.set_title(f'Chemically Meaningful Occlusion Analysis\\n(Baseline Accuracy: {baseline:.1f}%)', fontsize=14)\n    plt.xticks(rotation=45, ha='right')\n\n    # Add value labels on bars\n    for bar, val in zip(bars, acc_drops):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n                f'{val:.1f}%', ha='center', va='bottom', fontsize=10)\n\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n# ============================================================================\n# VISUALIZATION FUNCTIONS (Tasks 4-8)\n# ============================================================================\ndef plot_representative_spectra_by_concentration(spectra_data, wavenumbers, ratios_dict,\n                                                  output_path, figsize=(14, 10)):\n    \"\"\"Plot representative spectra showing spectral evolution with concentration.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=figsize)\n\n    # Colormap for concentration gradient\n    colors = plt.cm.RdYlGn(np.linspace(0, 1, 11))\n\n    # Plot all concentrations overlaid\n    ax1 = axes[0]\n    for name, spectrum in spectra_data.items():\n        ratio = ratios_dict.get(name, 0.5)\n        label = f\"{int(ratio*100)}% Ethanol\"\n        ax1.plot(wavenumbers, normalize_spectrum(spectrum),\n                 color=colors[int(ratio*10)], label=label, alpha=0.8)\n\n    ax1.set_xlabel('Wavenumber (cm$^{-1}$)')\n    ax1.set_ylabel('Normalized Intensity')\n    ax1.set_title('Spectral Evolution: Methanol to Ethanol')\n    ax1.legend(loc='upper right', fontsize=8, ncol=2)\n    ax1.grid(True, alpha=0.3)\n\n    # Highlight characteristic regions\n    ax1.axvspan(1020, 1055, color='red', alpha=0.1, label='Methanol C-O')\n    ax1.axvspan(1055, 1100, color='green', alpha=0.1, label='Ethanol C-O')\n\n    # Waterfall plot\n    ax2 = axes[1]\n    offset = 0\n    for name, spectrum in spectra_data.items():\n        ratio = ratios_dict.get(name, 0.5)\n        ax2.plot(wavenumbers, normalize_spectrum(spectrum) + offset,\n                 color=colors[int(ratio*10)], linewidth=1)\n        ax2.text(wavenumbers[-1] + 50, offset + 0.5, f\"{int(ratio*100)}%\", fontsize=8)\n        offset += 1.2\n\n    ax2.set_xlabel('Wavenumber (cm$^{-1}$)')\n    ax2.set_ylabel('Intensity (offset)')\n    ax2.set_title('Waterfall Plot: Concentration Series')\n\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef visualize_augmentation_effects(original_spectrum, noise_data, wavenumbers, output_path):\n    \"\"\"Visualize noise types and baseline simulations used in augmentation.\"\"\"\n    fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n    spectrum_length = len(original_spectrum)\n    x_range = np.linspace(0, spectrum_length, spectrum_length)\n\n    # (a) Original vs Gaussian noise\n    ax = axes[0, 0]\n    np.random.seed(42)\n    noisy = original_spectrum + np.random.normal(0, 0.05, len(original_spectrum))\n    ax.plot(wavenumbers, original_spectrum, 'b-', label='Original', alpha=0.7)\n    ax.plot(wavenumbers, noisy, 'r-', label='+ Gaussian noise', alpha=0.7)\n    ax.set_title('(a) Gaussian Noise Addition')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n\n    # (b) Original vs Dataset noise\n    ax = axes[0, 1]\n    noise_idx = 0\n    pure = noise_data[noise_idx, 0, 0, :, 0]\n    noisy_sample = noise_data[noise_idx, 0, 1, :, 0]\n    real_noise = noisy_sample - pure\n    ax.plot(wavenumbers, original_spectrum, 'b-', label='Original', alpha=0.7)\n    ax.plot(wavenumbers, original_spectrum + real_noise * 0.5, 'r-', label='+ Real noise', alpha=0.7)\n    ax.set_title('(b) Real Instrument Noise Addition')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n\n    # (c) Polynomial baseline\n    ax = axes[1, 0]\n    poly_bl = poly_baseline(x_range, p=2.0, intensity=0.8, b=0.0)\n    ax.plot(wavenumbers, original_spectrum, 'b-', label='Original', alpha=0.7)\n    ax.plot(wavenumbers, poly_bl, 'g--', label='Polynomial baseline', alpha=0.7)\n    ax.plot(wavenumbers, original_spectrum + poly_bl, 'r-', label='With baseline', alpha=0.7)\n    ax.set_title('(c) Polynomial Baseline Simulation')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n\n    # (d) Gaussian baseline (fluorescence)\n    ax = axes[1, 1]\n    gauss_bl = gaussian_baseline(x_range, mean=440, sd=280, intensity=0.8, b=0.0)\n    ax.plot(wavenumbers, original_spectrum, 'b-', label='Original', alpha=0.7)\n    ax.plot(wavenumbers, gauss_bl, 'g--', label='Gaussian baseline', alpha=0.7)\n    ax.plot(wavenumbers, original_spectrum + gauss_bl, 'r-', label='With baseline', alpha=0.7)\n    ax.set_title('(d) Gaussian Baseline (Fluorescence) Simulation')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n\n    # (e) Shift augmentation\n    ax = axes[2, 0]\n    shifted = shift_spectrum(original_spectrum, 10)\n    ax.plot(wavenumbers, original_spectrum, 'b-', label='Original', alpha=0.7)\n    ax.plot(wavenumbers, shifted, 'r-', label='Shifted (+10 pts)', alpha=0.7)\n    ax.set_title('(e) Spectral Shift Augmentation')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n\n    # (f) Stretch augmentation\n    ax = axes[2, 1]\n    stretched = stretch_spectrum(original_spectrum, 1.5)\n    ax.plot(wavenumbers, original_spectrum, 'b-', label='Original', alpha=0.7)\n    ax.plot(wavenumbers, stretched, 'r-', label='Stretched (a=1.5)', alpha=0.7)\n    ax.set_title('(f) Spectral Stretch Augmentation')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n\n    plt.suptitle('Data Augmentation Techniques', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef plot_preprocessing_comparison(raw_spectrum, hybrid_corrected, airpls_corrected,\n                                   wavenumbers, output_path):\n    \"\"\"Compare preprocessing methods: Raw, Hybrid DL, airPLS.\"\"\"\n    fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n\n    # (a) Raw with fluorescence baseline\n    ax = axes[0]\n    ax.plot(wavenumbers, raw_spectrum, 'b-', linewidth=1.5)\n    ax.fill_between(wavenumbers, 0, raw_spectrum, alpha=0.3)\n    ax.set_ylabel('Intensity')\n    ax.set_title('(a) Raw Raman Spectrum with Fluorescence Baseline')\n    ax.grid(True, alpha=0.3)\n\n    # (b) Hybrid DL preprocessing\n    ax = axes[1]\n    ax.plot(wavenumbers, hybrid_corrected, 'g-', linewidth=1.5)\n    ax.fill_between(wavenumbers, 0, hybrid_corrected, alpha=0.3, color='green')\n    ax.set_ylabel('Intensity')\n    ax.set_title('(b) After Hybrid Deep Learning Preprocessing')\n    ax.grid(True, alpha=0.3)\n\n    # (c) airPLS only\n    ax = axes[2]\n    ax.plot(wavenumbers, airpls_corrected, 'r-', linewidth=1.5)\n    ax.fill_between(wavenumbers, 0, airpls_corrected, alpha=0.3, color='red')\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n    ax.set_ylabel('Intensity')\n    ax.set_title('(c) After airPLS Baseline Correction')\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef visualize_gadf_samples(X_2d, labels, output_path):\n    \"\"\"Generate GADF images for pure methanol, 50:50 mixture, and pure ethanol.\"\"\"\n    # Find representative samples\n    labels_arr = np.array(labels)\n    pure_methanol_idx = np.where(labels_arr == 0)[0][0]\n    mixture_idx = np.where(labels_arr == 5)[0][0]\n    pure_ethanol_idx = np.where(labels_arr == 10)[0][0]\n\n    samples = [\n        (pure_methanol_idx, 'Pure Methanol (Class 0)', '100% Methanol'),\n        (mixture_idx, '50:50 Mixture (Class 5)', '50% Ethanol / 50% Methanol'),\n        (pure_ethanol_idx, 'Pure Ethanol (Class 10)', '100% Ethanol')\n    ]\n\n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n    for ax, (idx, title, subtitle) in zip(axes, samples):\n        gadf_img = X_2d[idx, :, :, 0]\n        im = ax.imshow(gadf_img, cmap='viridis', aspect='auto',\n                       extent=[0, 64, 64, 0])\n        ax.set_title(f'{title}\\n{subtitle}', fontsize=12)\n        ax.set_xlabel('GADF Column')\n        ax.set_ylabel('GADF Row')\n        plt.colorbar(im, ax=ax, fraction=0.046)\n\n    plt.suptitle('64x64 GADF Representations of Raman Spectra', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef plot_peak_preservation_snr(raw_spectrum, processed_spectrum, wavenumbers,\n                                metrics, output_path):\n    \"\"\"Illustrate peak preservation and SNR improvement.\"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n\n    # Top: Before preprocessing\n    ax = axes[0]\n    ax.plot(wavenumbers, raw_spectrum, 'b-', linewidth=1.5, label='Raw Spectrum')\n\n    # Annotate peaks\n    for band_name, wn in [('C-O (EtOH)', 1050), ('C-O (MeOH)', 1030), ('C-H', 2900)]:\n        idx = wavenumber_to_index(wn, wavenumbers)\n        if idx < len(raw_spectrum):\n            ax.annotate(band_name, xy=(wn, raw_spectrum[idx]),\n                        xytext=(wn, raw_spectrum[idx] + 0.1),\n                        arrowprops=dict(arrowstyle='->', color='red'),\n                        fontsize=10, ha='center')\n\n    ax.set_ylabel('Intensity')\n    snr_raw = metrics.get('snr_raw', 0)\n    ax.set_title(f'Before Preprocessing (SNR = {snr_raw:.1f} dB)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # Bottom: After preprocessing\n    ax = axes[1]\n    ax.plot(wavenumbers, processed_spectrum, 'g-', linewidth=1.5, label='Processed Spectrum')\n\n    for band_name, wn in [('C-O (EtOH)', 1050), ('C-O (MeOH)', 1030), ('C-H', 2900)]:\n        idx = wavenumber_to_index(wn, wavenumbers)\n        if idx < len(processed_spectrum):\n            ax.annotate(band_name, xy=(wn, processed_spectrum[idx]),\n                        xytext=(wn, processed_spectrum[idx] + 0.1),\n                        arrowprops=dict(arrowstyle='->', color='red'),\n                        fontsize=10, ha='center')\n\n    ax.set_xlabel('Wavenumber (cm$^{-1}$)')\n    ax.set_ylabel('Intensity')\n    snr_processed = metrics.get('snr_processed', 0)\n    snr_improvement = metrics.get('snr_improvement', 0)\n    ax.set_title(f'After Preprocessing (SNR = {snr_processed:.1f} dB, Improvement = {snr_improvement:.1f} dB)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\n    # Add peak ratio comparison text box\n    ratios_raw = metrics.get('peak_ratios_raw', {})\n    ratios_proc = metrics.get('peak_ratios_processed', {})\n    textstr = (f\"Peak Ratio Preservation:\\n\"\n               f\"EtOH C-O/C-H: {ratios_raw.get('ethanol_co_to_ch', 0):.3f} -> \"\n               f\"{ratios_proc.get('ethanol_co_to_ch', 0):.3f}\\n\"\n               f\"MeOH C-O/C-H: {ratios_raw.get('methanol_co_to_ch', 0):.3f} -> \"\n               f\"{ratios_proc.get('methanol_co_to_ch', 0):.3f}\")\n    ax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\ndef plot_all_training_histories(histories, output_path):\n    \"\"\"Plot training curves for all models.\"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n    model_names = ['densenet_1d', 'densenet_2d', 'resnet_1d', 'resnet_2d']\n    titles = ['DenseNet 1D', 'DenseNet 2D (GADF)', 'ResNet 1D', 'ResNet 2D (GADF)']\n\n    for ax, name, title in zip(axes.flat, model_names, titles):\n        if name not in histories:\n            ax.text(0.5, 0.5, f'{title}\\n(No data)', ha='center', va='center')\n            ax.set_title(title)\n            continue\n        h = histories[name]\n\n        # Plot loss and accuracy\n        ax2 = ax.twinx()\n        l1, = ax.plot(h['loss'], 'b-', label='Train Loss')\n        l2, = ax.plot(h.get('val_loss', []), 'b--', label='Val Loss')\n        l3, = ax2.plot(h['accuracy'], 'r-', label='Train Acc')\n        l4, = ax2.plot(h.get('val_accuracy', []), 'r--', label='Val Acc')\n\n        ax.set_xlabel('Epoch')\n        ax.set_ylabel('Loss', color='blue')\n        ax2.set_ylabel('Accuracy', color='red')\n        ax.set_title(title)\n\n        lines = [l1, l2, l3, l4]\n        labels = [l.get_label() for l in lines]\n        ax.legend(lines, labels, loc='center right', fontsize=8)\n        ax.grid(True, alpha=0.3)\n\n    plt.suptitle('Model Training History', fontsize=14, fontweight='bold')\n    plt.tight_layout()\n    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n    plt.close()\n\n# ============================================================================\n# DATA LOADING FUNCTIONS\n# ============================================================================\ndef load_excel_data():\n    excel_path = os.path.join(data_dir, 'Ethanol_Methanol.xlsx')\n    try:\n        df = pd.read_excel(excel_path, usecols='A:L')\n        raman_shift = df['Raman Shift (cm-1)'].values\n        spectra_data = {\n            'ethanol': df['Ethanol'].values,\n            'methanol': df['Methanol'].values,\n            'EM1_a': df['EM1_a'].values,\n            'EM2_a': df['EM2_a'].values,\n            'EM3_a': df['EM3_a'].values,\n            'EM4_a': df['EM4_a'].values,\n            'EM5_a': df['EM5_a'].values,\n            'EM6_a': df['EM6_a'].values,\n            'EM7_a': df['EM7_a'].values,\n            'EM8_a': df['EM8_a'].values,\n            'EM9_a': df['EM9_a'].values\n        }\n        for key in spectra_data:\n            spectrum = spectra_data[key]\n            spectrum = spectrum[~np.isnan(spectrum)]\n            spectra_data[key] = interpolate_spectrum(spectrum, len(spectrum), 880)\n        return spectra_data, raman_shift\n    except Exception as e:\n        print(f\"Error loading data from {excel_path}: {e}\")\n        return {}, np.array([])\n\ndef load_noise_data():\n    try:\n        noise_data = np.load(os.path.join(data_dir, 'dataset_noise_pure_182.npy'))\n        return noise_data\n    except Exception as e:\n        print(f\"Error loading noise data: {e}\")\n        return np.array([])\n\n# ============================================================================\n# MODEL DEFINITIONS\n# ============================================================================\ndef build_1d_densenet(input_shape=(880, 1), num_classes=11, growth_rate=12):\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Conv1D(48, 7, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    def dense_block(x, num_layers, filters):\n        for _ in range(num_layers):\n            y = layers.BatchNormalization()(x)\n            y = layers.Activation('relu')(y)\n            y = layers.Conv1D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n            x = layers.Concatenate()([x, y])\n        return x\n    def transition_layer(x):\n        filters = x.shape[-1]\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation('relu')(x)\n        x = layers.Conv1D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n        x = layers.MaxPooling1D(pool_size=2)(x)\n        return x\n    for _ in range(3):\n        x = dense_block(x, num_layers=4, filters=growth_rate)\n        x = transition_layer(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.4)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    return models.Model(inputs, outputs)\n\ndef build_2d_densenet(input_shape=(64, 64, 1), num_classes=11, growth_rate=12):\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Conv2D(48, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    def dense_block(x, num_layers, filters):\n        for _ in range(num_layers):\n            y = layers.BatchNormalization()(x)\n            y = layers.Activation('relu')(y)\n            y = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n            x = layers.Concatenate()([x, y])\n        return x\n    def transition_layer(x):\n        filters = x.shape[-1]\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation('relu')(x)\n        x = layers.Conv2D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n        return x\n    for _ in range(3):\n        x = dense_block(x, num_layers=4, filters=growth_rate)\n        x = transition_layer(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation('relu')(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.4)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    return models.Model(inputs, outputs)\n\ndef build_1d_resnet(input_shape=(880, 1), num_classes=11):\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Conv1D(64, 5, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    def residual_block(x, filters, kernel_size=3):\n        shortcut = x\n        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n        x = layers.BatchNormalization()(x)\n        if shortcut.shape[-1] != filters:\n            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n        x = layers.Add()([shortcut, x])\n        x = layers.Activation('relu')(x)\n        return x\n    x = residual_block(x, 64)\n    x = residual_block(x, 64)\n    x = layers.MaxPooling1D(pool_size=2)(x)\n    x = residual_block(x, 128)\n    x = residual_block(x, 128)\n    x = residual_block(x, 128)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    return models.Model(inputs, outputs)\n\ndef build_2d_resnet(input_shape=(64, 64, 1), num_classes=11):\n    inputs = layers.Input(shape=input_shape)\n    x = layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    def residual_block(x, filters, kernel_size=3):\n        shortcut = x\n        x = layers.Conv2D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        if shortcut.shape[-1] != filters:\n            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n        x = layers.Add()([shortcut, x])\n        x = layers.Activation('relu')(x)\n        return x\n    x = residual_block(x, 32)\n    x = residual_block(x, 32)\n    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(128, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    return models.Model(inputs, outputs)\n\n# ============================================================================\n# CONFUSION MATRIX\n# ============================================================================\ndef plot_confusion_matrix(y_true, y_pred, title, filename):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=[f\"{i*10}% Ethanol\" for i in range(11)],\n                yticklabels=[f\"{i*10}% Ethanol\" for i in range(11)])\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n    plt.close()\n\n# ============================================================================\n# DATA AUGMENTATION LAYERS\n# ============================================================================\ndata_augmentation_1d = models.Sequential([\n    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n])\ndata_augmentation_2d = models.Sequential([\n    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n])\n\n# ============================================================================\n# LOAD AND PROCESS DATA\n# ============================================================================\nspectra_data, raman_shift = load_excel_data()\nnoise_data = load_noise_data()\nif not spectra_data or noise_data.size == 0:\n    raise FileNotFoundError(\"Cannot load data from Excel file or noise file.\")\n\n# Initialize and train baseline model\nbaseline_model = create_baseline_model(input_shape=880)\ntry:\n    baseline_model.load_weights(os.path.join(data_dir, 'model.weights.h5'))\n    print(\"Baseline model weights loaded successfully!\")\nexcept Exception as e:\n    print(f\"Error loading weights: {e}. Training baseline model.\")\n    baseline_model = train_baseline_model(baseline_model, noise_data)\n\n# ============================================================================\n# GENERATE SYNTHETIC DATA\n# ============================================================================\nX_1d = []\nX_2d = []\nlabels = []\nsample_ids = []\nexample_spectra = {}\ntotal_spectra = 0\nspectrum_length = 880\n\nratio_to_label = {0.0: 0, 0.1: 1, 0.2: 2, 0.3: 3, 0.4: 4, 0.5: 5, 0.6: 6, 0.7: 7, 0.8: 8, 0.9: 9, 1.0: 10}\nratios = {\n    'ethanol': 1.0,\n    'methanol': 0.0,\n    'EM1_a': 0.9,\n    'EM2_a': 0.8,\n    'EM3_a': 0.7,\n    'EM4_a': 0.6,\n    'EM5_a': 0.5,\n    'EM6_a': 0.4,\n    'EM7_a': 0.3,\n    'EM8_a': 0.2,\n    'EM9_a': 0.1\n}\n\n# Store raw and corrected spectra for visualization\nraw_spectra_for_viz = {}\ncorrected_spectra_for_viz = {}\n\nprint(\"Starting synthetic data generation...\")\nfor spectrum_type, ethanol_ratio in ratios.items():\n    print(f\"Processing {spectrum_type} with ethanol ratio {ethanol_ratio}...\")\n    input_spectrum = spectra_data[spectrum_type]\n    normalized_raw = normalize_spectrum(input_spectrum)\n\n    # Store for visualization\n    raw_spectra_for_viz[spectrum_type] = normalized_raw.copy()\n\n    corrected_spectrum = enhanced_baseline_correction(normalized_raw, baseline_model)\n    corrected_spectrum = normalize_spectrum(corrected_spectrum)\n\n    corrected_spectra_for_viz[spectrum_type] = corrected_spectrum.copy()\n\n    X_1d.append(corrected_spectrum)\n    X_2d.append(create_gadf_map(corrected_spectrum, image_size=64))\n    label_idx = ratio_to_label[ethanol_ratio]\n    labels.append(label_idx)\n    sample_ids.append(f\"{spectrum_type}_original\")\n    total_spectra += 1\n    if total_spectra == 1 or ethanol_ratio not in example_spectra:\n        example_spectra[ethanol_ratio] = {\n            'raw': normalized_raw,\n            'corrected': corrected_spectrum,\n            'ethanol': spectra_data['ethanol'] if ethanol_ratio > 0 else None,\n            'methanol': spectra_data['methanol'] if ethanol_ratio < 1 else None\n        }\n    for i in range(999):\n        synthetic_spectrum = generate_synthetic_spectrum(normalized_raw, noise_data, spectrum_length)\n        corrected_spectrum = enhanced_baseline_correction(synthetic_spectrum, baseline_model)\n        corrected_spectrum = normalize_spectrum(corrected_spectrum)\n        X_1d.append(corrected_spectrum)\n        X_2d.append(create_gadf_map(corrected_spectrum, image_size=64))\n        labels.append(label_idx)\n        sample_ids.append(f\"{spectrum_type}_synthetic_{i}\")\n        total_spectra += 1\n        if i == 0:\n            example_spectra[ethanol_ratio] = {\n                'raw': synthetic_spectrum,\n                'corrected': corrected_spectrum,\n                'ethanol': spectra_data['ethanol'] if ethanol_ratio > 0 else None,\n                'methanol': spectra_data['methanol'] if ethanol_ratio < 1 else None\n            }\n        if total_spectra % 1000 == 0:\n            print(f\"Processed {total_spectra} spectra.\")\n\nprint(f\"Total spectra: {total_spectra}\")\n\n# Convert to numpy arrays\nX_1d = np.array(X_1d)[:, :, np.newaxis]\nX_2d = np.array(X_2d)\nlabels_df = pd.DataFrame({'label': labels, 'sample_id': sample_ids})\n\nprint(\"X_1d shape:\", X_1d.shape)\nprint(\"X_2d shape:\", X_2d.shape)\nprint(\"labels_df shape:\", labels_df.shape)\nprint(\"Label distribution:\\n\", labels_df[\"label\"].value_counts())\n\n# Save data\nlabels_df.to_csv(os.path.join(labels_dir, \"labels.csv\"), index=False)\nnp.save(os.path.join(synthetic_dir, \"synthetic_1d.npy\"), X_1d)\nnp.save(os.path.join(maps_dir, \"spectral_maps_gadf.npy\"), X_2d)\n\n# ============================================================================\n# GENERATE VISUALIZATIONS (Tasks 4-8)\n# ============================================================================\nwavenumbers = np.linspace(500, 3500, 880)\n\n# Task 4: Representative spectra visualization\nprint(\"Generating representative spectra visualization...\")\nplot_representative_spectra_by_concentration(\n    spectra_data, wavenumbers, ratios,\n    os.path.join(visualizations_dir, 'representative_spectra.png')\n)\n\n# Task 5: Augmentation effects visualization\nprint(\"Generating augmentation effects visualization...\")\nsample_spectrum = normalize_spectrum(spectra_data['EM5_a'])\nvisualize_augmentation_effects(\n    sample_spectrum, noise_data, wavenumbers,\n    os.path.join(visualizations_dir, 'augmentation_effects.png')\n)\n\n# Task 6: Preprocessing comparison\nprint(\"Generating preprocessing comparison visualization...\")\nsample_raw = normalize_spectrum(spectra_data['EM5_a'])\nsample_synthetic = sample_raw + gaussian_baseline(\n    np.linspace(0, 880, 880), mean=440, sd=280, intensity=0.8, b=0.0\n)\nsample_hybrid = enhanced_baseline_correction(sample_synthetic, baseline_model)\nsample_airpls = np.clip(sample_synthetic - airPLS(sample_synthetic), 0, None)\nplot_preprocessing_comparison(\n    sample_synthetic, normalize_spectrum(sample_hybrid), normalize_spectrum(sample_airpls),\n    wavenumbers, os.path.join(visualizations_dir, 'preprocessing_comparison.png')\n)\n\n# Task 7: GADF visualization\nprint(\"Generating GADF visualization...\")\nvisualize_gadf_samples(X_2d, labels, os.path.join(visualizations_dir, 'gadf_samples.png'))\n\n# Task 8: Peak preservation and SNR improvement\nprint(\"Generating peak preservation/SNR visualization...\")\nsample_raw = normalize_spectrum(spectra_data['EM5_a'])\nsample_corrected = normalize_spectrum(corrected_spectra_for_viz.get('EM5_a', sample_raw))\nmetrics = evaluate_preprocessing_quality(sample_raw, sample_corrected, wavenumbers)\nplot_peak_preservation_snr(\n    sample_raw, sample_corrected, wavenumbers, metrics,\n    os.path.join(visualizations_dir, 'peak_preservation_snr.png')\n)\n\n# Legacy visualization\ndef plot_spectra_comparison(wavenumbers, raw_spectrum, corrected_spectrum, ethanol_spectrum, methanol_spectrum, title, filename):\n    plt.figure(figsize=(15, 10))\n    plt.subplot(2, 1, 1)\n    plt.plot(wavenumbers, raw_spectrum, label=\"Raw Spectrum\", color='blue')\n    plt.axvspan(1000, 1020, color='red', alpha=0.2, label=\"Methanol Region\")\n    plt.axvspan(870, 890, color='green', alpha=0.2, label=\"Ethanol Region\")\n    plt.xlabel(\"Wavenumber (cm$^{-1}$)\")\n    plt.ylabel(\"Normalized Intensity\")\n    plt.title(f\"Raw Spectrum ({title})\")\n    plt.grid(True)\n    plt.legend()\n    plt.subplot(2, 1, 2)\n    plt.plot(wavenumbers, corrected_spectrum, label=\"Corrected Spectrum\", color='orange')\n    if ethanol_spectrum is not None:\n        plt.plot(wavenumbers, normalize_spectrum(ethanol_spectrum), label=\"Ethanol Component\", color='green', linestyle='--')\n    if methanol_spectrum is not None:\n        plt.plot(wavenumbers, normalize_spectrum(methanol_spectrum), label=\"Methanol Component\", color='red', linestyle='--')\n    plt.axvspan(1000, 1020, color='red', alpha=0.2)\n    plt.axvspan(870, 890, color='green', alpha=0.2)\n    plt.xlabel(\"Wavenumber (cm$^{-1}$)\")\n    plt.ylabel(\"Normalized Intensity\")\n    plt.title(f\"Baseline Corrected Spectrum ({title})\")\n    plt.grid(True)\n    plt.legend()\n    plt.tight_layout()\n    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n    plt.close()\n\nfor ratio in example_spectra:\n    title = f\"Ratio Ethanol/Methanol = {ratio:.2f}/{1-ratio:.2f}\" if ratio < 1.0 else \"Pure Ethanol\" if ratio == 1.0 else \"Pure Methanol\"\n    plot_spectra_comparison(\n        wavenumbers,\n        example_spectra[ratio]['raw'],\n        example_spectra[ratio]['corrected'],\n        example_spectra[ratio]['ethanol'],\n        example_spectra[ratio]['methanol'],\n        title,\n        f\"spectra_comparison_ratio_{ratio:.2f}.png\"\n    )\n\n# ============================================================================\n# SPLIT DATA\n# ============================================================================\ny = labels_df[\"label\"].values\nX_1d_train, X_1d_test, y_train, y_test = train_test_split(X_1d, y, test_size=0.2, random_state=42)\nX_2d_train, X_2d_test, y_train_2d, y_test_2d = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n\n# Compute class weights\nclass_weights = compute_class_weight('balanced', classes=np.arange(11), y=y)\nclass_weight = {i: w for i, w in enumerate(class_weights)}",
   "id": "78dcb016f92b987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ============================================================================\n# TRAIN MODELS WITH HISTORY CAPTURE (Task 9)\n# ============================================================================\nearly_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n\n# Dictionary to store training histories\nhistories = {}\n\n# DenseNet 1D\nprint(\"Training DenseNet 1D...\")\ntf.keras.backend.clear_session()\nlr_schedule_1d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\noptimizer_1d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_densenet)\ndensenet_1d = models.Sequential([data_augmentation_1d, build_1d_densenet()])\ndensenet_1d.compile(optimizer=optimizer_1d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory_densenet_1d = densenet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_1d.keras\"), save_best_only=True), early_stopping],\n                class_weight=class_weight)\nhistories['densenet_1d'] = history_densenet_1d.history\n\n# DenseNet 2D\nprint(\"Training DenseNet 2D...\")\ntf.keras.backend.clear_session()\nlr_schedule_2d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\noptimizer_2d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_densenet)\ndensenet_2d = models.Sequential([data_augmentation_2d, build_2d_densenet()])\ndensenet_2d.compile(optimizer=optimizer_2d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory_densenet_2d = densenet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_2d.keras\"), save_best_only=True), early_stopping],\n                class_weight=class_weight)\nhistories['densenet_2d'] = history_densenet_2d.history\n\n# ResNet 1D\nprint(\"Training ResNet 1D...\")\ntf.keras.backend.clear_session()\nlr_schedule_1d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\noptimizer_1d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_resnet)\nresnet_1d = models.Sequential([data_augmentation_1d, build_1d_resnet()])\nresnet_1d.compile(optimizer=optimizer_1d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory_resnet_1d = resnet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_1d.keras\"), save_best_only=True), early_stopping],\n              class_weight=class_weight)\nhistories['resnet_1d'] = history_resnet_1d.history\n\n# ResNet 2D\nprint(\"Training ResNet 2D...\")\ntf.keras.backend.clear_session()\nlr_schedule_2d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\noptimizer_2d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_resnet)\nresnet_2d = models.Sequential([data_augmentation_2d, build_2d_resnet()])\nresnet_2d.compile(optimizer=optimizer_2d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory_resnet_2d = resnet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_2d.keras\"), save_best_only=True), early_stopping],\n              class_weight=class_weight)\nhistories['resnet_2d'] = history_resnet_2d.history\n\n# Plot training histories\nprint(\"Generating training curves visualization...\")\nplot_all_training_histories(histories, os.path.join(visualizations_dir, 'training_curves.png'))\n\n# Save histories for later analysis\nimport json\n# Convert numpy arrays to lists for JSON serialization\nhistories_serializable = {}\nfor k, v in histories.items():\n    histories_serializable[k] = {key: [float(x) for x in val] for key, val in v.items()}\nwith open(os.path.join(results_dir, 'training_histories.json'), 'w') as f:\n    json.dump(histories_serializable, f, indent=2)\nprint(\"Training histories saved.\")",
   "id": "e0d9115d1f64619c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# ============================================================================\n# EVALUATION & OCCLUSION ANALYSIS (Task 2)\n# ============================================================================\n\n# Model predictions\ny_pred_1d_densenet = densenet_1d.predict(X_1d_test)\ny_pred_2d_densenet = densenet_2d.predict(X_2d_test)\ny_pred_1d_resnet = resnet_1d.predict(X_1d_test)\ny_pred_2d_resnet = resnet_2d.predict(X_2d_test)\n\ny_pred_1d_densenet_labels = np.argmax(y_pred_1d_densenet, axis=1)\ny_pred_2d_densenet_labels = np.argmax(y_pred_2d_densenet, axis=1)\ny_pred_1d_resnet_labels = np.argmax(y_pred_1d_resnet, axis=1)\ny_pred_2d_resnet_labels = np.argmax(y_pred_2d_resnet, axis=1)\n\n# Calculate metrics\ndensenet_1d_acc = np.mean(y_pred_1d_densenet_labels == y_test)\ndensenet_2d_acc = np.mean(y_pred_2d_densenet_labels == y_test_2d)\nresnet_1d_acc = np.mean(y_pred_1d_resnet_labels == y_test)\nresnet_2d_acc = np.mean(y_pred_2d_resnet_labels == y_test_2d)\n\ndensenet_1d_precision = precision_score(y_test, y_pred_1d_densenet_labels, average='macro')\ndensenet_2d_precision = precision_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\nresnet_1d_precision = precision_score(y_test, y_pred_1d_resnet_labels, average='macro')\nresnet_2d_precision = precision_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n\ndensenet_1d_recall = recall_score(y_test, y_pred_1d_densenet_labels, average='macro')\ndensenet_2d_recall = recall_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\nresnet_1d_recall = recall_score(y_test, y_pred_1d_resnet_labels, average='macro')\nresnet_2d_recall = recall_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n\ndensenet_1d_f1 = f1_score(y_test, y_pred_1d_densenet_labels, average='macro')\ndensenet_2d_f1 = f1_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\nresnet_1d_f1 = f1_score(y_test, y_pred_1d_resnet_labels, average='macro')\nresnet_2d_f1 = f1_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"EVALUATION RESULTS\")\nprint(\"=\"*70)\nprint(f\"DenseNet 1D - Accuracy: {densenet_1d_acc:.4f}, Precision: {densenet_1d_precision:.4f}, Recall: {densenet_1d_recall:.4f}, F1: {densenet_1d_f1:.4f}\")\nprint(f\"DenseNet 2D (GADF) - Accuracy: {densenet_2d_acc:.4f}, Precision: {densenet_2d_precision:.4f}, Recall: {densenet_2d_recall:.4f}, F1: {densenet_2d_f1:.4f}\")\nprint(f\"ResNet 1D - Accuracy: {resnet_1d_acc:.4f}, Precision: {resnet_1d_precision:.4f}, Recall: {resnet_1d_recall:.4f}, F1: {resnet_1d_f1:.4f}\")\nprint(f\"ResNet 2D (GADF) - Accuracy: {resnet_2d_acc:.4f}, Precision: {resnet_2d_precision:.4f}, Recall: {resnet_2d_recall:.4f}, F1: {resnet_2d_f1:.4f}\")\n\n# Generate confusion matrices\nprint(\"\\nGenerating confusion matrices...\")\nplot_confusion_matrix(y_test, y_pred_1d_densenet_labels, \"DenseNet 1D Confusion Matrix\", \"confusion_densenet_1d.png\")\nplot_confusion_matrix(y_test_2d, y_pred_2d_densenet_labels, \"DenseNet 2D (GADF) Confusion Matrix\", \"confusion_densenet_2d.png\")\nplot_confusion_matrix(y_test, y_pred_1d_resnet_labels, \"ResNet 1D Confusion Matrix\", \"confusion_resnet_1d.png\")\nplot_confusion_matrix(y_test_2d, y_pred_2d_resnet_labels, \"ResNet 2D (GADF) Confusion Matrix\", \"confusion_resnet_2d.png\")\n\n# ============================================================================\n# CHEMICALLY MEANINGFUL OCCLUSION ANALYSIS (Task 2)\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"CHEMICALLY MEANINGFUL OCCLUSION ANALYSIS\")\nprint(\"=\"*70)\nprint(\"Performing occlusion analysis on best 1D model (DenseNet 1D)...\")\n\n# Perform occlusion analysis on the best 1D model\nocclusion_results = chemically_meaningful_occlusion(\n    densenet_1d, X_1d_test, y_test, wavenumbers, RAMAN_BANDS\n)\n\nprint(f\"\\nBaseline Accuracy: {occlusion_results['baseline_accuracy']:.2f}%\")\nprint(\"\\nAccuracy Drop by Raman Band:\")\nprint(\"-\" * 50)\nfor band_key, band_data in occlusion_results['bands'].items():\n    print(f\"  {band_data['name']:25s}: {band_data['accuracy_drop']:+6.2f}% \"\n          f\"({band_data['wavenumber_range'][0]}-{band_data['wavenumber_range'][1]} cm^-1)\")\n\n# Plot occlusion analysis results\nplot_occlusion_analysis(\n    occlusion_results,\n    os.path.join(visualizations_dir, 'occlusion_analysis.png')\n)\n\n# ============================================================================\n# SAVE COMPREHENSIVE RESULTS\n# ============================================================================\nresults = {\n    'models': {\n        'densenet_1d': {\n            'accuracy': float(densenet_1d_acc),\n            'precision': float(densenet_1d_precision),\n            'recall': float(densenet_1d_recall),\n            'f1': float(densenet_1d_f1)\n        },\n        'densenet_2d': {\n            'accuracy': float(densenet_2d_acc),\n            'precision': float(densenet_2d_precision),\n            'recall': float(densenet_2d_recall),\n            'f1': float(densenet_2d_f1)\n        },\n        'resnet_1d': {\n            'accuracy': float(resnet_1d_acc),\n            'precision': float(resnet_1d_precision),\n            'recall': float(resnet_1d_recall),\n            'f1': float(resnet_1d_f1)\n        },\n        'resnet_2d': {\n            'accuracy': float(resnet_2d_acc),\n            'precision': float(resnet_2d_precision),\n            'recall': float(resnet_2d_recall),\n            'f1': float(resnet_2d_f1)\n        }\n    },\n    'occlusion_analysis': {\n        'baseline_accuracy': occlusion_results['baseline_accuracy'],\n        'bands': {k: {\n            'name': v['name'],\n            'wavenumber_range': v['wavenumber_range'],\n            'accuracy_drop': v['accuracy_drop'],\n            'occluded_accuracy': v['occluded_accuracy']\n        } for k, v in occlusion_results['bands'].items()}\n    },\n    'preprocessing_quality': {\n        'snr_raw': float(metrics['snr_raw']) if not np.isinf(metrics['snr_raw']) else 'inf',\n        'snr_processed': float(metrics['snr_processed']) if not np.isinf(metrics['snr_processed']) else 'inf',\n        'snr_improvement': float(metrics['snr_improvement']) if not np.isinf(metrics['snr_improvement']) else 'inf',\n        'peak_ratios_raw': {k: float(v) for k, v in metrics['peak_ratios_raw'].items()},\n        'peak_ratios_processed': {k: float(v) for k, v in metrics['peak_ratios_processed'].items()}\n    }\n}\n\nwith open(os.path.join(results_dir, 'comprehensive_results.json'), 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"All visualizations saved to:\", visualizations_dir)\nprint(\"Results saved to:\", results_dir)\nprint(\"=\"*70)\n\n# List generated visualizations\nprint(\"\\nGenerated visualizations:\")\nfor viz_file in [\n    'representative_spectra.png',\n    'augmentation_effects.png',\n    'preprocessing_comparison.png',\n    'gadf_samples.png',\n    'peak_preservation_snr.png',\n    'training_curves.png',\n    'occlusion_analysis.png',\n    'confusion_densenet_1d.png',\n    'confusion_densenet_2d.png',\n    'confusion_resnet_1d.png',\n    'confusion_resnet_2d.png'\n]:\n    print(f\"  - {viz_file}\")",
   "id": "5fe37d1694c5ba18"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}