{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First Run For Best Model",
   "id": "e96fbe20cd6c2010"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T03:26:44.569733Z",
     "start_time": "2025-11-17T03:26:37.893655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.optimize import curve_fit\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, regularizers\n",
    "import pyts\n",
    "from pyts.transformation.transformation import GADF\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a fixed seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Create experiment directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join('experiments', f'experiment_{timestamp}')\n",
    "os.makedirs(experiment_dir, exist_ok=True)\n",
    "model_dir = os.path.join(experiment_dir, 'models')\n",
    "results_dir = os.path.join(experiment_dir, 'results')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Define directories\n",
    "data_dir = 'data'\n",
    "synthetic_dir = os.path.join(data_dir, 'synthetic')\n",
    "maps_dir = os.path.join(data_dir, 'maps')\n",
    "labels_dir = os.path.join(data_dir, 'labels')\n",
    "visualizations_dir = os.path.join(data_dir, 'visualizations')\n",
    "problematic_spectra_dir = os.path.join(data_dir, 'problematic_spectra')\n",
    "os.makedirs(synthetic_dir, exist_ok=True)\n",
    "os.makedirs(maps_dir, exist_ok=True)\n",
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "os.makedirs(visualizations_dir, exist_ok=True)\n",
    "os.makedirs(problematic_spectra_dir, exist_ok=True)\n",
    "\n",
    "# Save experiment configuration\n",
    "config = {\n",
    "    'seed': SEED,\n",
    "    'spectrum_length': 880,\n",
    "    'image_size': 64,\n",
    "    'num_synthetic_per_type': 999,\n",
    "    'num_types': 11,\n",
    "    'total_spectra': 11000,\n",
    "    'epochs': 10,\n",
    "    'batch_size_1d': 64,\n",
    "    'batch_size_2d': 32,\n",
    "    'validation_split': 0.1,\n",
    "    'test_size': 0.2,\n",
    "    'growth_rate': 12,\n",
    "    'num_classes': 11,\n",
    "    'experiment_timestamp': timestamp\n",
    "}\n",
    "with open(os.path.join(experiment_dir, 'config.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "# Baseline functions\n",
    "def poly_baseline(x, p, intensity, b):\n",
    "    y = (x / len(x)) ** p + b\n",
    "    return y * intensity / max(y)\n",
    "\n",
    "def gaussian_baseline(x, mean, sd, intensity, b):\n",
    "    y = np.exp(-(x - mean) ** 2 / (2 * sd ** 2)) / (sd * np.sqrt(2 * np.pi)) + b\n",
    "    return y * intensity / max(y)\n",
    "\n",
    "def pg_baseline(x, p, in1, mean, sd, in2, b):\n",
    "    y1 = (x / len(x)) ** p + b\n",
    "    y2 = np.exp(-(x - mean) ** 2 / (2 * sd ** 2)) / (sd * np.sqrt(2 * np.pi)) + b\n",
    "    return y1 / max(y1) * in1 + y2 / max(y2) * in2\n",
    "\n",
    "def mix_min_no(sp, baseline):\n",
    "    return np.minimum(baseline, sp)\n",
    "\n",
    "def iterative_fitting_with_bounds_no(sp, model, ite=10):\n",
    "    fitted_baseline = np.zeros(sp.shape[0])\n",
    "    x = np.linspace(1, sp.shape[0], sp.shape[0])\n",
    "    tempb = sp\n",
    "    torch_tempb = tf.expand_dims(tempb, axis=0)\n",
    "    i = 0\n",
    "    while i < ite:\n",
    "        tadvice = model(torch_tempb)\n",
    "        if tadvice[0][0] >= 0.5 and tadvice[0][1] >= 0.5:\n",
    "            try:\n",
    "                p, c = curve_fit(pg_baseline, x, tempb,\n",
    "                                bounds=([1, 0.5, 0, 100, 0.5, -0.5], [3, 1, sp.shape[0], 600, 1, 0.5]),\n",
    "                                maxfev=10000)\n",
    "                fitted_baseline = pg_baseline(x, p[0], p[1], p[2], p[3], p[4], p[5])\n",
    "            except RuntimeError:\n",
    "                fitted_baseline = tempb\n",
    "        elif tadvice[0][0] >= 0.5:\n",
    "            try:\n",
    "                p, c = curve_fit(poly_baseline, x, tempb,\n",
    "                                bounds=([1, 0.5, -0.5], [3, 1, 0.5]),\n",
    "                                maxfev=10000)\n",
    "                fitted_baseline = poly_baseline(x, p[0], p[1], p[2])\n",
    "            except RuntimeError:\n",
    "                fitted_baseline = tempb\n",
    "        elif tadvice[0][1] >= 0.5:\n",
    "            try:\n",
    "                p, c = curve_fit(gaussian_baseline, x, tempb,\n",
    "                                bounds=([0, 100, 0.5, -0.5], [sp.shape[0], 600, 1, 0.5]),\n",
    "                                maxfev=10000)\n",
    "                fitted_baseline = gaussian_baseline(x, p[0], p[1], p[2], p[3])\n",
    "            except RuntimeError:\n",
    "                fitted_baseline = tempb\n",
    "        tempb = mix_min_no(tempb, fitted_baseline)\n",
    "        tempb_np = np.array(tempb)\n",
    "        torch_tempb = tempb_np.reshape(1, sp.shape[0])\n",
    "        i += 1\n",
    "    return tempb\n",
    "\n",
    "def create_baseline_model(input_shape=880):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Reshape((input_shape, 1)),\n",
    "        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),\n",
    "        layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_baseline_model(baseline_model, noise_data, epochs=10, batch_size=32):\n",
    "    # Load labels from labels_noise_pure_182.npy\n",
    "    try:\n",
    "        labels = np.load(os.path.join(data_dir, 'labels_noise_pure_182.npy'))\n",
    "        print(\"Đã tải nhãn từ labels_noise_pure_182.npy thành công!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải nhãn: {e}. Sử dụng nhãn ngẫu nhiên.\")\n",
    "        labels = np.random.randint(0, 2, size=noise_data.shape[0])\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(noise_data.shape[0]):\n",
    "        pure = noise_data[i, 0, 0, :, 0]\n",
    "        noisy = noise_data[i, 0, 1, :, 0]\n",
    "        X.append(noisy)\n",
    "        y.append(labels[i])  # Sử dụng nhãn thực\n",
    "    X = np.array(X)[:, :, np.newaxis]\n",
    "    y = np.array(y)\n",
    "    baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    baseline_model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    baseline_model.save_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    return baseline_model\n",
    "\n",
    "# Normalize spectrum\n",
    "def normalize_spectrum(spectrum):\n",
    "    spectrum = spectrum - np.min(spectrum)\n",
    "    if np.max(spectrum) > 0:\n",
    "        spectrum = spectrum / np.max(spectrum)\n",
    "    return spectrum\n",
    "\n",
    "# WhittakerSmooth and airPLS\n",
    "def WhittakerSmooth(x, w, lambda_=1, differences=1):\n",
    "    X = np.matrix(x)\n",
    "    m = X.size\n",
    "    E = eye(m, format='csc')\n",
    "    for i in range(differences):\n",
    "        E = E[1:] - E[:-1]\n",
    "    W = diags(w, 0, shape=(m, m))\n",
    "    A = csc_matrix(W + (lambda_ * E.T * E))\n",
    "    B = csc_matrix(W * X.T)\n",
    "    background = spsolve(A, B)\n",
    "    return np.array(background)\n",
    "\n",
    "def airPLS(x, lambda_=100, porder=1, itermax=15):\n",
    "    m = x.shape[0]\n",
    "    w = np.ones(m)\n",
    "    lambda_ = max(50, min(500, 50 * np.std(x) / (np.mean(np.abs(x)) + 1e-6)))\n",
    "    for i in range(1, itermax + 1):\n",
    "        z = WhittakerSmooth(x, w, lambda_, porder)\n",
    "        d = x - z\n",
    "        dssn = np.abs(d[d < 0].sum())\n",
    "        if dssn < 0.001 * np.abs(x).sum():\n",
    "            return z\n",
    "        if i == itermax:\n",
    "            print(f'WARNING: Max iteration reached! lambda_={lambda_:.2f}, dssn={dssn:.2e}')\n",
    "            np.save(os.path.join(problematic_spectra_dir, f'problematic_spectrum_{np.random.randint(1000000)}.npy'), x)\n",
    "            return WhittakerSmooth(x, np.ones(m), lambda_=50)\n",
    "        w[d >= 0] = 0\n",
    "        w[d < 0] = np.exp(i * np.abs(d[d < 0]) / dssn)\n",
    "        w[0] = np.exp(i * (d[d < 0]).max() / dssn)\n",
    "        w[-1] = w[0]\n",
    "    return z\n",
    "\n",
    "# Enhanced baseline correction\n",
    "def enhanced_baseline_correction(spectrum, baseline_model):\n",
    "    \"\"\"\n",
    "    Trừ nền: Dùng baseline_model để đoán và trừ nền sơ bộ (đa thức/Gaussian),\n",
    "    sau đó dùng airPLS để tinh chỉnh. Nếu lỗi, dùng airPLS trực tiếp.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        baseline = iterative_fitting_with_bounds_no(spectrum, baseline_model)\n",
    "        fine_corrected = airPLS(baseline, lambda_=100, itermax=15)\n",
    "        return np.clip(spectrum - fine_corrected, 0, None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in baseline correction: {e}. Falling back to airPLS.\")\n",
    "        return np.clip(spectrum - airPLS(spectrum, lambda_=100, itermax=15), 0, None)\n",
    "\n",
    "# Interpolate spectrum\n",
    "def interpolate_spectrum(spectrum, original_length, target_length=880):\n",
    "    x_original = np.linspace(0, original_length - 1, original_length)\n",
    "    x_target = np.linspace(0, original_length - 1, target_length)\n",
    "    interpolator = interp1d(x_original, spectrum, kind='linear', fill_value=\"extrapolate\")\n",
    "    return interpolator(x_target)\n",
    "\n",
    "# Shift spectrum\n",
    "def shift_spectrum(spectrum, shift):\n",
    "    return np.roll(spectrum, shift)\n",
    "\n",
    "# Stretch spectrum\n",
    "def stretch_spectrum(spectrum, alpha):\n",
    "    original_len = len(spectrum)\n",
    "    new_len = int(original_len / alpha)\n",
    "    if new_len < 1:\n",
    "        new_len = 1\n",
    "    if new_len > original_len * 10:\n",
    "        new_len = original_len * 10\n",
    "    x_original = np.linspace(0, original_len - 1, original_len)\n",
    "    x_new = np.linspace(0, original_len - 1, new_len)\n",
    "    interpolator = interp1d(x_original, spectrum, kind='linear', fill_value=\"extrapolate\")\n",
    "    stretched = interpolator(x_new)\n",
    "    return interpolate_spectrum(stretched, new_len, original_len)\n",
    "\n",
    "# Generate synthetic spectrum\n",
    "def generate_synthetic_spectrum(input_spectrum, noise_data, spectrum_length=880):\n",
    "    \"\"\"\n",
    "    Tạo phổ tổng hợp từ một phổ gốc duy nhất:\n",
    "    - Thêm nhiễu từ noise_data và nhiễu Gaussian.\n",
    "    - Thêm đường nền (đa thức/Gaussian hoặc không).\n",
    "    - Dịch chuyển hoặc kéo dãn/nén.\n",
    "    - Không trộn với phổ khác.\n",
    "    \"\"\"\n",
    "    x_range = np.linspace(0, spectrum_length, spectrum_length)\n",
    "    # Add noise from dataset\n",
    "    noise_idx = np.random.randint(0, noise_data.shape[0])\n",
    "    pure = noise_data[noise_idx, 0, 0, :, 0]\n",
    "    noisy = noise_data[noise_idx, 0, 1, :, 0]\n",
    "    noise = noisy - pure  # Shape (880,)\n",
    "    scale = np.random.uniform(1.0, 2.0)\n",
    "    synthetic_spectrum = input_spectrum + noise * scale\n",
    "    # Add Gaussian noise\n",
    "    synthetic_spectrum += np.random.normal(0, 0.05 * np.std(input_spectrum), spectrum_length)\n",
    "    # Add synthetic baseline\n",
    "    baseline_type = np.random.choice(['poly', 'gaussian', 'none'], p=[0.3, 0.3, 0.4])\n",
    "    if baseline_type == 'poly':\n",
    "        baseline = poly_baseline(x_range, p=np.random.uniform(1.9, 2.1),\n",
    "                                intensity=np.random.uniform(0.75, 0.8),\n",
    "                                b=np.random.uniform(-0.1, 0.1))\n",
    "        synthetic_spectrum += baseline\n",
    "    elif baseline_type == 'gaussian':\n",
    "        baseline = gaussian_baseline(x_range, mean=np.random.uniform(0, spectrum_length),\n",
    "                                   sd=np.random.uniform(250, 300),\n",
    "                                   intensity=np.random.uniform(0.75, 0.8),\n",
    "                                   b=np.random.uniform(-0.1, 0.1))\n",
    "        synthetic_spectrum += baseline\n",
    "    # Probabilistic augmentation\n",
    "    aug_type = np.random.choice(['none', 'shift', 'stretch'], p=[0.5, 0.25, 0.25])\n",
    "    if aug_type == 'shift':\n",
    "        shift = np.random.randint(-10, 11)\n",
    "        synthetic_spectrum = shift_spectrum(synthetic_spectrum, shift)\n",
    "    elif aug_type == 'stretch':\n",
    "        alpha = np.random.uniform(0.5, 2.0)\n",
    "        synthetic_spectrum = stretch_spectrum(synthetic_spectrum, alpha)\n",
    "    return synthetic_spectrum\n",
    "\n",
    "# Create GADF map\n",
    "def create_gadf_map(spectrum, image_size=64):\n",
    "    spectrum = normalize_spectrum(spectrum)\n",
    "    spectrum = 2 * spectrum - 1\n",
    "    target_length = image_size * (spectrum.shape[0] // image_size)\n",
    "    if target_length != spectrum.shape[0]:\n",
    "        spectrum = interpolate_spectrum(spectrum, spectrum.shape[0], target_length)\n",
    "    gadf = GADF(image_size=image_size, overlapping=False, scale='-1')\n",
    "    return gadf.fit_transform(spectrum.reshape(1, -1))[0][:, :, np.newaxis]\n",
    "\n",
    "# Load Excel data\n",
    "def load_excel_data():\n",
    "    excel_path = os.path.join(data_dir, 'Ethanol_Methanol.xlsx')\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path, usecols='A:L')\n",
    "        raman_shift = df['Raman Shift (cm-1)'].values\n",
    "        spectra_data = {\n",
    "            'ethanol': df['Ethanol'].values,\n",
    "            'methanol': df['Methanol'].values,\n",
    "            'EM1_a': df['EM1_a'].values,\n",
    "            'EM2_a': df['EM2_a'].values,\n",
    "            'EM3_a': df['EM3_a'].values,\n",
    "            'EM4_a': df['EM4_a'].values,\n",
    "            'EM5_a': df['EM5_a'].values,\n",
    "            'EM6_a': df['EM6_a'].values,\n",
    "            'EM7_a': df['EM7_a'].values,\n",
    "            'EM8_a': df['EM8_a'].values,\n",
    "            'EM9_a': df['EM9_a'].values\n",
    "        }\n",
    "        for key in spectra_data:\n",
    "            spectrum = spectra_data[key]\n",
    "            spectrum = spectrum[~np.isnan(spectrum)]\n",
    "            spectra_data[key] = interpolate_spectrum(spectrum, len(spectrum), 880)\n",
    "        return spectra_data, raman_shift\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải dữ liệu từ {excel_path}: {e}\")\n",
    "        return {}, np.array([])\n",
    "\n",
    "# Load noise data\n",
    "def load_noise_data():\n",
    "    try:\n",
    "        # Noise data shape: (15000, 1, 2, 880, 1) - 15000 samples, 2 spectra (pure, noisy), 880 points\n",
    "        noise_data = np.load(os.path.join(data_dir, 'dataset_noise_pure_182.npy'))\n",
    "        return noise_data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải dữ liệu nhiễu: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# Process data\n",
    "spectra_data, raman_shift = load_excel_data()\n",
    "noise_data = load_noise_data()\n",
    "if not spectra_data or noise_data.size == 0:\n",
    "    raise FileNotFoundError(\"Không thể tải dữ liệu từ file Excel hoặc file nhiễu.\")\n",
    "\n",
    "# Initialize and train baseline model\n",
    "baseline_model = create_baseline_model(input_shape=880)\n",
    "try:\n",
    "    baseline_model.load_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    print(\"Trọng số mô hình baseline đã được tải thành công!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tải trọng số: {e}. Training baseline model.\")\n",
    "    baseline_model = train_baseline_model(baseline_model, noise_data)\n",
    "\n",
    "# Generate synthetic data\n",
    "X_1d = []\n",
    "X_2d = []\n",
    "labels = []\n",
    "sample_ids = []\n",
    "example_spectra = {}\n",
    "total_spectra = 0\n",
    "spectrum_length = 880\n",
    "\n",
    "ratio_to_label = {0.0: 0, 0.1: 1, 0.2: 2, 0.3: 3, 0.4: 4, 0.5: 5, 0.6: 6, 0.7: 7, 0.8: 8, 0.9: 9, 1.0: 10}\n",
    "ratios = {\n",
    "    'ethanol': 1.0,\n",
    "    'methanol': 0.0,\n",
    "    'EM1_a': 0.9,\n",
    "    'EM2_a': 0.8,\n",
    "    'EM3_a': 0.7,\n",
    "    'EM4_a': 0.6,\n",
    "    'EM5_a': 0.5,\n",
    "    'EM6_a': 0.4,\n",
    "    'EM7_a': 0.3,\n",
    "    'EM8_a': 0.2,\n",
    "    'EM9_a': 0.1\n",
    "}\n",
    "\n",
    "print(\"Bắt đầu sinh dữ liệu tổng hợp...\")\n",
    "for spectrum_type, ethanol_ratio in ratios.items():\n",
    "    print(f\"Xử lý {spectrum_type} với ethanol ratio {ethanol_ratio}...\")\n",
    "    input_spectrum = spectra_data[spectrum_type]\n",
    "    normalized_raw = normalize_spectrum(input_spectrum)\n",
    "    corrected_spectrum = enhanced_baseline_correction(normalized_raw, baseline_model)\n",
    "    corrected_spectrum = normalize_spectrum(corrected_spectrum)\n",
    "    X_1d.append(corrected_spectrum)\n",
    "    X_2d.append(create_gadf_map(corrected_spectrum, image_size=64))\n",
    "    label_idx = ratio_to_label[ethanol_ratio]\n",
    "    labels.append(label_idx)\n",
    "    sample_ids.append(f\"{spectrum_type}_original\")\n",
    "    total_spectra += 1\n",
    "    if total_spectra == 1 or ethanol_ratio not in example_spectra:\n",
    "        example_spectra[ethanol_ratio] = {\n",
    "            'raw': normalized_raw,\n",
    "            'corrected': corrected_spectrum,\n",
    "            'ethanol': spectra_data['ethanol'] if ethanol_ratio > 0 else None,\n",
    "            'methanol': spectra_data['methanol'] if ethanol_ratio < 1 else None\n",
    "        }\n",
    "    for i in range(999):\n",
    "        synthetic_spectrum = generate_synthetic_spectrum(normalized_raw, noise_data, spectrum_length)\n",
    "        corrected_spectrum = enhanced_baseline_correction(synthetic_spectrum, baseline_model)\n",
    "        corrected_spectrum = normalize_spectrum(corrected_spectrum)\n",
    "        X_1d.append(corrected_spectrum)\n",
    "        X_2d.append(create_gadf_map(corrected_spectrum, image_size=64))\n",
    "        labels.append(label_idx)\n",
    "        sample_ids.append(f\"{spectrum_type}_synthetic_{i}\")\n",
    "        total_spectra += 1\n",
    "        if i == 0:\n",
    "            example_spectra[ethanol_ratio] = {\n",
    "                'raw': synthetic_spectrum,\n",
    "                'corrected': corrected_spectrum,\n",
    "                'ethanol': spectra_data['ethanol'] if ethanol_ratio > 0 else None,\n",
    "                'methanol': spectra_data['methanol'] if ethanol_ratio < 1 else None\n",
    "            }\n",
    "        if total_spectra % 1000 == 0:\n",
    "            print(f\"Đã xử lý {total_spectra} phổ.\")\n",
    "\n",
    "print(f\"Tổng số phổ: {total_spectra}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_1d = np.array(X_1d)[:, :, np.newaxis]\n",
    "X_2d = np.array(X_2d)\n",
    "labels_df = pd.DataFrame({'label': labels, 'sample_id': sample_ids})\n",
    "\n",
    "print(\"X_1d shape:\", X_1d.shape)\n",
    "print(\"X_2d shape:\", X_2d.shape)\n",
    "print(\"labels_df shape:\", labels_df.shape)\n",
    "print(\"Label distribution:\\n\", labels_df[\"label\"].value_counts())\n",
    "\n",
    "# Save data\n",
    "labels_df.to_csv(os.path.join(labels_dir, \"labels.csv\"), index=False)\n",
    "np.save(os.path.join(synthetic_dir, \"synthetic_1d.npy\"), X_1d)\n",
    "np.save(os.path.join(maps_dir, \"spectral_maps_gadf.npy\"), X_2d)\n",
    "\n",
    "# Plot spectra\n",
    "def plot_spectra_comparison(wavenumbers, raw_spectrum, corrected_spectrum, ethanol_spectrum, methanol_spectrum, title, filename):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(wavenumbers, raw_spectrum, label=\"Raw Spectrum\", color='blue')\n",
    "    plt.axvspan(1000, 1020, color='red', alpha=0.2, label=\"Vùng Methanol\")\n",
    "    plt.axvspan(870, 890, color='green', alpha=0.2, label=\"Vùng Ethanol\")\n",
    "    plt.xlabel(\"Bước sóng (cm⁻¹)\")\n",
    "    plt.ylabel(\"Cường độ chuẩn hóa\")\n",
    "    plt.title(f\"Phổ Raw ({title})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(wavenumbers, corrected_spectrum, label=\"Corrected Spectrum\", color='orange')\n",
    "    if ethanol_spectrum is not None:\n",
    "        plt.plot(wavenumbers, normalize_spectrum(ethanol_spectrum), label=\"Ethanol Component\", color='green', linestyle='--')\n",
    "    if methanol_spectrum is not None:\n",
    "        plt.plot(wavenumbers, normalize_spectrum(methanol_spectrum), label=\"Methanol Component\", color='red', linestyle='--')\n",
    "    plt.axvspan(1000, 1020, color='red', alpha=0.2)\n",
    "    plt.axvspan(870, 890, color='green', alpha=0.2)\n",
    "    plt.xlabel(\"Bước sóng (cm⁻¹)\")\n",
    "    plt.ylabel(\"Cường độ chuẩn hóa\")\n",
    "    plt.title(f\"Phổ Sau Trừ Nền và Thành Phần ({title})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Sử dụng phạm vi wavenumbers từ 500–3500 cm⁻¹\n",
    "wavenumbers = np.linspace(500, 3500, 880)\n",
    "for ratio in example_spectra:\n",
    "    title = f\"Ratio Ethanol/Methanol = {ratio:.2f}/{1-ratio:.2f}\" if ratio < 1.0 else \"Pure Ethanol\" if ratio == 1.0 else \"Pure Methanol\"\n",
    "    plot_spectra_comparison(\n",
    "        wavenumbers,\n",
    "        example_spectra[ratio]['raw'],\n",
    "        example_spectra[ratio]['corrected'],\n",
    "        example_spectra[ratio]['ethanol'],\n",
    "        example_spectra[ratio]['methanol'],\n",
    "        title,\n",
    "        f\"spectra_comparison_ratio_{ratio:.2f}.png\"\n",
    "    )\n",
    "\n",
    "# Define DenseNet models\n",
    "def build_1d_densenet(input_shape=(880, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(48, 7, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv1D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv1D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_densenet(input_shape=(64, 64, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(48, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Define ResNet models\n",
    "def build_1d_resnet(input_shape=(880, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, 5, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_resnet(input_shape=(64, 64, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=[f\"{i*10}% Ethanol\" for i in range(11)],\n",
    "                yticklabels=[f\"{i*10}% Ethanol\" for i in range(11)])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation_1d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "data_augmentation_2d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "y = labels_df[\"label\"].values\n",
    "X_1d_train, X_1d_test, y_train, y_test = train_test_split(X_1d, y, test_size=0.2, random_state=42)\n",
    "X_2d_train, X_2d_test, y_train_2d, y_test_2d = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.arange(11), y=y)\n",
    "class_weight = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# Train models with fresh optimizers\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# DenseNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_densenet)\n",
    "densenet_1d = models.Sequential([data_augmentation_1d, build_1d_densenet()])\n",
    "densenet_1d.compile(optimizer=optimizer_1d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# DenseNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_densenet)\n",
    "densenet_2d = models.Sequential([data_augmentation_2d, build_2d_densenet()])\n",
    "densenet_2d.compile(optimizer=optimizer_2d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# ResNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_resnet)\n",
    "resnet_1d = models.Sequential([data_augmentation_1d, build_1d_resnet()])\n",
    "resnet_1d.compile(optimizer=optimizer_1d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# ResNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_resnet)\n",
    "resnet_2d = models.Sequential([data_augmentation_2d, build_2d_resnet()])\n",
    "resnet_2d.compile(optimizer=optimizer_2d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_1d_densenet = densenet_1d.predict(X_1d_test)\n",
    "y_pred_2d_densenet = densenet_2d.predict(X_2d_test)\n",
    "y_pred_1d_resnet = resnet_1d.predict(X_1d_test)\n",
    "y_pred_2d_resnet = resnet_2d.predict(X_2d_test)\n",
    "\n",
    "y_pred_1d_densenet_labels = np.argmax(y_pred_1d_densenet, axis=1)\n",
    "y_pred_2d_densenet_labels = np.argmax(y_pred_2d_densenet, axis=1)\n",
    "y_pred_1d_resnet_labels = np.argmax(y_pred_1d_resnet, axis=1)\n",
    "y_pred_2d_resnet_labels = np.argmax(y_pred_2d_resnet, axis=1)\n",
    "\n",
    "densenet_1d_acc = np.mean(y_pred_1d_densenet_labels == y_test)\n",
    "densenet_2d_acc = np.mean(y_pred_2d_densenet_labels == y_test_2d)\n",
    "resnet_1d_acc = np.mean(y_pred_1d_resnet_labels == y_test)\n",
    "resnet_2d_acc = np.mean(y_pred_2d_resnet_labels == y_test_2d)\n",
    "\n",
    "densenet_1d_precision = precision_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_precision = precision_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_precision = precision_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_precision = precision_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_recall = recall_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_recall = recall_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_recall = recall_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_recall = recall_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_f1 = f1_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_f1 = f1_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_f1 = f1_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_f1 = f1_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "print(\"\\nKết quả đánh giá:\")\n",
    "print(f\"DenseNet 1D - Accuracy: {densenet_1d_acc:.4f}, Precision: {densenet_1d_precision:.4f}, Recall: {densenet_1d_recall:.4f}, F1: {densenet_1d_f1:.4f}\")\n",
    "print(f\"DenseNet 2D (GADF) - Accuracy: {densenet_2d_acc:.4f}, Precision: {densenet_2d_precision:.4f}, Recall: {densenet_2d_recall:.4f}, F1: {densenet_2d_f1:.4f}\")\n",
    "print(f\"ResNet 1D - Accuracy: {resnet_1d_acc:.4f}, Precision: {resnet_1d_precision:.4f}, Recall: {resnet_1d_recall:.4f}, F1: {resnet_1d_f1:.4f}\")\n",
    "print(f\"ResNet 2D (GADF) - Accuracy: {resnet_2d_acc:.4f}, Precision: {resnet_2d_precision:.4f}, Recall: {resnet_2d_recall:.4f}, F1: {resnet_2d_f1:.4f}\")\n",
    "\n",
    "# Save results\n",
    "plot_confusion_matrix(y_test, y_pred_1d_densenet_labels, \"Confusion Matrix - DenseNet 1D\", \"cm_densenet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_densenet_labels, \"Confusion Matrix - DenseNet 2D (GADF)\", \"cm_densenet_2d_gadf.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_1d_resnet_labels, \"Confusion Matrix - ResNet 1D\", \"cm_resnet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_resnet_labels, \"Confusion Matrix - ResNet 2D (GADF)\", \"cm_resnet_2d_gadf.png\")\n",
    "\n",
    "densenet_1d.save(os.path.join(model_dir, 'densenet_1d_full.keras'))\n",
    "densenet_2d.save(os.path.join(model_dir, 'densenet_2d_full.keras'))\n",
    "resnet_1d.save(os.path.join(model_dir, 'resnet_1d_full.keras'))\n",
    "resnet_2d.save(os.path.join(model_dir, 'resnet_2d_full.keras'))\n",
    "\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_densenet.npy'), y_pred_1d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_densenet.npy'), y_pred_2d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_resnet.npy'), y_pred_1d_resnet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_resnet.npy'), y_pred_2d_resnet)\n",
    "\n",
    "metrics = {\n",
    "    'densenet_1d': {'accuracy': densenet_1d_acc, 'precision': densenet_1d_precision, 'recall': densenet_1d_recall, 'f1': densenet_1d_f1},\n",
    "    'densenet_2d': {'accuracy': densenet_2d_acc, 'precision': densenet_2d_precision, 'recall': densenet_2d_recall, 'f1': densenet_2d_f1},\n",
    "    'resnet_1d': {'accuracy': resnet_1d_acc, 'precision': resnet_1d_precision, 'recall': resnet_1d_recall, 'f1': resnet_1d_f1},\n",
    "    'resnet_2d': {'accuracy': resnet_2d_acc, 'precision': resnet_2d_precision, 'recall': resnet_2d_recall, 'f1': resnet_2d_f1}\n",
    "}\n",
    "with open(os.path.join(results_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ],
   "id": "cb26a07948bb3caa",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 335\u001B[0m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([])\n\u001B[0;32m    334\u001B[0m \u001B[38;5;66;03m# Process data\u001B[39;00m\n\u001B[1;32m--> 335\u001B[0m spectra_data, raman_shift \u001B[38;5;241m=\u001B[39m \u001B[43mload_excel_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    336\u001B[0m noise_data \u001B[38;5;241m=\u001B[39m load_noise_data()\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m spectra_data \u001B[38;5;129;01mor\u001B[39;00m noise_data\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[1], line 300\u001B[0m, in \u001B[0;36mload_excel_data\u001B[1;34m()\u001B[0m\n\u001B[0;32m    298\u001B[0m excel_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(data_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEthanol_Methanol.xlsx\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 300\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_excel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexcel_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musecols\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mA:L\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    301\u001B[0m     raman_shift \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRaman Shift (cm-1)\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m    302\u001B[0m     spectra_data \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    303\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124methanol\u001B[39m\u001B[38;5;124m'\u001B[39m: df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEthanol\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues,\n\u001B[0;32m    304\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmethanol\u001B[39m\u001B[38;5;124m'\u001B[39m: df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMethanol\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    313\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEM9_a\u001B[39m\u001B[38;5;124m'\u001B[39m: df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEM9_a\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m    314\u001B[0m     }\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001B[0m, in \u001B[0;36mread_excel\u001B[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001B[0m\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(io, ExcelFile):\n\u001B[0;32m    494\u001B[0m     should_close \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 495\u001B[0m     io \u001B[38;5;241m=\u001B[39m \u001B[43mExcelFile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine \u001B[38;5;129;01mand\u001B[39;00m engine \u001B[38;5;241m!=\u001B[39m io\u001B[38;5;241m.\u001B[39mengine:\n\u001B[0;32m    502\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    503\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEngine should not be specified when passing \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124man ExcelFile - ExcelFile already has the engine set\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1567\u001B[0m, in \u001B[0;36mExcelFile.__init__\u001B[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001B[0m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mengine \u001B[38;5;241m=\u001B[39m engine\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstorage_options \u001B[38;5;241m=\u001B[39m storage_options\n\u001B[1;32m-> 1567\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reader \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engines\u001B[49m\u001B[43m[\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1568\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_io\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1569\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1570\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1571\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:552\u001B[0m, in \u001B[0;36mOpenpyxlReader.__init__\u001B[1;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001B[0m\n\u001B[0;32m    534\u001B[0m \u001B[38;5;129m@doc\u001B[39m(storage_options\u001B[38;5;241m=\u001B[39m_shared_docs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstorage_options\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    535\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    536\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    539\u001B[0m     engine_kwargs: \u001B[38;5;28mdict\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    540\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    541\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    542\u001B[0m \u001B[38;5;124;03m    Reader using openpyxl engine.\u001B[39;00m\n\u001B[0;32m    543\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;124;03m        Arbitrary keyword arguments passed to excel engine.\u001B[39;00m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 552\u001B[0m     \u001B[43mimport_optional_dependency\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mopenpyxl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    554\u001B[0m         filepath_or_buffer,\n\u001B[0;32m    555\u001B[0m         storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    556\u001B[0m         engine_kwargs\u001B[38;5;241m=\u001B[39mengine_kwargs,\n\u001B[0;32m    557\u001B[0m     )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\pandas\\compat\\_optional.py:135\u001B[0m, in \u001B[0;36mimport_optional_dependency\u001B[1;34m(name, extra, errors, min_version)\u001B[0m\n\u001B[0;32m    130\u001B[0m msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    131\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing optional dependency \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minstall_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mextra\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    132\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUse pip or conda to install \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minstall_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    133\u001B[0m )\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 135\u001B[0m     module \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\importlib\\__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m    125\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1030\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1007\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:986\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:680\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:850\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:228\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\openpyxl\\__init__.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnumbers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NUMPY\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxml\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DEFUSEDXML, LXML\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mworkbook\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Workbook\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreader\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexcel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_workbook \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;28mopen\u001B[39m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreader\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexcel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m load_workbook\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\openpyxl\\workbook\\__init__.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Copyright (c) 2010-2024 openpyxl\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mworkbook\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Workbook\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\openpyxl\\workbook\\workbook.py:17\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdatetime\u001B[39;00m\u001B[38;5;250m  \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m WINDOWS_EPOCH, MAC_EPOCH\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexceptions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ReadOnlyWorkbookException\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwriter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexcel\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m save_workbook\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstyles\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcell_style\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StyleArray\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstyles\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnamed_styles\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m NamedStyle\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\openpyxl\\writer\\excel.py:24\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdrawing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspreadsheet_drawing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m SpreadsheetDrawing\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tostring, fromstring\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmanifest\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Manifest\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrelationship\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     26\u001B[0m     get_rels_path,\n\u001B[0;32m     27\u001B[0m     RelationshipList,\n\u001B[0;32m     28\u001B[0m     Relationship,\n\u001B[0;32m     29\u001B[0m )\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomments\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomment_sheet\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m CommentSheet\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\openpyxl\\packaging\\manifest.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mopenpyxl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mxml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m tostring\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# initialise mime-types\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m mimetypes \u001B[38;5;241m=\u001B[39m \u001B[43mMimeTypes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m mimetypes\u001B[38;5;241m.\u001B[39madd_type(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapplication/xml\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.xml\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     28\u001B[0m mimetypes\u001B[38;5;241m.\u001B[39madd_type(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mapplication/vnd.openxmlformats-package.relationships+xml\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.rels\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\mimetypes.py:68\u001B[0m, in \u001B[0;36mMimeTypes.__init__\u001B[1;34m(self, filenames, strict)\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, filenames\u001B[38;5;241m=\u001B[39m(), strict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     67\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m inited:\n\u001B[1;32m---> 68\u001B[0m         \u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencodings_map \u001B[38;5;241m=\u001B[39m _encodings_map_default\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msuffix_map \u001B[38;5;241m=\u001B[39m _suffix_map_default\u001B[38;5;241m.\u001B[39mcopy()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\mimetypes.py:351\u001B[0m, in \u001B[0;36minit\u001B[1;34m(files)\u001B[0m\n\u001B[0;32m    349\u001B[0m db \u001B[38;5;241m=\u001B[39m MimeTypes()\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _winreg:\n\u001B[1;32m--> 351\u001B[0m     \u001B[43mdb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_windows_registry\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m files \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    354\u001B[0m     files \u001B[38;5;241m=\u001B[39m knownfiles\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raman_NCKH\\lib\\mimetypes.py:257\u001B[0m, in \u001B[0;36mMimeTypes.read_windows_registry\u001B[1;34m(self, strict)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m subkeyname \u001B[38;5;129;01min\u001B[39;00m enum_types(hkcr):\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 257\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m _winreg\u001B[38;5;241m.\u001B[39mOpenKey(hkcr, subkeyname) \u001B[38;5;28;01mas\u001B[39;00m subkey:\n\u001B[0;32m    258\u001B[0m             \u001B[38;5;66;03m# Only check file extensions\u001B[39;00m\n\u001B[0;32m    259\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m subkeyname\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    260\u001B[0m                 \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Re-run Model (best-model form experiment)",
   "id": "ebd1b5a013eff628"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers, models, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Set a fixed seed\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "# Define directories\n",
    "data_dir = 'data'\n",
    "synthetic_dir = os.path.join(data_dir, 'synthetic')\n",
    "maps_dir = os.path.join(data_dir, 'maps')\n",
    "labels_dir = os.path.join(data_dir, 'labels')\n",
    "visualizations_dir = os.path.join(data_dir, 'visualizations')\n",
    "experiment_dir = os.path.join('experiments', 'experiment_20250914_220130')  # Thay bằng timestamp của lần chạy trước\n",
    "model_dir = os.path.join(experiment_dir, 'models')\n",
    "results_dir = os.path.join(experiment_dir, 'results')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Load saved data\n",
    "X_1d = np.load(os.path.join(synthetic_dir, \"synthetic_1d.npy\"))\n",
    "X_2d = np.load(os.path.join(maps_dir, \"spectral_maps_gadf.npy\"))\n",
    "labels_df = pd.read_csv(os.path.join(labels_dir, \"labels.csv\"))\n",
    "y = labels_df[\"label\"].values\n",
    "\n",
    "print(\"X_1d shape:\", X_1d.shape)\n",
    "print(\"X_2d shape:\", X_2d.shape)\n",
    "print(\"labels_df shape:\", labels_df.shape)\n",
    "print(\"Label distribution:\\n\", labels_df[\"label\"].value_counts())\n",
    "\n",
    "# Define baseline model (for completeness, in case needed)\n",
    "def create_baseline_model(input_shape=880):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Reshape((input_shape, 1)),\n",
    "        layers.Conv1D(filters=16, kernel_size=5, strides=1, activation='relu'),\n",
    "        layers.AveragePooling1D(pool_size=2, strides=2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(2, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def train_baseline_model(baseline_model, noise_data, epochs=10, batch_size=32):\n",
    "    try:\n",
    "        labels = np.load(os.path.join(data_dir, 'labels_noise_pure_182.npy'))\n",
    "        print(\"Đã tải nhãn từ labels_noise_pure_182.npy thành công!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải nhãn: {e}. Sử dụng nhãn ngẫu nhiên.\")\n",
    "        labels = np.random.randint(0, 2, size=noise_data.shape[0])\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(noise_data.shape[0]):\n",
    "        pure = noise_data[i, 0, 0, :, 0]\n",
    "        noisy = noise_data[i, 0, 1, :, 0]\n",
    "        X.append(noisy)\n",
    "        y.append(labels[i])\n",
    "    X = np.array(X)[:, :, np.newaxis]\n",
    "    y = np.array(y)\n",
    "    baseline_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    baseline_model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    baseline_model.save_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    return baseline_model\n",
    "\n",
    "# Load noise data (if needed for baseline model training)\n",
    "def load_noise_data():\n",
    "    try:\n",
    "        noise_data = np.load(os.path.join(data_dir, 'dataset_noise_pure_182.npy'))\n",
    "        return noise_data\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi tải dữ liệu nhiễu: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "# Load or train baseline model\n",
    "baseline_model = create_baseline_model(input_shape=880)\n",
    "try:\n",
    "    baseline_model.load_weights(os.path.join(data_dir, 'model.weights.h5'))\n",
    "    print(\"Trọng số mô hình baseline đã được tải thành công!\")\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi tải trọng số: {e}. Training baseline model.\")\n",
    "    noise_data = load_noise_data()\n",
    "    if noise_data.size == 0:\n",
    "        raise FileNotFoundError(\"Không thể tải dữ liệu nhiễu.\")\n",
    "    baseline_model = train_baseline_model(baseline_model, noise_data)\n",
    "\n",
    "# Define DenseNet models\n",
    "def build_1d_densenet(input_shape=(880, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(48, 7, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv1D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv1D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_densenet(input_shape=(64, 64, 1), num_classes=11, growth_rate=12):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(48, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def dense_block(x, num_layers, filters):\n",
    "        for _ in range(num_layers):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.Activation('relu')(y)\n",
    "            y = layers.Conv2D(filters, 3, padding='same', kernel_regularizer=regularizers.l2(0.0005))(y)\n",
    "            x = layers.Concatenate()([x, y])\n",
    "        return x\n",
    "    def transition_layer(x):\n",
    "        filters = x.shape[-1]\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2D(filters // 2, 1, padding='same', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "    for _ in range(3):\n",
    "        x = dense_block(x, num_layers=4, filters=growth_rate)\n",
    "        x = transition_layer(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Define ResNet models\n",
    "def build_1d_resnet(input_shape=(880, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(64, 5, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv1D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 64)\n",
    "    x = residual_block(x, 64)\n",
    "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = residual_block(x, 128)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_2d_resnet(input_shape=(64, 64, 1), num_classes=11):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    def residual_block(x, filters, kernel_size=3):\n",
    "        shortcut = x\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(filters, kernel_size, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if shortcut.shape[-1] != filters:\n",
    "            shortcut = layers.Conv2D(filters, 1, padding='same')(shortcut)\n",
    "        x = layers.Add()([shortcut, x])\n",
    "        x = layers.Activation('relu')(x)\n",
    "        return x\n",
    "    x = residual_block(x, 32)\n",
    "    x = residual_block(x, 32)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "# Plot confusion matrix (updated version)\n",
    "def plot_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f', cmap='viridis', cbar=True,  # Changed to normalized values and viridis colormap\n",
    "                xticklabels=[f\"{i*10}% Ethanol\" for i in range(11)],\n",
    "                yticklabels=[f\"{i*10}% Ethanol\" for i in range(11)],\n",
    "                annot_kws={\"size\": 10}, norm=plt.Normalize(vmin=0, vmax=np.max(cm)))\n",
    "    plt.title(title, fontsize=14, pad=15)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Data augmentation\n",
    "data_augmentation_1d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "data_augmentation_2d = models.Sequential([\n",
    "    layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=0.05)),\n",
    "    layers.Lambda(lambda x: x * tf.random.uniform((), 0.8, 1.2)),\n",
    "    layers.Lambda(lambda x: tf.roll(x, shift=tf.random.uniform((), -5, 5, dtype=tf.int32), axis=1))\n",
    "])\n",
    "\n",
    "# Split data\n",
    "X_1d_train, X_1d_test, y_train, y_test = train_test_split(X_1d, y, test_size=0.2, random_state=42)\n",
    "X_2d_train, X_2d_test, y_train_2d, y_test_2d = train_test_split(X_2d, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.arange(11), y=y)\n",
    "class_weight = {i: w for i, w in enumerate(class_weights)}\n",
    "\n",
    "# Train models with fresh optimizers\n",
    "early_stopping = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# DenseNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_densenet)\n",
    "densenet_1d = models.Sequential([data_augmentation_1d, build_1d_densenet()])\n",
    "densenet_1d.compile(optimizer=optimizer_1d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# DenseNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_densenet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_densenet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_densenet)\n",
    "densenet_2d = models.Sequential([data_augmentation_2d, build_2d_densenet()])\n",
    "densenet_2d.compile(optimizer=optimizer_2d_densenet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "densenet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "                callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_densenet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "                class_weight=class_weight)\n",
    "\n",
    "# ResNet 1D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_1d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_1d_train)//64)\n",
    "optimizer_1d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_1d_resnet)\n",
    "resnet_1d = models.Sequential([data_augmentation_1d, build_1d_resnet()])\n",
    "resnet_1d.compile(optimizer=optimizer_1d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_1d.fit(X_1d_train, y_train, validation_split=0.1, epochs=10, batch_size=64,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_1d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# ResNet 2D\n",
    "tf.keras.backend.clear_session()\n",
    "lr_schedule_2d_resnet = keras.optimizers.schedules.CosineDecay(initial_learning_rate=0.001, decay_steps=10*len(X_2d_train)//32)\n",
    "optimizer_2d_resnet = keras.optimizers.Adam(learning_rate=lr_schedule_2d_resnet)\n",
    "resnet_2d = models.Sequential([data_augmentation_2d, build_2d_resnet()])\n",
    "resnet_2d.compile(optimizer=optimizer_2d_resnet, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "resnet_2d.fit(X_2d_train, y_train_2d, validation_split=0.1, epochs=10, batch_size=32,\n",
    "              callbacks=[keras.callbacks.ModelCheckpoint(os.path.join(model_dir, \"best_resnet_2d.keras\"), save_best_only=True), early_stopping],\n",
    "              class_weight=class_weight)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_1d_densenet = densenet_1d.predict(X_1d_test)\n",
    "y_pred_2d_densenet = densenet_2d.predict(X_2d_test)\n",
    "y_pred_1d_resnet = resnet_1d.predict(X_1d_test)\n",
    "y_pred_2d_resnet = resnet_2d.predict(X_2d_test)\n",
    "\n",
    "y_pred_1d_densenet_labels = np.argmax(y_pred_1d_densenet, axis=1)\n",
    "y_pred_2d_densenet_labels = np.argmax(y_pred_2d_densenet, axis=1)\n",
    "y_pred_1d_resnet_labels = np.argmax(y_pred_1d_resnet, axis=1)\n",
    "y_pred_2d_resnet_labels = np.argmax(y_pred_2d_resnet, axis=1)\n",
    "\n",
    "densenet_1d_acc = np.mean(y_pred_1d_densenet_labels == y_test)\n",
    "densenet_2d_acc = np.mean(y_pred_2d_densenet_labels == y_test_2d)\n",
    "resnet_1d_acc = np.mean(y_pred_1d_resnet_labels == y_test)\n",
    "resnet_2d_acc = np.mean(y_pred_2d_resnet_labels == y_test_2d)\n",
    "\n",
    "densenet_1d_precision = precision_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_precision = precision_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_precision = precision_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_precision = precision_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_recall = recall_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_recall = recall_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_recall = recall_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_recall = recall_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "densenet_1d_f1 = f1_score(y_test, y_pred_1d_densenet_labels, average='macro')\n",
    "densenet_2d_f1 = f1_score(y_test_2d, y_pred_2d_densenet_labels, average='macro')\n",
    "resnet_1d_f1 = f1_score(y_test, y_pred_1d_resnet_labels, average='macro')\n",
    "resnet_2d_f1 = f1_score(y_test_2d, y_pred_2d_resnet_labels, average='macro')\n",
    "\n",
    "print(\"\\nKết quả đánh giá:\")\n",
    "print(f\"DenseNet 1D - Accuracy: {densenet_1d_acc:.4f}, Precision: {densenet_1d_precision:.4f}, Recall: {densenet_1d_recall:.4f}, F1: {densenet_1d_f1:.4f}\")\n",
    "print(f\"DenseNet 2D (GADF) - Accuracy: {densenet_2d_acc:.4f}, Precision: {densenet_2d_precision:.4f}, Recall: {densenet_2d_recall:.4f}, F1: {densenet_2d_f1:.4f}\")\n",
    "print(f\"ResNet 1D - Accuracy: {resnet_1d_acc:.4f}, Precision: {resnet_1d_precision:.4f}, Recall: {resnet_1d_recall:.4f}, F1: {resnet_1d_f1:.4f}\")\n",
    "print(f\"ResNet 2D (GADF) - Accuracy: {resnet_2d_acc:.4f}, Precision: {resnet_2d_precision:.4f}, Recall: {resnet_2d_recall:.4f}, F1: {resnet_2d_f1:.4f}\")\n",
    "\n",
    "# Save results\n",
    "plot_confusion_matrix(y_test, y_pred_1d_densenet_labels, \"Confusion Matrix - DenseNet 1D\", \"cm_densenet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_densenet_labels, \"Confusion Matrix - DenseNet 2D (GADF)\", \"cm_densenet_2d_gadf.png\")\n",
    "plot_confusion_matrix(y_test, y_pred_1d_resnet_labels, \"Confusion Matrix - ResNet 1D\", \"cm_resnet_1d.png\")\n",
    "plot_confusion_matrix(y_test_2d, y_pred_2d_resnet_labels, \"Confusion Matrix - ResNet 2D (GADF)\", \"cm_resnet_2d_gadf.png\")\n",
    "\n",
    "densenet_1d.save(os.path.join(model_dir, 'densenet_1d_full.keras'))\n",
    "densenet_2d.save(os.path.join(model_dir, 'densenet_2d_full.keras'))\n",
    "resnet_1d.save(os.path.join(model_dir, 'resnet_1d_full.keras'))\n",
    "resnet_2d.save(os.path.join(model_dir, 'resnet_2d_full.keras'))\n",
    "\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_densenet.npy'), y_pred_1d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_densenet.npy'), y_pred_2d_densenet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_1d_resnet.npy'), y_pred_1d_resnet)\n",
    "np.save(os.path.join(results_dir, 'y_pred_2d_resnet.npy'), y_pred_2d_resnet)\n",
    "\n",
    "metrics = {\n",
    "    'densenet_1d': {'accuracy': densenet_1d_acc, 'precision': densenet_1d_precision, 'recall': densenet_1d_recall, 'f1': densenet_1d_f1},\n",
    "    'densenet_2d': {'accuracy': densenet_2d_acc, 'precision': densenet_2d_precision, 'recall': densenet_2d_recall, 'f1': densenet_2d_f1},\n",
    "    'resnet_1d': {'accuracy': resnet_1d_acc, 'precision': resnet_1d_precision, 'recall': resnet_1d_recall, 'f1': resnet_1d_f1},\n",
    "    'resnet_2d': {'accuracy': resnet_2d_acc, 'precision': resnet_2d_precision, 'recall': resnet_2d_recall, 'f1': resnet_2d_f1}\n",
    "}\n",
    "with open(os.path.join(results_dir, 'metrics.json'), 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)"
   ],
   "id": "47660530d40a3884"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Visualization",
   "id": "64c45e18bc03e77f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T09:44:56.845565Z",
     "start_time": "2025-09-15T09:44:52.200819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set matplotlib font to support superscript minus\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# =======================================================\n",
    "# Directories\n",
    "# =======================================================\n",
    "data_dir = 'data'\n",
    "visualizations_dir = os.path.join(data_dir, 'visualizations')\n",
    "experiment_dir = os.path.join('experiments', 'experiment_20250914_220130')  # Fixed timestamp\n",
    "results_dir = os.path.join(experiment_dir, 'results')\n",
    "labels_dir = os.path.join(data_dir, 'labels')\n",
    "\n",
    "# Create visualizations directory if it doesn't exist\n",
    "os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "# =======================================================\n",
    "# Load test labels\n",
    "# =======================================================\n",
    "try:\n",
    "    labels_df = pd.read_csv(os.path.join(labels_dir, 'labels.csv'))\n",
    "    y = labels_df['label'].values\n",
    "    _, y_test = train_test_split(y, test_size=0.2, random_state=42)\n",
    "    print(\"Test labels loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading labels:\", str(e))\n",
    "    raise\n",
    "\n",
    "# Convert ethanol labels to methanol (100 - ethanol%)\n",
    "y_test_methanol = 100 - y_test * 10  # Assuming y_test is in range 0-10 for 0-100%\n",
    "\n",
    "# =======================================================\n",
    "# Load predictions\n",
    "# =======================================================\n",
    "try:\n",
    "    y_pred_1d_densenet = np.load(os.path.join(results_dir, 'y_pred_1d_densenet.npy'))\n",
    "    y_pred_2d_densenet = np.load(os.path.join(results_dir, 'y_pred_2d_densenet.npy'))\n",
    "    y_pred_1d_resnet = np.load(os.path.join(results_dir, 'y_pred_1d_resnet.npy'))\n",
    "    y_pred_2d_resnet = np.load(os.path.join(results_dir, 'y_pred_2d_resnet.npy'))\n",
    "    print(\"All prediction files loaded successfully!\")\n",
    "except FileNotFoundError as e:\n",
    "    print(\"Error: One or more prediction files not found:\", str(e))\n",
    "    print(\"Please ensure all files (y_pred_1d_densenet.npy, y_pred_2d_densenet.npy, \"\n",
    "          \"y_pred_1d_resnet.npy, y_pred_2d_resnet.npy) exist in\", results_dir)\n",
    "    raise\n",
    "\n",
    "# Convert predictions to labels\n",
    "y_pred_1d_densenet_labels = np.argmax(y_pred_1d_densenet, axis=1)\n",
    "y_pred_2d_densenet_labels = np.argmax(y_pred_2d_densenet, axis=1)\n",
    "y_pred_1d_resnet_labels = np.argmax(y_pred_1d_resnet, axis=1)\n",
    "y_pred_2d_resnet_labels = np.argmax(y_pred_2d_resnet, axis=1)\n",
    "\n",
    "# Convert predicted ethanol labels to methanol (100 - ethanol%)\n",
    "y_pred_1d_densenet_labels_methanol = 100 - y_pred_1d_densenet_labels * 10\n",
    "y_pred_2d_densenet_labels_methanol = 100 - y_pred_2d_densenet_labels * 10\n",
    "y_pred_1d_resnet_labels_methanol = 100 - y_pred_1d_resnet_labels * 10\n",
    "y_pred_2d_resnet_labels_methanol = 100 - y_pred_2d_resnet_labels * 10\n",
    "\n",
    "# =======================================================\n",
    "# Normalized confusion matrix function\n",
    "# =======================================================\n",
    "def calculate_normalized_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "    cm_normalized = np.nan_to_num(cm_normalized, nan=0.0)\n",
    "    return cm_normalized\n",
    "\n",
    "def plot_normalized_confusion_matrix(y_true, y_pred, title, filename):\n",
    "    cm_normalized = calculate_normalized_confusion_matrix(y_true, y_pred)\n",
    "    labels = [f\"{i}%\" for i in range(0, 101, 10)][::-1]  # Reversed for methanol (100% to 0%)\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.1f', cmap='Blues',\n",
    "                xticklabels=labels, yticklabels=labels, cbar_kws={'label': '%'},\n",
    "                annot_kws={'size': 14},  # Increase annotation font size\n",
    "                square=True)  # Ensure square cells\n",
    "    plt.xlabel('Predicted Methanol Ratio', fontsize=18, weight='bold')\n",
    "    plt.ylabel('True Methanol Ratio', fontsize=18, weight='bold')\n",
    "    plt.title(title, fontsize=20, weight='bold', pad=20)\n",
    "    plt.xticks(fontsize=14, rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=14, rotation=0)\n",
    "    cbar = plt.gcf().axes[-1]\n",
    "    cbar.set_ylabel('%', fontsize=16, weight='bold')\n",
    "    cbar.tick_params(labelsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(visualizations_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =======================================================\n",
    "# Plot normalized confusion matrices for all models\n",
    "# =======================================================\n",
    "# DenseNet 1D\n",
    "plot_normalized_confusion_matrix(\n",
    "    y_test_methanol,\n",
    "    y_pred_1d_densenet_labels_methanol,\n",
    "    \"Normalized Confusion Matrix for DenseNet 1D (%)\",\n",
    "    \"normalized_confusion_matrix_densenet_1d.png\"\n",
    ")\n",
    "\n",
    "# DenseNet 2D\n",
    "plot_normalized_confusion_matrix(\n",
    "    y_test_methanol,\n",
    "    y_pred_2d_densenet_labels_methanol,\n",
    "    \"Normalized Confusion Matrix for DenseNet 2D (%)\",\n",
    "    \"normalized_confusion_matrix_densenet_2d.png\"\n",
    ")\n",
    "\n",
    "# ResNet 1D\n",
    "plot_normalized_confusion_matrix(\n",
    "    y_test_methanol,\n",
    "    y_pred_1d_resnet_labels_methanol,\n",
    "    \"Normalized Confusion Matrix for ResNet 1D (%)\",\n",
    "    \"normalized_confusion_matrix_resnet_1d.png\"\n",
    ")\n",
    "\n",
    "# ResNet 2D\n",
    "plot_normalized_confusion_matrix(\n",
    "    y_test_methanol,\n",
    "    y_pred_2d_resnet_labels_methanol,\n",
    "    \"Normalized Confusion Matrix for ResNet 2D (%)\",\n",
    "    \"normalized_confusion_matrix_resnet_2d.png\"\n",
    ")\n",
    "\n",
    "# =======================================================\n",
    "# Print normalized confusion matrix values\n",
    "# =======================================================\n",
    "models = [\n",
    "    ('DenseNet 1D', y_pred_1d_densenet_labels_methanol),\n",
    "    ('DenseNet 2D', y_pred_2d_densenet_labels_methanol),\n",
    "    ('ResNet 1D', y_pred_1d_resnet_labels_methanol),\n",
    "    ('ResNet 2D', y_pred_2d_resnet_labels_methanol)\n",
    "]\n",
    "\n",
    "for model_name, y_pred_labels in models:\n",
    "    cm_normalized = calculate_normalized_confusion_matrix(y_test_methanol, y_pred_labels)\n",
    "    print(f\"\\nNormalized Confusion Matrix for {model_name} (%):\")\n",
    "    for i, row in enumerate(cm_normalized):\n",
    "        print(f\"True {100 - i*10}% Methanol:\")\n",
    "        for j, value in enumerate(row):\n",
    "            print(f\"  Predicted {100 - j*10}%: {value:.1f}%\")"
   ],
   "id": "b0a40a270fe1e90e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels loaded successfully!\n",
      "All prediction files loaded successfully!\n",
      "\n",
      "Normalized Confusion Matrix for DenseNet 1D (%):\n",
      "True 100% Methanol:\n",
      "  Predicted 100%: 91.7%\n",
      "  Predicted 90%: 7.8%\n",
      "  Predicted 80%: 0.5%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 90% Methanol:\n",
      "  Predicted 100%: 3.8%\n",
      "  Predicted 90%: 92.9%\n",
      "  Predicted 80%: 3.3%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 80% Methanol:\n",
      "  Predicted 100%: 0.9%\n",
      "  Predicted 90%: 7.9%\n",
      "  Predicted 80%: 90.2%\n",
      "  Predicted 70%: 0.9%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 70% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.5%\n",
      "  Predicted 80%: 4.1%\n",
      "  Predicted 70%: 93.4%\n",
      "  Predicted 60%: 2.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 60% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.5%\n",
      "  Predicted 70%: 8.3%\n",
      "  Predicted 60%: 90.7%\n",
      "  Predicted 50%: 0.5%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 50% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.5%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 97.6%\n",
      "  Predicted 40%: 1.9%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 40% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 3.9%\n",
      "  Predicted 40%: 91.6%\n",
      "  Predicted 30%: 4.5%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 30% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 7.6%\n",
      "  Predicted 30%: 86.9%\n",
      "  Predicted 20%: 5.6%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 20% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 5.2%\n",
      "  Predicted 20%: 87.6%\n",
      "  Predicted 10%: 7.2%\n",
      "  Predicted 0%: 0.0%\n",
      "True 10% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.9%\n",
      "  Predicted 10%: 95.3%\n",
      "  Predicted 0%: 3.8%\n",
      "True 0% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 1.5%\n",
      "  Predicted 0%: 98.5%\n",
      "\n",
      "Normalized Confusion Matrix for DenseNet 2D (%):\n",
      "True 100% Methanol:\n",
      "  Predicted 100%: 98.1%\n",
      "  Predicted 90%: 1.9%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 90% Methanol:\n",
      "  Predicted 100%: 1.1%\n",
      "  Predicted 90%: 98.4%\n",
      "  Predicted 80%: 0.5%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 80% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 3.3%\n",
      "  Predicted 80%: 95.3%\n",
      "  Predicted 70%: 1.4%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 70% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 1.0%\n",
      "  Predicted 70%: 98.5%\n",
      "  Predicted 60%: 0.5%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 60% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.5%\n",
      "  Predicted 60%: 99.5%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 50% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 99.0%\n",
      "  Predicted 40%: 1.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 40% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 1.1%\n",
      "  Predicted 40%: 98.3%\n",
      "  Predicted 30%: 0.6%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 30% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.5%\n",
      "  Predicted 30%: 98.0%\n",
      "  Predicted 20%: 1.5%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 20% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.5%\n",
      "  Predicted 20%: 88.1%\n",
      "  Predicted 10%: 11.3%\n",
      "  Predicted 0%: 0.0%\n",
      "True 10% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 5.2%\n",
      "  Predicted 10%: 94.8%\n",
      "  Predicted 0%: 0.0%\n",
      "True 0% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.5%\n",
      "  Predicted 0%: 99.5%\n",
      "\n",
      "Normalized Confusion Matrix for ResNet 1D (%):\n",
      "True 100% Methanol:\n",
      "  Predicted 100%: 98.5%\n",
      "  Predicted 90%: 1.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.5%\n",
      "  Predicted 0%: 0.0%\n",
      "True 90% Methanol:\n",
      "  Predicted 100%: 0.5%\n",
      "  Predicted 90%: 97.3%\n",
      "  Predicted 80%: 2.2%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 80% Methanol:\n",
      "  Predicted 100%: 0.9%\n",
      "  Predicted 90%: 5.1%\n",
      "  Predicted 80%: 90.7%\n",
      "  Predicted 70%: 3.3%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 70% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 2.5%\n",
      "  Predicted 70%: 94.4%\n",
      "  Predicted 60%: 3.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 60% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 7.8%\n",
      "  Predicted 60%: 92.2%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 50% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 90.0%\n",
      "  Predicted 40%: 9.0%\n",
      "  Predicted 30%: 0.5%\n",
      "  Predicted 20%: 0.5%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 40% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 2.8%\n",
      "  Predicted 40%: 90.4%\n",
      "  Predicted 30%: 6.7%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 30% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 4.5%\n",
      "  Predicted 30%: 91.4%\n",
      "  Predicted 20%: 4.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 20% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 2.1%\n",
      "  Predicted 20%: 96.4%\n",
      "  Predicted 10%: 1.5%\n",
      "  Predicted 0%: 0.0%\n",
      "True 10% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.9%\n",
      "  Predicted 20%: 7.5%\n",
      "  Predicted 10%: 91.1%\n",
      "  Predicted 0%: 0.5%\n",
      "True 0% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 6.9%\n",
      "  Predicted 0%: 93.1%\n",
      "\n",
      "Normalized Confusion Matrix for ResNet 2D (%):\n",
      "True 100% Methanol:\n",
      "  Predicted 100%: 97.1%\n",
      "  Predicted 90%: 2.4%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.5%\n",
      "  Predicted 0%: 0.0%\n",
      "True 90% Methanol:\n",
      "  Predicted 100%: 0.5%\n",
      "  Predicted 90%: 97.3%\n",
      "  Predicted 80%: 2.2%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 80% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 3.7%\n",
      "  Predicted 80%: 94.4%\n",
      "  Predicted 70%: 1.4%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.5%\n",
      "True 70% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 5.6%\n",
      "  Predicted 70%: 94.4%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 60% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 3.4%\n",
      "  Predicted 60%: 96.6%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 50% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.5%\n",
      "  Predicted 50%: 99.0%\n",
      "  Predicted 40%: 0.5%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 40% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 1.7%\n",
      "  Predicted 40%: 91.0%\n",
      "  Predicted 30%: 7.3%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 30% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 7.1%\n",
      "  Predicted 30%: 89.9%\n",
      "  Predicted 20%: 3.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 0.0%\n",
      "True 20% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 1.0%\n",
      "  Predicted 20%: 92.3%\n",
      "  Predicted 10%: 6.7%\n",
      "  Predicted 0%: 0.0%\n",
      "True 10% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.5%\n",
      "  Predicted 10%: 99.1%\n",
      "  Predicted 0%: 0.5%\n",
      "True 0% Methanol:\n",
      "  Predicted 100%: 0.0%\n",
      "  Predicted 90%: 0.0%\n",
      "  Predicted 80%: 0.0%\n",
      "  Predicted 70%: 0.0%\n",
      "  Predicted 60%: 0.0%\n",
      "  Predicted 50%: 0.0%\n",
      "  Predicted 40%: 0.0%\n",
      "  Predicted 30%: 0.0%\n",
      "  Predicted 20%: 0.0%\n",
      "  Predicted 10%: 0.0%\n",
      "  Predicted 0%: 100.0%\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Occasional Analysis",
   "id": "733966bb9db0e28e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-15T04:08:06.156089Z",
     "start_time": "2025-09-15T03:38:50.749071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.layers import Layer\n",
    "from keras import layers, models, optimizers, regularizers\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set matplotlib font to support superscript minus\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "# =======================================================\n",
    "# Custom layers to replace Lambda layers\n",
    "# =======================================================\n",
    "class L2NormLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.math.l2_normalize(inputs, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "class ReduceMeanLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.reduce_mean(inputs, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super().get_config()\n",
    "\n",
    "class DataAugmentation1D(Layer):\n",
    "    def __init__(self, noise_factor=0.08, **kwargs):  # Reduced noise_factor\n",
    "        super().__init__(**kwargs)\n",
    "        self.noise_factor = noise_factor\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0,\n",
    "                                     stddev=self.noise_factor, dtype=inputs.dtype)\n",
    "            return inputs + noise\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"noise_factor\": self.noise_factor})\n",
    "        return config\n",
    "\n",
    "class DataAugmentation2D(Layer):\n",
    "    def __init__(self, noise_factor=0.08, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.noise_factor = noise_factor\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(shape=tf.shape(inputs), mean=0.0,\n",
    "                                     stddev=self.noise_factor, dtype=inputs.dtype)\n",
    "            return inputs + noise\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"noise_factor\": self.noise_factor})\n",
    "        return config\n",
    "\n",
    "# =======================================================\n",
    "# DenseNet-1D architecture with balanced regularization\n",
    "# =======================================================\n",
    "def build_1d_densenet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Initial conv block\n",
    "    x = DataAugmentation1D(0.08)(inputs)\n",
    "    x = layers.Conv1D(64, 7, strides=2, padding='same', activation='relu',\n",
    "                      kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Reduced dropout\n",
    "\n",
    "    # Dense block\n",
    "    for _ in range(6):\n",
    "        y = layers.BatchNormalization()(x)\n",
    "        y = layers.Activation('relu')(y)\n",
    "        y = layers.Conv1D(32, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(y)\n",
    "        y = layers.Dropout(0.1)(y)  # Reduced dropout\n",
    "        x = layers.Concatenate()([x, y])\n",
    "\n",
    "    # Transition\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv1D(128, 1, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.AveragePooling1D(pool_size=2, strides=2, padding='same')(x)\n",
    "    x = layers.Dropout(0.1)(x)  # Reduced dropout\n",
    "\n",
    "    # Global features\n",
    "    x = ReduceMeanLayer(name=\"reduce_mean\")(x)\n",
    "    x = L2NormLayer(name=\"l2_norm\")(x)\n",
    "\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs, name='DenseNet1D')\n",
    "    return model\n",
    "\n",
    "# =======================================================\n",
    "# Directories\n",
    "# =======================================================\n",
    "data_dir = 'data'\n",
    "synthetic_dir = os.path.join(data_dir, 'synthetic')\n",
    "labels_dir = os.path.join(data_dir, 'labels')\n",
    "visualizations_dir = os.path.join(data_dir, 'visualizations')\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "experiment_dir = os.path.join('experiments', f'experiment_{timestamp}')\n",
    "model_dir = os.path.join(experiment_dir, 'models')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(visualizations_dir, exist_ok=True)\n",
    "\n",
    "# =======================================================\n",
    "# Load data\n",
    "# =======================================================\n",
    "try:\n",
    "    X_1d = np.load(os.path.join(synthetic_dir, 'synthetic_1d.npy'))\n",
    "    labels_df = pd.read_csv(os.path.join(labels_dir, 'labels.csv'))\n",
    "    y = labels_df['label'].values\n",
    "    print(\"Data loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading data:\", str(e))\n",
    "    raise\n",
    "\n",
    "# Plot sample spectra to inspect for artifacts\n",
    "wavenumbers = np.linspace(500, 3500, 880)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(5):  # Plot 5 random spectra\n",
    "    idx = np.random.randint(0, X_1d.shape[0])\n",
    "    plt.plot(wavenumbers, X_1d[idx, :, 0], label=f'Sample {i+1} (Label {y[idx]})')\n",
    "plt.xlabel('Wavenumber (cm⁻¹)')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('Sample Raman Spectra')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(visualizations_dir, 'sample_spectra.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1d, y, test_size=0.2, random_state=42)\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=11)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=11)\n",
    "\n",
    "# =======================================================\n",
    "# Build and train model\n",
    "# =======================================================\n",
    "densenet_1d = build_1d_densenet(input_shape=(880, 1), num_classes=11)\n",
    "densenet_1d.compile(optimizer=optimizers.Adam(1e-3),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "densenet_1d.fit(X_train, y_train_cat,\n",
    "                validation_data=(X_test, y_test_cat),\n",
    "                epochs=50,  # Increased for better convergence\n",
    "                batch_size=32,\n",
    "                verbose=1)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(model_dir, 'densenet_1d_full.keras')\n",
    "densenet_1d.save(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# =======================================================\n",
    "# Load model for occlusion analysis\n",
    "# =======================================================\n",
    "custom_objects = {\n",
    "    \"DataAugmentation1D\": DataAugmentation1D,\n",
    "    \"DataAugmentation2D\": DataAugmentation2D,\n",
    "    \"L2NormLayer\": L2NormLayer,\n",
    "    \"ReduceMeanLayer\": ReduceMeanLayer\n",
    "}\n",
    "try:\n",
    "    densenet_1d = models.load_model(model_path, custom_objects=custom_objects)\n",
    "    print(\"Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading model:\", str(e))\n",
    "    print(\"Possible issues:\")\n",
    "    print(\"- Model file corrupted or saved with incompatible Keras version.\")\n",
    "    print(\"- Ensure custom layers (L2NormLayer, ReduceMeanLayer) are defined correctly.\")\n",
    "    raise\n",
    "\n",
    "# =======================================================\n",
    "# Occlusion analysis with 20 regions\n",
    "# =======================================================\n",
    "# Select 550 spectra for occlusion (stratified, ~50 per class)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=550, random_state=42)\n",
    "for _, test_idx in sss.split(X_test, y_test):\n",
    "    X_occlusion = X_test[test_idx]\n",
    "    y_occlusion = y_test[test_idx]\n",
    "\n",
    "print(\"Occlusion test set shape:\", X_occlusion.shape)\n",
    "print(\"Label distribution:\\n\", pd.Series(y_occlusion).value_counts())\n",
    "\n",
    "def occlusion_analysis(model, X, y, window_size=44, wavenumbers=None):\n",
    "    num_windows = X.shape[1] // window_size  # 20 windows\n",
    "    windows = [(i * window_size, (i + 1) * window_size) for i in range(num_windows)]\n",
    "    window_labels = [f\"W{i} ({int(wavenumbers[start])}-{int(wavenumbers[end-1])} cm⁻¹)\"\n",
    "                     for i, (start, end) in enumerate(windows)]\n",
    "\n",
    "    # Original accuracy\n",
    "    y_pred = model.predict(X, verbose=0)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    original_accuracy = accuracy_score(y, y_pred_labels)\n",
    "    print(f\"Original Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "    # Occlusion\n",
    "    occlusion_accuracies = np.zeros(num_windows)\n",
    "    accuracy_drops = np.zeros(num_windows)\n",
    "\n",
    "    for win_idx, (start, end) in enumerate(windows):\n",
    "        X_occluded = X.copy()\n",
    "        X_occluded[:, start:end, :] = 0  # Occlude by setting to 0\n",
    "        y_pred_occluded = model.predict(X_occluded, verbose=0)\n",
    "        y_pred_occluded_labels = np.argmax(y_pred_occluded, axis=1)\n",
    "        acc = accuracy_score(y, y_pred_occluded_labels)\n",
    "        occlusion_accuracies[win_idx] = acc\n",
    "        accuracy_drops[win_idx] = original_accuracy - acc\n",
    "        print(f\"{window_labels[win_idx]}: Acc = {acc:.4f}, Drop = {accuracy_drops[win_idx]:.4f}\")\n",
    "\n",
    "    # Per-class analysis with console output\n",
    "    unique_labels = sorted(np.unique(y))\n",
    "    group_accuracy_drops = np.zeros((len(unique_labels), num_windows))\n",
    "    print(\"\\nPer-class Accuracy Drops:\")\n",
    "    for grp_idx, label in enumerate(unique_labels):\n",
    "        mask = y == label\n",
    "        X_group = X[mask]\n",
    "        y_group = y[mask]\n",
    "        y_pred_group = model.predict(X_group, verbose=0)\n",
    "        orig_acc_group = accuracy_score(y_group, np.argmax(y_pred_group, axis=1))\n",
    "        print(f\"\\nClass {label} ({label*10}% Ethanol / {(10-label)*10}% Methanol): Original Acc = {orig_acc_group:.4f}\")\n",
    "        for win_idx, (start, end) in enumerate(windows):\n",
    "            X_occluded_group = X_group.copy()\n",
    "            X_occluded_group[:, start:end, :] = 0\n",
    "            y_pred_occ_group = model.predict(X_occluded_group, verbose=0)\n",
    "            acc_group = accuracy_score(y_group, np.argmax(y_pred_occ_group, axis=1))\n",
    "            group_accuracy_drops[grp_idx, win_idx] = orig_acc_group - acc_group\n",
    "            print(f\"  {window_labels[win_idx]}: Acc = {acc_group:.4f}, Drop = {group_accuracy_drops[grp_idx, win_idx]:.4f}\")\n",
    "\n",
    "    # Summary of top windows per class\n",
    "    print(\"\\nTop Windows per Class:\")\n",
    "    for grp_idx, label in enumerate(unique_labels):\n",
    "        top_windows = np.argsort(group_accuracy_drops[grp_idx])[::-1][:3]  # Top 3 windows\n",
    "        top_drops = group_accuracy_drops[grp_idx, top_windows]\n",
    "        print(f\"Class {label} ({label*10}% Ethanol / {(10-label)*10}% Methanol):\")\n",
    "        for i, win_idx in enumerate(top_windows):\n",
    "            print(f\"  {window_labels[win_idx]}: Drop = {top_drops[i]:.4f}\")\n",
    "\n",
    "    return accuracy_drops, group_accuracy_drops, window_labels, unique_labels\n",
    "\n",
    "# Run occlusion analysis\n",
    "wavenumbers = np.linspace(500, 3500, 880)\n",
    "accuracy_drops, group_accuracy_drops, window_labels, unique_labels = occlusion_analysis(\n",
    "    densenet_1d, X_occlusion, y_occlusion, window_size=44, wavenumbers=wavenumbers\n",
    ")\n",
    "\n",
    "# =======================================================\n",
    "# Plotting\n",
    "# =======================================================\n",
    "# Plot 1: Bar chart of accuracy drops\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(window_labels, accuracy_drops * 100, color='skyblue')\n",
    "plt.xlabel('Occluded Window (Wavenumber Range)')\n",
    "plt.ylabel('Accuracy Drop (%)')\n",
    "plt.title('Accuracy Drop When Occluding Each Window (DenseNet 1D)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ethanol_idx = 3  # W3 contains ~870-890 cm⁻¹ (851-998 cm⁻¹)\n",
    "methanol_idx = 4  # W4 contains ~1000-1020 cm⁻¹ (1002-1149 cm⁻¹)\n",
    "plt.axvspan(ethanol_idx - 0.5, ethanol_idx + 0.5, color='green', alpha=0.1, label='Ethanol Peak (~870-890 cm⁻¹)')\n",
    "plt.axvspan(methanol_idx - 0.5, methanol_idx + 0.5, color='red', alpha=0.1, label='Methanol Peak (~1000-1020 cm⁻¹)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, 'occlusion_accuracy_drop.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Plot 2: Heatmap of per-class accuracy drops\n",
    "ratio_labels = [f\"{i*10}% Ethanol / {(10-i)*10}% Methanol\" for i in unique_labels]\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(group_accuracy_drops * 100, annot=True, fmt='.2f', cmap='Reds',\n",
    "            xticklabels=window_labels, yticklabels=ratio_labels,\n",
    "            cbar_kws={'label': 'Accuracy Drop (%)'})\n",
    "plt.xlabel('Occluded Window')\n",
    "plt.ylabel('Ethanol/Methanol Ratio')\n",
    "plt.title('Accuracy Drop (%) per Window and Ratio (Red = Important Region)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(visualizations_dir, 'occlusion_heatmap_per_ratio.png'), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# =======================================================\n",
    "# Interpretation\n",
    "# =======================================================\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- High accuracy drop in a window → That region is important for the model.\")\n",
    "print(\"- Expect high drops in W3 (851-998 cm⁻¹) for high Ethanol ratios (labels 6-10) and W4 (1002-1149 cm⁻¹) for high Methanol ratios (labels 0-5).\")\n",
    "print(\"- Check the heatmap: Red regions in W3/W4 indicate reliance on physically meaningful features.\")\n",
    "print(\"- If high drops occur in non-characteristic windows (e.g., W17: 2902-3049 cm⁻¹), inspect the dataset for artifacts (see sample_spectra.png).\")\n",
    "print(\"- Consider increasing epochs or adjusting regularization if W3/W4 drops are low.\")"
   ],
   "id": "81f23d7907d55000",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Epoch 1/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m48s\u001B[0m 147ms/step - accuracy: 0.1826 - loss: 4.0311 - val_accuracy: 0.0918 - val_loss: 3.1717\n",
      "Epoch 2/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 132ms/step - accuracy: 0.4230 - loss: 1.7904 - val_accuracy: 0.1423 - val_loss: 3.2719\n",
      "Epoch 3/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 133ms/step - accuracy: 0.4735 - loss: 1.5572 - val_accuracy: 0.2114 - val_loss: 2.0559\n",
      "Epoch 4/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 133ms/step - accuracy: 0.5240 - loss: 1.4284 - val_accuracy: 0.2327 - val_loss: 2.1822\n",
      "Epoch 5/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 124ms/step - accuracy: 0.5551 - loss: 1.3842 - val_accuracy: 0.2323 - val_loss: 1.9839\n",
      "Epoch 6/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 122ms/step - accuracy: 0.5792 - loss: 1.2632 - val_accuracy: 0.3827 - val_loss: 1.6827\n",
      "Epoch 7/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 119ms/step - accuracy: 0.6269 - loss: 1.1786 - val_accuracy: 0.5777 - val_loss: 1.2915\n",
      "Epoch 8/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 118ms/step - accuracy: 0.6390 - loss: 1.1301 - val_accuracy: 0.6268 - val_loss: 1.0224\n",
      "Epoch 9/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 118ms/step - accuracy: 0.6495 - loss: 1.1037 - val_accuracy: 0.3014 - val_loss: 1.8152\n",
      "Epoch 10/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 118ms/step - accuracy: 0.6849 - loss: 1.0419 - val_accuracy: 0.5364 - val_loss: 1.1836\n",
      "Epoch 11/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.6991 - loss: 0.9936 - val_accuracy: 0.6050 - val_loss: 1.0039\n",
      "Epoch 12/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 126ms/step - accuracy: 0.6760 - loss: 1.0187 - val_accuracy: 0.7218 - val_loss: 0.8749\n",
      "Epoch 13/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 118ms/step - accuracy: 0.6950 - loss: 0.9790 - val_accuracy: 0.6036 - val_loss: 1.1586\n",
      "Epoch 14/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.7011 - loss: 0.9413 - val_accuracy: 0.6164 - val_loss: 1.1045\n",
      "Epoch 15/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.7202 - loss: 0.9109 - val_accuracy: 0.8145 - val_loss: 0.7597\n",
      "Epoch 16/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.7179 - loss: 0.8927 - val_accuracy: 0.5677 - val_loss: 1.1523\n",
      "Epoch 17/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 118ms/step - accuracy: 0.7334 - loss: 0.8746 - val_accuracy: 0.7309 - val_loss: 0.7839\n",
      "Epoch 18/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 118ms/step - accuracy: 0.7216 - loss: 0.8757 - val_accuracy: 0.5009 - val_loss: 1.2415\n",
      "Epoch 19/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.7414 - loss: 0.8655 - val_accuracy: 0.5845 - val_loss: 1.1397\n",
      "Epoch 20/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 127ms/step - accuracy: 0.7452 - loss: 0.8487 - val_accuracy: 0.7486 - val_loss: 0.8263\n",
      "Epoch 21/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 125ms/step - accuracy: 0.7442 - loss: 0.8326 - val_accuracy: 0.8168 - val_loss: 0.7242\n",
      "Epoch 22/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 118ms/step - accuracy: 0.7420 - loss: 0.8415 - val_accuracy: 0.7132 - val_loss: 0.8496\n",
      "Epoch 23/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 126ms/step - accuracy: 0.7643 - loss: 0.7893 - val_accuracy: 0.5286 - val_loss: 1.1764\n",
      "Epoch 24/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 138ms/step - accuracy: 0.7484 - loss: 0.7988 - val_accuracy: 0.5232 - val_loss: 1.5525\n",
      "Epoch 25/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 131ms/step - accuracy: 0.7568 - loss: 0.8033 - val_accuracy: 0.7882 - val_loss: 0.6965\n",
      "Epoch 26/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 129ms/step - accuracy: 0.7676 - loss: 0.7704 - val_accuracy: 0.8909 - val_loss: 0.6288\n",
      "Epoch 27/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 126ms/step - accuracy: 0.7740 - loss: 0.7562 - val_accuracy: 0.8636 - val_loss: 0.5769\n",
      "Epoch 28/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 127ms/step - accuracy: 0.7806 - loss: 0.7479 - val_accuracy: 0.9332 - val_loss: 0.4885\n",
      "Epoch 29/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 127ms/step - accuracy: 0.7692 - loss: 0.7585 - val_accuracy: 0.8091 - val_loss: 0.6810\n",
      "Epoch 30/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 131ms/step - accuracy: 0.7768 - loss: 0.7495 - val_accuracy: 0.8273 - val_loss: 0.7055\n",
      "Epoch 31/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 141ms/step - accuracy: 0.7907 - loss: 0.7240 - val_accuracy: 0.8750 - val_loss: 0.5485\n",
      "Epoch 32/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 123ms/step - accuracy: 0.7901 - loss: 0.7154 - val_accuracy: 0.8427 - val_loss: 0.6133\n",
      "Epoch 33/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 119ms/step - accuracy: 0.7826 - loss: 0.7306 - val_accuracy: 0.6168 - val_loss: 1.3095\n",
      "Epoch 34/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 135ms/step - accuracy: 0.7781 - loss: 0.7315 - val_accuracy: 0.7614 - val_loss: 0.8346\n",
      "Epoch 35/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 124ms/step - accuracy: 0.7976 - loss: 0.7095 - val_accuracy: 0.9232 - val_loss: 0.5116\n",
      "Epoch 36/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 124ms/step - accuracy: 0.8023 - loss: 0.7015 - val_accuracy: 0.8000 - val_loss: 0.6599\n",
      "Epoch 37/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 123ms/step - accuracy: 0.7939 - loss: 0.7039 - val_accuracy: 0.8777 - val_loss: 0.5369\n",
      "Epoch 38/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 132ms/step - accuracy: 0.8111 - loss: 0.6725 - val_accuracy: 0.9300 - val_loss: 0.4384\n",
      "Epoch 39/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 122ms/step - accuracy: 0.8007 - loss: 0.6885 - val_accuracy: 0.8873 - val_loss: 0.5064\n",
      "Epoch 40/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.8088 - loss: 0.6575 - val_accuracy: 0.8741 - val_loss: 0.5385\n",
      "Epoch 41/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.8060 - loss: 0.6886 - val_accuracy: 0.8591 - val_loss: 0.5599\n",
      "Epoch 42/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 134ms/step - accuracy: 0.8132 - loss: 0.6675 - val_accuracy: 0.9177 - val_loss: 0.4967\n",
      "Epoch 43/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 126ms/step - accuracy: 0.8189 - loss: 0.6439 - val_accuracy: 0.9064 - val_loss: 0.5222\n",
      "Epoch 44/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.8223 - loss: 0.6410 - val_accuracy: 0.8177 - val_loss: 0.5955\n",
      "Epoch 45/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 117ms/step - accuracy: 0.8073 - loss: 0.6622 - val_accuracy: 0.9318 - val_loss: 0.4391\n",
      "Epoch 46/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 116ms/step - accuracy: 0.8177 - loss: 0.6394 - val_accuracy: 0.8095 - val_loss: 0.6179\n",
      "Epoch 47/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 130ms/step - accuracy: 0.8137 - loss: 0.6560 - val_accuracy: 0.9055 - val_loss: 0.4865\n",
      "Epoch 48/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 132ms/step - accuracy: 0.8182 - loss: 0.6455 - val_accuracy: 0.9473 - val_loss: 0.4355\n",
      "Epoch 49/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 130ms/step - accuracy: 0.8241 - loss: 0.6259 - val_accuracy: 0.6655 - val_loss: 0.8621\n",
      "Epoch 50/50\n",
      "\u001B[1m275/275\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 127ms/step - accuracy: 0.8285 - loss: 0.6207 - val_accuracy: 0.9423 - val_loss: 0.4269\n",
      "Model saved to experiments\\experiment_20250915_103850\\models\\densenet_1d_full.keras\n",
      "Model loaded successfully!\n",
      "Occlusion test set shape: (550, 880, 1)\n",
      "Label distribution:\n",
      " 5     53\n",
      "8     53\n",
      "1     53\n",
      "10    52\n",
      "6     51\n",
      "3     50\n",
      "0     50\n",
      "7     49\n",
      "2     48\n",
      "9     46\n",
      "4     45\n",
      "Name: count, dtype: int64\n",
      "Original Accuracy: 0.9564\n",
      "W0 (500-646 cm⁻¹): Acc = 0.9545, Drop = 0.0018\n",
      "W1 (650-796 cm⁻¹): Acc = 0.9527, Drop = 0.0036\n",
      "W2 (800-947 cm⁻¹): Acc = 0.9618, Drop = -0.0055\n",
      "W3 (950-1097 cm⁻¹): Acc = 0.5782, Drop = 0.3782\n",
      "W4 (1100-1247 cm⁻¹): Acc = 0.6055, Drop = 0.3509\n",
      "W5 (1250-1397 cm⁻¹): Acc = 0.9582, Drop = -0.0018\n",
      "W6 (1401-1547 cm⁻¹): Acc = 0.9400, Drop = 0.0164\n",
      "W7 (1551-1697 cm⁻¹): Acc = 0.9382, Drop = 0.0182\n",
      "W8 (1701-1848 cm⁻¹): Acc = 0.9564, Drop = 0.0000\n",
      "W9 (1851-1998 cm⁻¹): Acc = 0.9545, Drop = 0.0018\n",
      "W10 (2001-2148 cm⁻¹): Acc = 0.9545, Drop = 0.0018\n",
      "W11 (2151-2298 cm⁻¹): Acc = 0.9527, Drop = 0.0036\n",
      "W12 (2302-2448 cm⁻¹): Acc = 0.9527, Drop = 0.0036\n",
      "W13 (2452-2598 cm⁻¹): Acc = 0.9545, Drop = 0.0018\n",
      "W14 (2602-2749 cm⁻¹): Acc = 0.9564, Drop = 0.0000\n",
      "W15 (2752-2899 cm⁻¹): Acc = 0.9473, Drop = 0.0091\n",
      "W16 (2902-3049 cm⁻¹): Acc = 0.5109, Drop = 0.4455\n",
      "W17 (3052-3199 cm⁻¹): Acc = 0.3655, Drop = 0.5909\n",
      "W18 (3203-3349 cm⁻¹): Acc = 0.9545, Drop = 0.0018\n",
      "W19 (3353-3500 cm⁻¹): Acc = 0.9527, Drop = 0.0036\n",
      "\n",
      "Per-class Accuracy Drops:\n",
      "\n",
      "Class 0 (0% Ethanol / 100% Methanol): Original Acc = 1.0000\n",
      "  W0 (500-646 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W2 (800-947 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.9600, Drop = 0.0400\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.9600, Drop = 0.0400\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.0000, Drop = 1.0000\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.0200, Drop = 0.9800\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.9800, Drop = 0.0200\n",
      "\n",
      "Class 1 (10% Ethanol / 90% Methanol): Original Acc = 0.9811\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 1.0000, Drop = -0.0189\n",
      "  W2 (800-947 cm⁻¹): Acc = 1.0000, Drop = -0.0189\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 1.0000, Drop = -0.0189\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.9434, Drop = 0.0377\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.9623, Drop = 0.0189\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 1.0000, Drop = -0.0189\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 1.0000, Drop = -0.0189\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 1.0000, Drop = -0.0189\n",
      "\n",
      "Class 2 (20% Ethanol / 80% Methanol): Original Acc = 0.9167\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.9375, Drop = -0.0208\n",
      "  W1 (650-796 cm⁻¹): Acc = 0.9375, Drop = -0.0208\n",
      "  W2 (800-947 cm⁻¹): Acc = 0.9792, Drop = -0.0625\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.2083, Drop = 0.7083\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.3333, Drop = 0.5833\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.9792, Drop = -0.0625\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.9167, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.8542, Drop = 0.0625\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.9375, Drop = -0.0208\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.9375, Drop = -0.0208\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.9375, Drop = -0.0208\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.9167, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.9167, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 0.9167, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 0.9167, Drop = 0.0000\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.9167, Drop = 0.0000\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.6667, Drop = 0.2500\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.3958, Drop = 0.5208\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.9375, Drop = -0.0208\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.9167, Drop = 0.0000\n",
      "\n",
      "Class 3 (30% Ethanol / 70% Methanol): Original Acc = 0.9800\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W2 (800-947 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.0000, Drop = 0.9800\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.3600, Drop = 0.6200\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.9600, Drop = 0.0200\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.9600, Drop = 0.0200\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.6600, Drop = 0.3200\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.5800, Drop = 0.4000\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.9600, Drop = 0.0200\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.9800, Drop = 0.0000\n",
      "\n",
      "Class 4 (40% Ethanol / 60% Methanol): Original Acc = 1.0000\n",
      "  W0 (500-646 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W2 (800-947 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.3556, Drop = 0.6444\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.2000, Drop = 0.8000\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.9111, Drop = 0.0889\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "\n",
      "Class 5 (50% Ethanol / 50% Methanol): Original Acc = 1.0000\n",
      "  W0 (500-646 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W2 (800-947 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.6604, Drop = 0.3396\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.5472, Drop = 0.4528\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.9623, Drop = 0.0377\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.6792, Drop = 0.3208\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 1.0000, Drop = 0.0000\n",
      "\n",
      "Class 6 (60% Ethanol / 40% Methanol): Original Acc = 0.8235\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 0.8039, Drop = 0.0196\n",
      "  W2 (800-947 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.2157, Drop = 0.6078\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.1961, Drop = 0.6275\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.7843, Drop = 0.0392\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 0.8431, Drop = -0.0196\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.7843, Drop = 0.0392\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.5098, Drop = 0.3137\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.3137, Drop = 0.5098\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.8431, Drop = -0.0196\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.8235, Drop = 0.0000\n",
      "\n",
      "Class 7 (70% Ethanol / 30% Methanol): Original Acc = 0.9796\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W2 (800-947 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.8367, Drop = 0.1429\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.3265, Drop = 0.6531\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.9388, Drop = 0.0408\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.2245, Drop = 0.7551\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.0000, Drop = 0.9796\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.9796, Drop = 0.0000\n",
      "\n",
      "Class 8 (80% Ethanol / 20% Methanol): Original Acc = 0.9811\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W1 (650-796 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W2 (800-947 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.9434, Drop = 0.0377\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.7547, Drop = 0.2264\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.9623, Drop = 0.0189\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.4340, Drop = 0.5472\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.1321, Drop = 0.8491\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.9811, Drop = 0.0000\n",
      "\n",
      "Class 9 (90% Ethanol / 10% Methanol): Original Acc = 0.9783\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W1 (650-796 cm⁻¹): Acc = 0.9348, Drop = 0.0435\n",
      "  W2 (800-947 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.9783, Drop = 0.0000\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 1.0000, Drop = -0.0217\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.8696, Drop = 0.1087\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.9783, Drop = 0.0000\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.1304, Drop = 0.8478\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.0000, Drop = 0.9783\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.9565, Drop = 0.0217\n",
      "\n",
      "Class 10 (100% Ethanol / 0% Methanol): Original Acc = 0.8846\n",
      "  W0 (500-646 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W1 (650-796 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W2 (800-947 cm⁻¹): Acc = 0.8846, Drop = 0.0000\n",
      "  W3 (950-1097 cm⁻¹): Acc = 0.1538, Drop = 0.7308\n",
      "  W4 (1100-1247 cm⁻¹): Acc = 0.9231, Drop = -0.0385\n",
      "  W5 (1250-1397 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W6 (1401-1547 cm⁻¹): Acc = 0.8846, Drop = 0.0000\n",
      "  W7 (1551-1697 cm⁻¹): Acc = 0.8846, Drop = 0.0000\n",
      "  W8 (1701-1848 cm⁻¹): Acc = 0.8846, Drop = 0.0000\n",
      "  W9 (1851-1998 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W10 (2001-2148 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W11 (2151-2298 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W12 (2302-2448 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W13 (2452-2598 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W14 (2602-2749 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W15 (2752-2899 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W16 (2902-3049 cm⁻¹): Acc = 0.0385, Drop = 0.8462\n",
      "  W17 (3052-3199 cm⁻¹): Acc = 0.0000, Drop = 0.8846\n",
      "  W18 (3203-3349 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "  W19 (3353-3500 cm⁻¹): Acc = 0.8654, Drop = 0.0192\n",
      "\n",
      "Top Windows per Class:\n",
      "Class 0 (0% Ethanol / 100% Methanol):\n",
      "  W16 (2902-3049 cm⁻¹): Drop = 1.0000\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.9800\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.0400\n",
      "Class 1 (10% Ethanol / 90% Methanol):\n",
      "  W6 (1401-1547 cm⁻¹): Drop = 0.0377\n",
      "  W7 (1551-1697 cm⁻¹): Drop = 0.0189\n",
      "  W9 (1851-1998 cm⁻¹): Drop = 0.0000\n",
      "Class 2 (20% Ethanol / 80% Methanol):\n",
      "  W3 (950-1097 cm⁻¹): Drop = 0.7083\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.5833\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.5208\n",
      "Class 3 (30% Ethanol / 70% Methanol):\n",
      "  W3 (950-1097 cm⁻¹): Drop = 0.9800\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.6200\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.4000\n",
      "Class 4 (40% Ethanol / 60% Methanol):\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.8000\n",
      "  W3 (950-1097 cm⁻¹): Drop = 0.6444\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.0889\n",
      "Class 5 (50% Ethanol / 50% Methanol):\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.4528\n",
      "  W3 (950-1097 cm⁻¹): Drop = 0.3396\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.3208\n",
      "Class 6 (60% Ethanol / 40% Methanol):\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.6275\n",
      "  W3 (950-1097 cm⁻¹): Drop = 0.6078\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.5098\n",
      "Class 7 (70% Ethanol / 30% Methanol):\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.9796\n",
      "  W16 (2902-3049 cm⁻¹): Drop = 0.7551\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.6531\n",
      "Class 8 (80% Ethanol / 20% Methanol):\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.8491\n",
      "  W16 (2902-3049 cm⁻¹): Drop = 0.5472\n",
      "  W4 (1100-1247 cm⁻¹): Drop = 0.2264\n",
      "Class 9 (90% Ethanol / 10% Methanol):\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.9783\n",
      "  W16 (2902-3049 cm⁻¹): Drop = 0.8478\n",
      "  W6 (1401-1547 cm⁻¹): Drop = 0.1087\n",
      "Class 10 (100% Ethanol / 0% Methanol):\n",
      "  W17 (3052-3199 cm⁻¹): Drop = 0.8846\n",
      "  W16 (2902-3049 cm⁻¹): Drop = 0.8462\n",
      "  W3 (950-1097 cm⁻¹): Drop = 0.7308\n",
      "\n",
      "Interpretation:\n",
      "- High accuracy drop in a window → That region is important for the model.\n",
      "- Expect high drops in W3 (851-998 cm⁻¹) for high Ethanol ratios (labels 6-10) and W4 (1002-1149 cm⁻¹) for high Methanol ratios (labels 0-5).\n",
      "- Check the heatmap: Red regions in W3/W4 indicate reliance on physically meaningful features.\n",
      "- If high drops occur in non-characteristic windows (e.g., W17: 2902-3049 cm⁻¹), inspect the dataset for artifacts (see sample_spectra.png).\n",
      "- Consider increasing epochs or adjusting regularization if W3/W4 drops are low.\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Compare performance with Baseline Removal to not have Baseline Removal",
   "id": "2201f484c310787d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T18:56:52.928062Z",
     "start_time": "2025-11-24T18:33:06.110797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from pyts.transformation import GADF\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ==============================\n",
    "# SEED & THƯ MỤC\n",
    "# ==============================\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "exp_dir = f'git s/NO_BASELINE_{timestamp}'\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "os.makedirs(f'{exp_dir}/models', exist_ok=True)\n",
    "os.makedirs(f'{exp_dir}/results', exist_ok=True)\n",
    "\n",
    "# ==============================\n",
    "# LOAD DỮ LIỆU\n",
    "# ==============================\n",
    "df = pd.read_excel('data/Ethanol_Methanol.xlsx', usecols='A:L')\n",
    "\n",
    "# Tên cột đúng trong file Excel của bạn\n",
    "name_to_ratio = {\n",
    "    'Ethanol': 1.0, 'Methanol': 0.0,\n",
    "    'EM1_a': 0.9, 'EM2_a': 0.8, 'EM3_a': 0.7,\n",
    "    'EM4_a': 0.6, 'EM5_a': 0.5, 'EM6_a': 0.4,\n",
    "    'EM7_a': 0.3, 'EM8_a': 0.2, 'EM9_a': 0.1\n",
    "}\n",
    "\n",
    "# Hàm interpolate chung (chỉ dùng cho dữ liệu gốc)\n",
    "def to_880(arr):\n",
    "    arr = arr[~np.isnan(arr)]                     # bỏ NaN\n",
    "    if len(arr) == 880:\n",
    "        return arr\n",
    "    x_old = np.linspace(0, len(arr)-1, len(arr))\n",
    "    x_new = np.linspace(0, len(arr)-1, 880)\n",
    "    f = interp1d(x_old, arr, kind='linear', fill_value='extrapolate')\n",
    "    return f(x_new)\n",
    "\n",
    "spectra_data = {name: to_880(df[name].values) for name in name_to_ratio}\n",
    "noise_data   = np.load('data/dataset_noise_pure_182.npy')   # (N,1,2,880,1)\n",
    "\n",
    "# ==============================\n",
    "# HELPER\n",
    "# ==============================\n",
    "def normalize(s):\n",
    "    s = s - s.min()\n",
    "    return s / s.max() if s.max() > 0 else s\n",
    "\n",
    "def generate_synthetic(clean_spec):\n",
    "    idx = np.random.randint(len(noise_data))\n",
    "    real_noise = noise_data[idx,0,1,:,0] - noise_data[idx,0,0,:,0]\n",
    "    s = clean_spec + real_noise * np.random.uniform(1.0, 2.0)\n",
    "    s += np.random.normal(0, 0.05*s.std(), 880)\n",
    "\n",
    "    # vẫn thêm nền tổng hợp như cũ\n",
    "    if np.random.rand() < 0.6:\n",
    "        x = np.arange(880)\n",
    "        if np.random.rand() < 0.5:                              # poly\n",
    "            baseline = (x/879)**np.random.uniform(1.9, 2.1)\n",
    "        else:                                                   # gaussian\n",
    "            mu, sd = np.random.uniform(0,880), np.random.uniform(250,300)\n",
    "            baseline = np.exp(-0.5*((x-mu)/sd)**2)\n",
    "        s += baseline * np.random.uniform(0.7, 0.9)\n",
    "\n",
    "    if np.random.rand() < 0.5:\n",
    "        s = np.roll(s, np.random.randint(-10, 11))\n",
    "    return s\n",
    "\n",
    "def gadf_map(spec, size=64):\n",
    "    spec = normalize(spec)\n",
    "    spec = 2*spec - 1\n",
    "    # cắt hoặc pad để chia hết cho 64\n",
    "    target_len = size * (len(spec)//size)\n",
    "    if len(spec) > target_len:\n",
    "        spec = spec[:target_len]\n",
    "    else:\n",
    "        spec = np.pad(spec, (0, target_len-len(spec)), mode='constant')\n",
    "    return GADF(image_size=size, overlapping=False, scale='-1')\\\n",
    "           .fit_transform(spec.reshape(1,-1))[0][:,:,np.newaxis]\n",
    "\n",
    "# ==============================\n",
    "# TẠO 11.000 PHỔ GADF (KHÔNG TRỪ NỀN)\n",
    "# ==============================\n",
    "X_2d = []\n",
    "y    = []\n",
    "\n",
    "# ánh xạ tỉ lệ → nhãn 0-10\n",
    "ratio_to_label = {v: i for i, v in enumerate(sorted(name_to_ratio.values()))}\n",
    "\n",
    "print(\"Đang tạo 11.000 phổ GADF (không trừ nền)...\")\n",
    "for name, ratio in name_to_ratio.items():\n",
    "    clean = spectra_data[name]\n",
    "    norm_clean = normalize(clean)\n",
    "\n",
    "    # phổ gốc\n",
    "    X_2d.append(gadf_map(norm_clean))\n",
    "    y.append(ratio_to_label[ratio])\n",
    "\n",
    "    # 999 phổ tổng hợp\n",
    "    for i in range(999):\n",
    "        synth = generate_synthetic(norm_clean)\n",
    "        X_2d.append(gadf_map(normalize(synth)))\n",
    "        y.append(ratio_to_label[ratio])\n",
    "\n",
    "        if (i+1) % 400 == 0:\n",
    "            print(f\"   → {name}: {i+1}/999\")\n",
    "\n",
    "X_2d = np.array(X_2d)\n",
    "y    = np.array(y)\n",
    "print(f\"\\nHoàn tất! X_2d shape: {X_2d.shape}  |  Label distribution:\\n{np.bincount(y)}\")\n",
    "\n",
    "# lưu tạm để lần sau không phải tạo lại\n",
    "np.save(f'{exp_dir}/gadf_no_baseline.npy', X_2d)\n",
    "np.save(f'{exp_dir}/labels_no_baseline.npy', y)\n",
    "\n",
    "# ==============================\n",
    "# MODEL 2D (DenseNet & ResNet)\n",
    "# ==============================\n",
    "def build_densenet_2d():\n",
    "    inputs = layers.Input((64,64,1))\n",
    "    x = layers.Conv2D(48,3,padding='same',activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    for _ in range(3):\n",
    "        for __ in range(4):\n",
    "            y = layers.BatchNormalization()(x)\n",
    "            y = layers.ReLU()(y)\n",
    "            y = layers.Conv2D(12,3,padding='same')(y)\n",
    "            x = layers.Concatenate()([x,y])\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.ReLU()(x)\n",
    "        x = layers.Conv2D(x.shape[-1]//2,1,padding='same')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128,activation='relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    outputs = layers.Dense(11,activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "def build_resnet_2d():\n",
    "    inputs = layers.Input((64,64,1))\n",
    "    x = layers.Conv2D(32,3,padding='same',activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    def rb(x,f):\n",
    "        s = x\n",
    "        x = layers.Conv2D(f,3,padding='same',activation='relu')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(f,3,padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        if s.shape[-1] != f:\n",
    "            s = layers.Conv2D(f,1,padding='same')(s)\n",
    "        x = layers.Add()([s,x])\n",
    "        return layers.ReLU()(x)\n",
    "    x = rb(x,32); x = rb(x,32)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128,activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(11,activation='softmax')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "aug = keras.Sequential([layers.RandomFlip(\"horizontal\"),\n",
    "                        layers.RandomRotation(0.05)])\n",
    "\n",
    "# ==============================\n",
    "# TRAIN & EVALUATE (10 epochs như gốc)\n",
    "# ==============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_2d, y, test_size=0.2,\n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.arange(11), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "def train_and_eval(build_fn, name):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = keras.Sequential([aug, build_fn()])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train,\n",
    "              epochs=10,\n",
    "              batch_size=32,\n",
    "              validation_split=0.1,\n",
    "              class_weight=class_weight_dict,\n",
    "              verbose=1,\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                       restore_best_weights=True)])\n",
    "    pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "    return {\n",
    "        'accuracy' : accuracy_score(y_test, pred),\n",
    "        'precision': precision_score(y_test, pred, average='macro', zero_division=0),\n",
    "        'recall'   : recall_score(y_test, pred, average='macro', zero_division=0),\n",
    "        'f1'       : f1_score(y_test, pred, average='macro', zero_division=0)\n",
    "    }\n",
    "\n",
    "print(\"\\n=== Bắt đầu huấn luyện (10 epochs, KHÔNG trừ nền) ===\")\n",
    "res_dn = train_and_eval(build_densenet_2d, \"DenseNet2D\")\n",
    "res_rn = train_and_eval(build_resnet_2d,   \"ResNet2D\")\n",
    "\n",
    "# ==============================\n",
    "# IN KẾT QUẢ CUỐI CÙNG\n",
    "# ==============================\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "print(\"KẾT QUẢ KHI KHÔNG TRỪ NỀN (10 epochs)\")\n",
    "print(\"=\"*75)\n",
    "print(f\"DenseNet 2D (GADF)  →  Acc: {res_dn['accuracy']:.4f}  |  Prec: {res_dn['precision']:.4f}  |  Rec: {res_dn['recall']:.4f}  |  F1: {res_dn['f1']:.4f}\")\n",
    "print(f\"ResNet   2D (GADF)  →  Acc: {res_rn['accuracy']:.4f}  |  Prec: {res_rn['precision']:.4f}  |  Rec: {res_rn['recall']:.4f}  |  F1: {res_rn['f1']:.4f}\")\n",
    "print(\"=\"*75)\n",
    "\n",
    "# lưu metrics\n",
    "import json\n",
    "with open(f'{exp_dir}/results/metrics_NO_baseline.json', 'w') as f:\n",
    "    json.dump({'DenseNet2D': res_dn, 'ResNet2D': res_rn}, f, indent=4)"
   ],
   "id": "9e9024c0e22181f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tạo 11.000 phổ GADF (không trừ nền)...\n",
      "   → Ethanol: 400/999\n",
      "   → Ethanol: 800/999\n",
      "   → Methanol: 400/999\n",
      "   → Methanol: 800/999\n",
      "   → EM1_a: 400/999\n",
      "   → EM1_a: 800/999\n",
      "   → EM2_a: 400/999\n",
      "   → EM2_a: 800/999\n",
      "   → EM3_a: 400/999\n",
      "   → EM3_a: 800/999\n",
      "   → EM4_a: 400/999\n",
      "   → EM4_a: 800/999\n",
      "   → EM5_a: 400/999\n",
      "   → EM5_a: 800/999\n",
      "   → EM6_a: 400/999\n",
      "   → EM6_a: 800/999\n",
      "   → EM7_a: 400/999\n",
      "   → EM7_a: 800/999\n",
      "   → EM8_a: 400/999\n",
      "   → EM8_a: 800/999\n",
      "   → EM9_a: 400/999\n",
      "   → EM9_a: 800/999\n",
      "\n",
      "Hoàn tất! X_2d shape: (11000, 64, 64, 1)  |  Label distribution:\n",
      "[1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000]\n",
      "\n",
      "=== Bắt đầu huấn luyện (10 epochs, KHÔNG trừ nền) ===\n",
      "WARNING:tensorflow:From C:\\Users\\Admin\\anaconda3\\envs\\Raman_NCKH\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m129s\u001B[0m 486ms/step - accuracy: 0.2310 - loss: 2.0297 - val_accuracy: 0.0966 - val_loss: 3.2822\n",
      "Epoch 2/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m97s\u001B[0m 391ms/step - accuracy: 0.6500 - loss: 0.8104 - val_accuracy: 0.5273 - val_loss: 1.1841\n",
      "Epoch 3/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m97s\u001B[0m 390ms/step - accuracy: 0.7760 - loss: 0.5504 - val_accuracy: 0.7250 - val_loss: 0.7295\n",
      "Epoch 4/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m111s\u001B[0m 447ms/step - accuracy: 0.8257 - loss: 0.4463 - val_accuracy: 0.8932 - val_loss: 0.2697\n",
      "Epoch 5/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m99s\u001B[0m 401ms/step - accuracy: 0.8739 - loss: 0.3306 - val_accuracy: 0.8705 - val_loss: 0.3119\n",
      "Epoch 6/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m92s\u001B[0m 372ms/step - accuracy: 0.9014 - loss: 0.2703 - val_accuracy: 0.9284 - val_loss: 0.1804\n",
      "Epoch 7/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m103s\u001B[0m 415ms/step - accuracy: 0.8971 - loss: 0.2709 - val_accuracy: 0.9068 - val_loss: 0.2504\n",
      "Epoch 8/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m101s\u001B[0m 408ms/step - accuracy: 0.9170 - loss: 0.2291 - val_accuracy: 0.9307 - val_loss: 0.1947\n",
      "Epoch 9/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m98s\u001B[0m 395ms/step - accuracy: 0.9208 - loss: 0.2016 - val_accuracy: 0.9307 - val_loss: 0.2132\n",
      "Epoch 10/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m94s\u001B[0m 379ms/step - accuracy: 0.9345 - loss: 0.1803 - val_accuracy: 0.8727 - val_loss: 0.3563\n",
      "Epoch 1/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m42s\u001B[0m 154ms/step - accuracy: 0.1399 - loss: 2.3827 - val_accuracy: 0.0818 - val_loss: 4.2700\n",
      "Epoch 2/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 146ms/step - accuracy: 0.3724 - loss: 1.5225 - val_accuracy: 0.4534 - val_loss: 1.3204\n",
      "Epoch 3/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m36s\u001B[0m 144ms/step - accuracy: 0.5707 - loss: 0.9879 - val_accuracy: 0.7727 - val_loss: 0.7012\n",
      "Epoch 4/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m34s\u001B[0m 138ms/step - accuracy: 0.6920 - loss: 0.7385 - val_accuracy: 0.8375 - val_loss: 0.4538\n",
      "Epoch 5/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m35s\u001B[0m 140ms/step - accuracy: 0.7759 - loss: 0.5816 - val_accuracy: 0.8136 - val_loss: 0.5151\n",
      "Epoch 6/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 130ms/step - accuracy: 0.8197 - loss: 0.4937 - val_accuracy: 0.7148 - val_loss: 0.6122\n",
      "Epoch 7/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m37s\u001B[0m 147ms/step - accuracy: 0.8367 - loss: 0.4405 - val_accuracy: 0.7932 - val_loss: 0.4995\n",
      "Epoch 8/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m38s\u001B[0m 153ms/step - accuracy: 0.8549 - loss: 0.4016 - val_accuracy: 0.7455 - val_loss: 0.6513\n",
      "Epoch 9/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m39s\u001B[0m 158ms/step - accuracy: 0.8721 - loss: 0.3615 - val_accuracy: 0.8920 - val_loss: 0.3045\n",
      "Epoch 10/10\n",
      "\u001B[1m248/248\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m33s\u001B[0m 133ms/step - accuracy: 0.8749 - loss: 0.3399 - val_accuracy: 0.8523 - val_loss: 0.3811\n",
      "\n",
      "===========================================================================\n",
      "KẾT QUẢ KHI KHÔNG TRỪ NỀN (10 epochs)\n",
      "===========================================================================\n",
      "DenseNet 2D (GADF)  →  Acc: 0.9259  |  Prec: 0.9258  |  Rec: 0.9259  |  F1: 0.9251\n",
      "ResNet   2D (GADF)  →  Acc: 0.8800  |  Prec: 0.8853  |  Rec: 0.8800  |  F1: 0.8784\n",
      "===========================================================================\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6456dc1e083f7b31"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
